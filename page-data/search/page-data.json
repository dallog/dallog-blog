{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 바로 직전 포스팅으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요? 프론트엔드 CD 다이어그램  프론트엔드의 지속적 배포 과정…","fields":{"slug":"/continuous-deploy-with-jenkins-2-frontend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (2) - 프론트엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 바로 [직전 포스팅](https://dallog.github.io/continuous-deploy-with-jenkins-1-backend/)으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요?\n\n## 프론트엔드 CD 다이어그램\n\n![프론트엔드 지속적 배포 환경](./fe.png)\n\n프론트엔드의 지속적 배포 과정은 사실 백엔드과 크게 다른점이 없습니다. PR이 생성되고, 병합되고, 이 이벤트가 Webhook을 통해 젠킨스 서버에 전달됩니다.\n\n젠킨스 서버는 Webpack을 이용하여 리액트 프로젝트를 빌드하고, `index.html`과 `bundle.js`를 생성합니다. 달록의 프론트엔드 EC2 인스턴스에는 **NGINX가 도커 컨테이너**로 띄워져 있는데요, 생성된 정적파일은 이 NGINX 디렉토리로 전송됩니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다. 달록팀 지속적 배포 환경 일단 현재 구성된 달록팀의 …","fields":{"slug":"/continuous-deploy-with-jenkins-1-backend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (1) - 백엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다.\n\n## 달록팀 지속적 배포 환경\n\n일단 현재 구성된 달록팀의 지속적 배포 환경을 간단한 다이어그램으로 살펴보겠습니다.\n\n![백엔드 지속적 배포 환경](./be.png)\n\n우선, 달록의 백엔드 개발자가 열심히 기능을 개발하여 Github에 PR을 생성합니다. 이때 PR 코드가 정상적으로 빌드되고, 모든 테스트를 통과하는지 **Github Actions**를 사용하여 우선적으로 검사합니다. 이때, PR 브랜치의 코드가 문제가 있다면 develop 브랜치로 병합이 불가능합니다.\n\n> 달록팀의 브랜치 전략에 대해서는 달록팀 매트가 작성한 '[달록팀의 git 브랜치 전략을 소개합니다.](https://dallog.github.io/git-branch-strategy/)' 포스팅을 참고해주세요!\n\n정상적으로 빌드가 되는 코드는 이후 달록 개발자들끼리 코드리뷰가 진행되고, develop 브랜치에 병합이 됩니다. 이때, Github는 달록팀이 구축한 젠킨스 서버에 **Webhook**을 통해 병합 사실을 알립니다.\n\nWebhook이란, 특정한 애플리케이션이 다른 애플리케이션으로 **이벤트 발생 정보를 실시간으로 제공하기 위한 방법**입니다. 젠킨스는 외부에 Webhook URL을 열어두고, Github으로부터 이 Webhook URL로 요청을 받아 이벤트가 발생한 즉시 그 사실을 알 수 있습니다.\n\nWebhook을 통해 신호를 받은 젠킨스는 미리 지정된 젠킨스 파이프라인 스크립트를 실행하여 스프링부트 어플리케이션을 빌드하여 `jar` 파일을 생성합니다.\n\n생성한 `jar` 파일은 스프링부트 어플리케이션이 실행되고 있는 EC2 인스턴스로 전송됩니다. 그리고 스프링부트 인스턴스에서 `jar` 파일이 실행되어 배포가 완료됩니다.\n\n## 프리스타일과 파이프라인\n\n젠킨스에서 잡(Job)을 생성하는 방식은 크게 **프리스타일과 파이프라인** 두가지로 나뉩니다.\n\n### 프리스타일\n\n프리스타일은 **GUI기반**으로 젠킨스 잡을 구성할 수 있습니다. 따라서 간단한 작업에 적합하고, **복잡한 작업에는 다소 적절치 않습니다.** 또한 GUI 기반이므로 설정 과정이 다소 난잡하게 느껴질 수 있습니다.\n\n### 파이프라인\n\n반면 젠킨스 파이프라인은 일련의 배포 과정을 **코드로 작성**할 수 있습니다. 이 코드는 **Jenkinsfile** 라고 불리는 파일로 관리할 수 있어, Git 등을 통한 **버전관리**도 가능합니다. 또한 파이프라인을 사용하면 스테이지(Stage)라는 단위로 각 작업에 소요되는 시간, 실패여부를 **시각화**하여 확인할 수 있습니다.\n\n달록은 프리스타일 대신 젠킨스 파이프라인을 이용하여 잡(Job)을 생성했습니다. 개발자 입장에서는 아무래도 GUI보다는 코드 기반으로 작업 과정을 작성하는 것이 편하게 느껴집니다. 또한 추후 배포 프로세스 자체를 Github 저장소에서 관리할 수 있다는 것이 큰 장점으로 다가왔습니다.\n\n#### Scripted vs Declarative\n\n파이프라인도 문법에 따라 크게 **Scripted Pipeline**과 **Declarative Pipeline** 두가지로 나뉩니다.\n\nScripted는 Groovy라는 언어로 작성되며, 변수 선언등이 지원되어 프로그래밍을 할 수 있다는 특징이 있다. 반면, Declarative는 Scripted에 비해 간단하며, Groovy를 알지 않아도 사용할 수 있다는 장점이 존재합니다. **Scripted 문법은 Declarative에 비해 유연성이나 확장성이 높지만, 복잡도와 유지보수 난이도가 더 높습니다**.\n\n최근 CI/CD 기조는 Declarative 스타일로 많이 이동되고 있다고 합니다 ([참고](https://www.theserverside.com/answer/Declarative-vs-scripted-pipelines-Whats-the-difference)). 당장 비교적 최근에 출시된 Github Actions 도 YAML 기반의 Declarative 스타일만 사용할 수 있습니다. 따라서 달록팀은 Declarative 문법을 사용하기로 결정했습니다.\n\n## 잡(Job) 생성 및 세팅\n\n### 파이프라인으로 생성\n\n![](./pipeline.png)\n\n젠킨스 메뉴에서 '새로운 Item'을 클릭하고, 'Pipeline'을 선택하여 새로운 잡을 생성합니다.\n\n### Github URL 설정\n\n**General > GitHub project > Project url** 에 저희 깃허브 저장소 주소인 **https://github.com/woowacourse-teams/2022-dallog** 를 입력합니다.\n\n### 오래된 빌드 삭제\n\n빌드 이력을 오래 가지고 있어봤자 큰 의미가 없을 것 같기도 하고, 아무래도 **t4g.micro** 에서 돌리다보니 용량이 넉넉치 않기도 합니다. 따라서 **General > 오래된 빌드 삭제** 옵션을 클릭하여 활성화하고, 보관할 최대 갯수를 3으로 지정했습니다.\n\n> 나중에 빌드 이력 보관 개수가 너무 적다고 판단되면 조금 더 늘릴 생각입니다 🙂\n\n### 빌드 트리거 설정\n\n**Build Triggers > GitHub hook trigger for GITScm polling** 을 체크해줍니다. Github의 Webhook을 통해 빌드가 트리거되는 옵션입니다.\n\n> 이 기능은 **GitHub plugin**에서 제공하는 기능입니다. 최초 젠킨스를 설치했을 때 Install suggested plugins 를 선택하지 않으면 이 플러그인이 설치되어있지 않을수도 있습니다.\n\n#### Github 저장소에 Webhook 등록하기\n\nWebhook을 사용하려면 Github 저장소의 Settings > Webhooks 에서 Webhook URL을 등록해주어야 합니다.\n\n```\nhttp://{서비스 IP 혹은 도메인 주소}/github-webhook/\n```\n\n위와 같이 URL을 등록해줍니다. 이때 URL의 마지막에 `/`가 들어가지 않으면 오류가 발생하니 꼭 추가합니다.\n\n## 파이프라인 작성\n\n이제 빌드가 트리거 되었을 때 실행될 파이프라인 스크립트를 작성할 차례입니다.\n\n```groovy\npipeline {\n   agent any\n   stages {\n       stage('Github') {\n           steps {\n               git branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n           }\n       }\n       stage('Build') {\n           steps {\n               dir('backend') {\n                   sh \"./gradlew bootJar\"\n               }\n           }\n       }\n       stage('Deploy') {\n           steps {\n               dir('backend/build/libs') {\n                   sshagent(credentials: ['key-dallog']) {\n                        sh 'scp backend-0.0.1-SNAPSHOT.jar ubuntu@192.168.XXX.XXX:/home/ubuntu'\n                        sh 'ssh ubuntu@192.168.XXX.XXX \"sh run.sh\" &'\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\n### Github stage\n\nGithub stage에서는 사용할 깃허브의 저장소 주소와 브랜치를 설정합니다. 젠킨스는 이 스테이지에 명시된 저장소와 브랜치를 기준으로 코드를 가져옵니다.\n\n### Build stage\n\n`gradlew` 파일을 사용하여 빌드합니다. 이때, `dir` 지시어(Directive)를 사용하여 명령을 수행할 디렉토리를 지정할 수 있습니다.\n\n### Deploy stage\n\n> 이 작업을 위해서 사전에 Jenkins 관리 > Manage Credentials 에서 'SSH Username with private key' 로 AWS에서 발급 받은 PEM 키를 먼저 등록해야합니다.\n\n달록팀은 SSH Agent 플러그인을 사용하여 배포 서버에 원격으로 접속합니다. SSH Agent 플러그인을 사용하여 파이프라인에서 젠킨스에 등록해둔 SSH 자격증명을 쉽게 사용할 수 있습니다. 플러그인을 설치하면 `sshagent` 라는 Directive를 사용할 수 있는데, 인자로 사전에 젠킨스 Credential로 등록한 PEM키의 이름을 넣어줍니다. 이렇게 만들어진 `sshagent` Directive Block 내부에서 `sh` 를 통해 `ssh` 관련 명령을 수행할 수 있습니다.\n\n#### jar 파일 전송\n\nSCP(Secure Copy)는 SSH 통신 기반으로 원격지에 파일이나 디렉토리를 전송할 수 있는 프로토콜입니다. 리눅스에서는 `scp` 명령을 통해 SCP 프로토콜을 사용할 수 있습니다. 달록팀은 이 `scp` 명령을 통해 스프링부트 애플리케이션이 실행되고 있는 EC2 인스턴스로 빌드된 `jar` 파일을 전송합니다.\n\n#### 원격지의 쉘 스크립트 실행\n\n스프링부트 인스턴스에는 사전에 `run.sh` 라는 이름의 쉘 스크립트가 작성되어 있습니다. 이 쉘 스크립트는 현재 실행중인 스프링부트 애플리케이션의 프로세스를 제거하고, 환경변수를 설정하고, 같은 디렉토리에 있는 `jar` 파일을 실행합니다. 스크립트는 아래와 같습니다.\n\n```shell\n#! /bin/bash\n\nPROJECT_NAME=backend\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n    echo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n    echo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n    kill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXXXXXXXXXXX\"\nexport JWT_EXPIRE_LENGTH=86400000\nexport JWT_SECRET_KEY=\"XXXXXXXXXXXXXX\"\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_NAME=$(ls | grep .jar | head -n 1)\nsudo -E nohup java -jar /home/ubuntu/$JAR_NAME &\n```\n\n지난 포스팅인 **[쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화](/deploy-automation-with-shell-script)** 에서 소개드린 쉘 스크립트에서 소스코드를 Pull 해오고 빌드하는 과정만 사라진 스크립트입니다.\n\n이 원격지에 있는 스크립트를 `ssh` 명령을 통해 실행하게되면 배포과정이 완료됩니다.\n\n## 트러블슈팅\n\n### Webhook 트리거 이슈\n\n처음 젠킨스를 설정할 때 아무리 Webhook 설정을 건드려봐도 빌드가 트리거되지 않는 이슈가 발생했었습니다.\n\n먼저 위 파이프라인 스크립트의 Github 스테이지에서 사용된 `git` Directive를 명시적으로 사용해줘야합니다.\n\n```\ngit branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n```\n\n또한 최초로 '지금 빌드' 버튼을 클릭해야지 그 이후 Webhook 요청을 수신할 수 있게됩니다.\n\n### SSH 자격 증명 이슈\n\n```\nHost key verification failed.\nlost connection\n```\n\n젠킨스 서버에서 다른 인스턴스에 최초로 SSH 연결을 시도할 경우 젠킨스에서 위와 같은 에러가 발생할 수 있습니다. 다들 SSH로 최초 원격접속 시 아래와 같은 메시지를 본 적이 있을 것 입니다.\n\n```\nroot@XXXXXXXX:/home# ssh -i key.pem ubuntu@192.168.XXX.XXX\nThe authenticity of host '192.168.XXX.XXX (192.168.XXX.XXX)' can't be established.\nECDSA key fingerprint is SHA256:XXXXXXXXXXXXXXXX\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n```\n\n터미널에서 직접 접속할 때에는 yes 를 입력하면, 곧바로 접속될텐데요. 젠킨스 파이프라인에서는 불가능합니다. 따라서 미리 젠킨스 측의 `known_hosts` 에 원격지의 공개키를 등록해야합니다.\n\n`keyscan` 명령을 통해서 호스트의 공개키를 수집할 수 있습니다. 이 명령을 통해서 `~/.ssh/known_hosts` 에 접속할 호스트의 공개키를 추가해야합니다.\n\n## 마치며\n\n이번 포스팅으로 달록팀이 젠킨스를 활용하여 스프링부트 애플리케이션의 지속적 배포 환경을 구축한 방법을 소개드렸습니다. 이어지는 다음 포스팅에서는 달록팀의 프론트엔드 배포 환경과 지속적 배포 환경 구축 방법을 소개드리려고 합니다. 읽어주셔서 감사드립니다 😄\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 외부와 의존성 분리하기 도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다.  인터페이스 활용하기 우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수…","fields":{"slug":"/separated-interface/"},"frontmatter":{"date":"July 24, 2022","title":"외부와 의존성 분리하기","tags":["분리된 인터페이스","의존성 분리"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## 외부와 의존성 분리하기\n\n도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다. \n\n### 인터페이스 활용하기\n\n우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수단으로 인터페이스를 활용할 수 있다. 간단한 예시로 `JpaRepository`를 살펴보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n\n    Optional<Member> findByEmail(final String email);\n\n    boolean existsByEmail(final String email);\n}\n```\n\n이러한 인터페이스 덕분에 우리는 실제 DB에 접근하는 내부 구현에 의존하지 않고 데이터를 조작할 수 있다. 핵심은 `실제 DB에 접근하는 행위`이다.\n\n아래는 `Spring Data`가 만든 `JpaRepository의 구현체` `SimpleJpaRepository`의 일부를 가져온 것이다.\n\n```java\n@Repository\n@Transactional(readOnly = true)\npublic class SimpleJpaRepository<T, ID> implements JpaRepositoryImplementation<T, ID> {\n\n\tprivate static final String ID_MUST_NOT_BE_NULL = \"The given id must not be null!\";\n\n\tprivate final JpaEntityInformation<T, ?> entityInformation;\n\tprivate final EntityManager em;\n\tprivate final PersistenceProvider provider;\n\n\tprivate @Nullable CrudMethodMetadata metadata;\n\tprivate EscapeCharacter escapeCharacter = EscapeCharacter.DEFAULT;\n\n\tpublic SimpleJpaRepository(JpaEntityInformation<T, ?> entityInformation, EntityManager entityManager) {\n\n\t\tAssert.notNull(entityInformation, \"JpaEntityInformation must not be null!\");\n\t\tAssert.notNull(entityManager, \"EntityManager must not be null!\");\n\n\t\tthis.entityInformation = entityInformation;\n\t\tthis.em = entityManager;\n\t\tthis.provider = PersistenceProvider.fromEntityManager(entityManager);\n\t}\n  ...\n}\n```\n\n해당 구현체는 `entityManger`를 통해 객체를 영속 시키는 행위를 진행하고 있기 때문에 `영속 계층`에 가깝다고 판단했다. 즉 도메인의 입장에서 `MemberRepository`를 바라볼 때 단순히 `JpaRepository`를 상속한 인터페이스를 가지고 있기 때문에 `영속 계층`에 대한 직접적인 의존성은 없다고 봐도 무방하다. 정리하면 우리는 인터페이스를 통해 실제 구현체에 의존하지 않고 로직을 수행할 수 있게 된다. \n\n### 관점 변경하기\n\n이러한 사례를 외부 서버와 통신을 담당하는 우리가 직접 만든 인터페이스인 `OAuthClient`에 대입해본다. `OAuthClient`의 가장 큰 역할은 n의 소셜에서 `OAuth 2.0`을 활용한 인증의 행위를 정의한 인터페이스이다. google, github 등 각자에 맞는 요청을 처리하기 위해 `OAuthClient`를 구현한 뒤 로직을 처리할 수 있다. 아래는 실제 google의 인가 코드를 기반으로 토큰 정보에서 회원 정보를 조회하는 로직을 담고 있다.\n\n```java\npublic interface OAuthClient {\n\n    OAuthMember getOAuthMember(final String code);\n}\n```\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final String googleRedirectUri;\n    private final String googleClientId;\n    private final String googleClientSecret;\n    private final String googleTokenUri;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(@Value(\"${oauth.google.redirect_uri}\") final String googleRedirectUri,\n                             @Value(\"${oauth.google.client_id}\") final String googleClientId,\n                             @Value(\"${oauth.google.client_secret}\") final String googleClientSecret,\n                             @Value(\"${oauth.google.token_uri}\") final String googleTokenUri,\n                             final RestTemplate restTemplate, final ObjectMapper objectMapper) {\n        this.googleRedirectUri = googleRedirectUri;\n        this.googleClientId = googleClientId;\n        this.googleClientSecret = googleClientSecret;\n        this.googleTokenUri = googleTokenUri;\n        this.restTemplate = restTemplate;\n        this.objectMapper = objectMapper;\n    }\n\n    @Override\n    public OAuthMember getOAuthMember(final String code) {\n        GoogleTokenResponse googleTokenResponse = requestGoogleToken(code);\n        String payload = getPayloadFrom(googleTokenResponse.getIdToken());\n        String decodedPayload = decodeJwtPayload(payload);\n\n        try {\n            return generateOAuthMemberBy(decodedPayload);\n        } catch (JsonProcessingException e) {\n            throw new IllegalArgumentException();\n        }\n    }\n\n    private GoogleTokenResponse requestGoogleToken(final String code) {\n        HttpHeaders headers = new HttpHeaders();\n        headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n        MultiValueMap<String, String> params = generateRequestParams(code);\n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(params, headers);\n        return restTemplate.postForEntity(googleTokenUri, request, GoogleTokenResponse.class).getBody();\n    }\n\n    private MultiValueMap<String, String> generateRequestParams(final String code) {\n        MultiValueMap<String, String> params = new LinkedMultiValueMap<>();\n        params.add(\"client_id\", googleClientId);\n        params.add(\"client_secret\", googleClientSecret);\n        params.add(\"code\", code);\n        params.add(\"grant_type\", \"authorization_code\");\n        params.add(\"redirect_uri\", googleRedirectUri);\n        return params;\n    }\n\n    private String getPayloadFrom(final String jwt) {\n        return jwt.split(JWT_DELIMITER)[1];\n    }\n\n    private String decodeJwtPayload(final String payload) {\n        return new String(Base64.getUrlDecoder().decode(payload), StandardCharsets.UTF_8);\n    }\n\n    private OAuthMember generateOAuthMemberBy(final String decodedIdToken) throws JsonProcessingException {\n        Map<String, String> userInfo = objectMapper.readValue(decodedIdToken, HashMap.class);\n        String email = userInfo.get(\"email\");\n        String displayName = userInfo.get(\"name\");\n        String profileImageUrl = userInfo.get(\"picture\");\n\n        return new OAuthMember(email, displayName, profileImageUrl);\n    }\n}\n```\n\n보통의 생각은 인터페이스인 `OAuthClient`와 구현체인 `GoogleOAuthClient`를 같은 패키지에 두려고 할 것이다. `GoogleOAuthClient`는 외부 의존성을 강하게 가지고 있기 때문에 `domain` 패키지와 별도로 관리하기 위한 `infrastructure` 패키지가 적합할 것이다. 결국 인터페이스인 `OAuthClient` 또한 `infrastructure`에 위치하게 될 것이다. 우리는 이러한 생각에서 벗어나 새로운 관점에서 살펴봐야 한다.\n\n앞서 언급한 의존성에 대해 생각해보자. 위 `OAuthClient`를 사용하는 주체는 누구일까? 우리는 이러한 주체를 `domain` 내에 인증을 담당하는 `auth` 패키지 내부의 `Authservice`로 결정 했다. 아래는 실제 `OAuthClient`를 사용하고 있는 주체인 `AuthService`이다.\n\n```java\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\n    private final OAuthEndpoint oAuthEndpoint;\n    private final OAuthClient oAuthClient;\n    private final MemberService memberService;\n    private final JwtTokenProvider jwtTokenProvider;\n\n    public AuthService(final OAuthEndpoint oAuthEndpoint, final OAuthClient oAuthClient,\n                       final MemberService memberService, final JwtTokenProvider jwtTokenProvider) {\n        this.oAuthEndpoint = oAuthEndpoint;\n        this.oAuthClient = oAuthClient;\n        this.memberService = memberService;\n        this.jwtTokenProvider = jwtTokenProvider;\n    }\n\n    public String generateGoogleLink() {\n        return oAuthEndpoint.generate();\n    }\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        String email = oAuthMember.getEmail();\n\n        if (!memberService.existsByEmail(email)) {\n            memberService.save(generateMemberBy(oAuthMember));\n        }\n\n        Member foundMember = memberService.findByEmail(email);\n        String accessToken = jwtTokenProvider.createToken(String.valueOf(foundMember.getId()));\n\n        return new TokenResponse(accessToken);\n    }\n\n    private Member generateMemberBy(final OAuthMember oAuthMember) {\n        return new Member(oAuthMember.getEmail(), oAuthMember.getProfileImageUrl(), oAuthMember.getDisplayName(), SocialType.GOOGLE);\n    }\n}\n```\n\n지금 까지 설명한 구조의 패키지 구조는 아래와 같다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   └── application\n    │   │               │       └── AuthService.java\n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │   │   └── client\n    │   │               │   │       ├── OAuthClient.java\n    │   │               │   │       └── GoogleOAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java     \n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n결국 이러한 구조는 아래와 같이 `domain` 패키지에서 `infrastructure`에 의존하게 된다.\n  \n```java\n...\nimport com.allog.dallog.infrastructure.dto.OAuthMember; // 의존성 발생!\nimport com.allog.dallog.infrastructure.oauth.client.OAuthClient; // 의존성 발생!\n...\n\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n### Separated Interface Pattern\n\n`분리된 인터페이스`를 활용하자. 즉 `인터페이스`와 `구현체`를 각각의 패키지로 분리한다. 분리된 인터페이스를 사용하여 `domain` 패키지에서 인터페이스를 정의하고 `infrastructure` 패키지에 구현체를 둔다. 이렇게 구성하면 인터페이스에 대한 종속성을 가진 주체가 구현체에 대해 인식하지 못하게 만들 수 있다.\n\n아래와 같은 구조로 인터페이스와 구현체를 분리했다고 가정한다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    │   │               │   │   ├── AuthService.java\n    │   │               │   │   └── OAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java         \n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │       └── client\n    │   │               │           └── GoogleOAuthClient.java\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n자연스럽게 `domain` 내에 있던 `infrastructure` 패키지에 대한 의존성도 제거된다. 즉 외부 서버와의 통신을 위한 의존성이 완전히 분리된 것을 확인할 수 있다.\n\n```java\n...\nimport com.allog.dallog.auth.dto.OAuthMember; // auth 패키지 내부를 의존\n...\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n## References.\n\n[Separated Interface](https://www.martinfowler.com/eaaCatalog/separatedInterface.htmlhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다. 지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를…","fields":{"slug":"/install-jenkins-with-docker-on-ec2/"},"frontmatter":{"date":"July 21, 2022","title":"EC2 환경에서 도커를 활용한 젠킨스 설치하기","tags":["DevOps","Jenkins"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다.\n\n지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를 **매번 실행해야한다는 단점**이 존재했습니다. 따라서 저희 팀은 메인 브랜치에 기능이 새로 병합 될 때마다 자동으로 감지하고, 스프링 어플리케이션을 `jar` 파일로 빌드하여 배포하는 환경이 필요하다고 느꼈습니다. 따라서 **CI/CD 도구를 도입**하기로 결정했습니다.\n\n이번 포스팅에서는 달록이 EC2 환경에서 도커를 사용하여 젠킨스를 설치한 방법에 대해서 정리합니다.\n\n## 젠킨스 도입 배경\n\n![유명한 CI/CD 도구들의 구글 트렌드 결과](./google-trends.png)\n\n달록은 CI/CD 도구로 **Jenkins**를 선정하였습니다. 위 사진은 시중에 배포되어있는 여러 CI/CD 도구의 구글 트렌드 분석 결과입니다. 파란색이 Jenkins 인데요, **압도적으로 높은 관심도**를 유지하고 있습니다.\n\n아무래도 달록팀 모두가 CI/CD에 익숙하지 않아 가장 사람들이 많이 사용하고, 그에 따라 **생태계가 넓고 레퍼런스가 많은 도구**를 선정하는 것이 좋다고 판단하였습니다. **레퍼런스가 많아** 초기 학습 비용이 적게 들고, 이슈가 발생했을때에도 **트러블슈팅이 비교적 쉽다**고 생각했습니다.\n\n## 도커\n\n달록팀은 EC2 인스턴스에 **도커를 사용하여** 젠킨스 컨테이너를 띄웠습니다. 도커를 사용하지 않고 젠킨스를 우분투에 직접 설치한다면 해주어야할 환경 설정이 가득합니다. 젠킨스를 돌리기 위한 JDK 설치, 젠킨스 설치, 젠킨스 포트 설정, 방화벽 설정 등등...\n\n하지만 도커를 사용하면 이런 **환경 설정 없이 간단한 명령어 몇가지로 젠킨스를 설치하고 서버에 띄울 수 있습니다.**\n\n도커는 서비스를 운용하는데 필요한 실행환경, 라이브러리, 소프트웨어, 코드 등을 컨테이너라는 단위로 가상화하는 컨테이너 기반 가상화 플랫폼입니다. 도커를 사용하면 EC2 인스턴스에는 미리 **도커라이징(Dockerizing)**된 이미지를 다운로드 받고 도커를 통해 실행하기만 하면되며, 해당 컨테이너가 어떤 환경을 필요로 하는지 전혀 알 필요가 없습니다.\n\n> 더 자세한 내용은 제가 작성한 [이론과 실습을 통해 이해하는 Docker 기초](https://hudi.blog/about-docker/)를 읽어보시면 좋을 것 같습니다 🤭\n\n### 우분투에 도커 설치\n\n> 달록팀은 EC2 t4g.micro 인스턴스에 우분투 22.04 (ARM 64) 환경을 사용하고 있습니다.\n\n> 아래 설치 방법은 [도커 공식 도큐먼트](https://docs.docker.com/engine/install/ubuntu/)에서 제공되는 내용입니다.\n\n#### 레포지토리 셋업\n\n아래 명령을 통해서 우분투의 `apt`의 패키지 인덱스를 최신화하고, `apt`가 HTTPS를 통해 패키지를 설치할 수 있도록 설정합니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n```\n\n#### 도커의 공식 GPG 키 추가\n\n```shell\n$ sudo mkdir -p /etc/apt/keyrings\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n```\n\n#### 레포지토리 셋업\n\n```shell\n$ echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n#### 도커 엔진 설치\n\n아래 명령을 실행하면 가장 최신버전의 도커 엔진이 설치됩니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n```\n\n#### 도커 설치 확인\n\n```shell\n$ sudo docker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n...\n```\n\n위 명령을 실행하여 위와 같이 `Hello from Docker!` 메시지가 출력되면 성공적으로 도커 설치가 완료된 것 입니다. 다음 단계로 넘어가볼까요? 🤗\n\n## 젠킨스 컨테이너 실행\n\n### 젠킨스 이미지 다운로드\n\n```shell\n$ docker pull jenkins/jenkins:lts\n```\n\n위 명령을 통해 Jenkins LTS(Long Term Support) 버전의 이미지를 다운로드 받습니다.\n\n### 젠킨스 컨테이너 띄우기\n\n```shell\n$ sudo docker run -d -p 8080:8080 -v /jenkins:/var/jenkins_home --name jenkins -u root jenkins/jenkins:lts\n```\n\n위 명령을 통해 다운로드 받은 젠킨스 이미지를 컨테이너로 띄울 수 있습니다. 사용된 각 옵션을 간단히 알아볼까요?\n\n- **-d** : 컨테이너를 **데몬**으로 띄웁니다.\n- **-p 8080:8080** : 컨테이너 외부와 내부 포트를 **포워딩**합니다. 좌측이 호스트 포트, 우측이 컨테이너 포트입니다.\n- **-v /jenkins:/var/jenkins_home** : 도커 컨테이너의 데이터는 **컨테이너가 종료되면 휘발**됩니다. 도커 컨테이너의 데이터를 보존하기 위한 여러 방법이 존재하는데, 그 중 한 방법이 **볼륨 마운트**입니다. 이 옵션을 사용하여 젠킨스 컨테이너의 `/var/jenkins_home` 이라는 디렉토리를 호스트의 `/jenkins` 와 마운트하고 데이터를 보존할 수 있습니다.\n- **--name jenkins** : 도커 컨테이너의 이름을 설정합니다.\n- **-u root** : 컨테이너가 실행될 리눅스의 사용자 계정을 root 로 명시합니다.\n\n### docker-compose 사용하기\n\n하지만 위와 같은 명령어를 모두 외우고 있다가, 도커 컨테이너를 실행할 때 마다 입력하게 된다면 굉장히 번거롭겠죠. 따라서 도커는 docker-compose 라는 것을 지원합니다. 도커 컴포즈는 여러 컨테이너의 실행을 한번에 관리할 수 있게 도와주는 도커의 도구입니다. 하지만 저희와 같이 하나의 컨테이너만 필요한 상황에서도 유용하게 사용할 수 있죠.\n\n```shell\n$ sudo apt install docker-compose\n```\n\n위 명령을 이용하여 `docker-compose` 를 설치합니다.\n\n그리고 도커를 실행할 경로에 `docker-compose.yml` 이란 파일을 만들고, 아래의 내용을 작성해줍니다.\n\n```yaml\nversion: \"3\"\nservices:\n  jenkins:\n    image: jenkins/jenkins:lts\n    user: root\n    volumes:\n      - ./jenkins:/var/jenkins_home\n    ports:\n      - 8080:8080\n```\n\n생성한 `docker-compose.yml` 이 존재하는 경로에서 아래의 명령을 실행하면 복잡한 명령 없이도 도커 컨테이너를 실행할 수 있습니다.\n\n```shell\n$ sudo docker-compose up -d\n```\n\n> -d 옵션은 컨테이너가 데몬으로 실행됨을 의미합니다.\n\n## 젠킨스 설정\n\n도커를 사용하여 젠킨스 컨테이너가 EC2 인스턴스에 성공적으로 띄워졌다면, EC2의 퍼블릭 IP를 통해 외부에서 접속할 수 있을 것 입니다. localhost:8080으로 접속하면 아래와 같은 화면이 보일 것 입니다.\n\n![](./unlock-jenkins.png)\n\n```shell\n$ sudo docker logs jenkins\n```\n\n위 명령을 사용하면, `jenkins` 컨테이너에 출력된 로그를 확인할 수 있습니다. 젠킨스를 최초로 설치하고 실행하면 사진에서 요구하는 initial admin password를 출력해주는데요, 로그를 확인해봅시다.\n\n```\n*************************************************************\n*************************************************************\n*************************************************************\n\nJenkins initial setup is required. An admin user has been created and a password generated.\n\nPlease use the following password to proceed to installation:\n\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n\nThis may also be found at: /var/jenkins_home/secrets/initialAdminPassword\n\n*************************************************************\n*************************************************************\n*************************************************************\n```\n\n위에서 표시된 `XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` 를 웹사이트에 넣어주시면 됩니다.\n\n혹은 아래의 명령으로 `jenkins` 컨테이너 내부에 직접 접속하여, `/var/jenkins_home/secrets/initialAdminPassword` 파일의 내용을 조회하는 방법도 있습니다.\n\n```shell\n$ sudo docker exec -it jenkins /bin/bash\n$ cat /var/jenkins_home/secrets/initialAdminPassword\n```\n\n![](./customize-jenkins.png)\n\n그 다음 위 화면에서 Install suggested plugins를 클릭하여 추천되는 플러그인을 설치합니다. 그 이후 요구되는 여러 정보의 입력을 끝 마쳐주세요.\n\n```\nSystem.setProperty('org.apache.commons.jelly.tags.fmt.timeZone\n```\n\n이후 대시보드에서 **Jenkins 관리 > Script Console** 에서 위의 스크립트를 입력하여 타임존을 서울로 설정하기만 하면, 젠킨스 기본 설정이 완료됩니다! 🎉\n\n## 마치며\n\n다음 포스팅에서는 달록이 젠킨스를 이용하여 어떻게 배포 자동화 프로세스를 구축하였는지 작성해보도록 하겠습니다. 많은 기대 부탁드립니다 🙏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정... 배…","fields":{"slug":"/deploy-automation-with-shell-script/"},"frontmatter":{"date":"July 19, 2022","title":"쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정...\n\n배포가 필요할때마다 이런 명령을 수동으로 일일히 입력한다면, 그건 너무 지루한 작업아닐까요? 😫 실수라도 하면 어쩌죠? 😰\n\n우테코 달록팀 백엔드는 이런 한계점을 극복하고자 쉘 스크립트를 활용하여 배포 프로세스를 자동화하였습니다. 달록팀은 어떻게 스프링부트 어플리케이션의 배포를 자동화했을까요?\n\n## 쉘 스크립트\n\n쉘 스크립트는 유닉스/리눅스 기반 운영체제에서의 일련의 명령으로 구성된 실행가능한 텍스트 파일입니다. 원래라면 일일히 키보드로 입력해야하는 리눅스 명령을 하나의 파일에 모아두고, 한번에 실행할 수 있죠. 작성된 명령은 셸이라고 불리는 명령줄 인터프리터에서 실행되며, 위에서부터 아래로 차례로 실행됩니다. 이 쉘 스크립트를 이용해 리눅스 환경에서 여러 프로세스를 자동화할 수 있습니다.\n\n달록이 스프링부트 어플리케이션을 배포하는 환경은 Ubuntu 22 버전이므로 쉘 스크립트를 활용할 수 있습니다.\n\n## 수동으로 배포하기\n\n배포 프로세스를 자동화하려면 우선 수동으로 어떤 명령을 사용해서 배포를 하는지 알아야합니다. 어떤과정을 거쳐 배포되는지부터 알아볼까요?\n\n> 레포지토리는 이미 Clone 되어있다고 가정합니다.\n\n### 1. Git Pull\n\n```shell\n$ cd 2022-dallog/backend\n$ git pull\n```\n\n우선 Github에서 가장 최신 버전을 pull 해와야겠죠?\n\n### 2. 빌드\n\n```shell\n./gradlew bootJar\n```\n\n`gradlew` 를 사용하여 자바 프로젝트를 빌드해서 `.jar` 파일을 생성합니다.\n\n### 3. 프로세스 종료\n\n```shell\n$ ps -ef | grep jar\n$ kill -15 XXXXX\n```\n\n`ps` 명령을 사용해서 실행중인 스프링부트 어플리케이션의 PID를 알아내고, `kill` 명령을 통해 프로세스를 종료합니다.\n\n### 4. 환경변수 설정\n\n달록의 스프링부트 어플리케이션은 여러 민감한 정보를 환경변수를 사용하여 외부에 노출되지 않도록 하였습니다. 따라서 어플리케이션이 실행될 때 환경변수도 함께 설정을 해주어야합니다.\n\n```shell\n$ export GOOGLE_CLIENT_ID=\"XXXXX\"\n$ export GOOGLE_CLIENT_SECRET=\"XXXXX\"\n$ export GOOGLE_REDIRECT_URI=\"XXXXX\"\n$ export GOOGLE_TOKEN_URI=\"XXXXX\"\n$ export JWT_SECRET_KEY=\"XXXXX\"\n$ export JWT_EXPIRE_LENGTH=3600\n\n...\n```\n\n### 5. 드디어 실행\n\n```shell\n$ sudo -E nohup java -jar ./build/libs/backend-0.0.1-SNAPSHOT.jar\n```\n\n드디어 스프링부트 어플리케이션을 실행합니다.\n\n이 귀찮은 과정을 배포 할때마다 해야한다니 벌써 머리가 어질어질 하네요. 😵‍💫 그렇다면 앞서 소개드린 쉘 스크립트를 통해서 이 과정을 자동화해볼까요?\n\n## 달록의 배포 쉘 스크립트\n\n```shell\n#! /bin/bash\n\nPROJECT_PATH=/home/ubuntu/2022-dallog\nPROJECT_NAME=backend\nPROJECT_BUILD_PATH=backend/build/libs\n\ncd $PROJECT_PATH/$PROJECT_NAME\n\nclear\n\necho \"🌈 Github에서 프로젝트를 Pull 합니다.\\n\"\n\ngit pull\n\necho \"\\n🌈 SpringBoot 프로젝트 빌드를 시작합니다.\\n\"\n\n./gradlew bootJar\n\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXX\"\nexport JWT_SECRET_KEY=\"XXXXX\"\nexport JWT_EXPIRE_LENGTH=3600\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n달록이 작성한 배포 자동화 쉘 스크립트는 아래와 같습니다. 차근차근 알아볼까요?\n\n### #! /bin/bash\n\n`#! /bin/bash` 은 해당 쉘 스크립트가 많은 쉘 중 **Bash Shell 로 실행됨**을 알립니다.\n\n### 변수 사용\n\n`PROJECT_PATH`, `PROJECT_NAME` 과 같이 자주 사용되는 데이터는 쉘 스크립트에서 제공하는 변수 기능으로 분리하였습니다. 이때 주의할 점은 쉘 스크립트에서 변수를 선언할 때 `=` **앞뒤에 공백이 와서는 안된다는 점** 입니다.\n\n### echo\n\n`echo` 명령을 통해 배포 프로세스가 어디까지 진행됐는지 사용자에게 알려줍니다.\n\n### 실행중인 어플리케이션의 PID 가져오기\n\n```shell\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n```\n\n#### pgrep\n\n쉘 스크립트 중 위와 같은 코드가 있었습니다. 위 코드는 우선 `pgrep` 이라는 명령을 통해서 실행중인 프로세스의 이름으로 PID 목록을 가져옵니다.\n\n#### pipe와 head\n\n그리고 파이프(`|`)명령으로 다른 프로세스로 PID 목록을 보냅니다. PID 목록은 `head` 명령으로 전달되며, `head` 명령은 PID 목록의 첫번째만을 가져옵니다.\n\n#### 명령의 실행결과를 변수에 담기\n\n이렇게 가져온 PID는 `$()` 문법을 통해 `CURRENT_PID` 변수에 저장됩니다. `$()` 는 `$(command)` 형태로 사용되며, 괄호 내부의 실행 결과를 변수로 저장하기 위해 사용됩니다.\n\n### 조건문\n\n쉘 스크립트에도 `if` 문을 사용하여 조건문을 작성할 수 있습니다. 다만, 우리에게 익숙한 프로그래밍 언어에서의 if문과는 조금 괴리가 존재해서 별도로 학습이 필요할수도 있습니다.\n\n```shell\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n```\n\n위 코드는 아까 PID를 담은 `CURRENT_PID` 가 비어있는지 확인한 후 존재하지 않다면 메시지만 출력하고, 존재한다면 해당 PID를 `kill` 명령으로 종료합니다.\n\n쉘 스크립트의 `if` 문에서 `-z` 는 조건식의 종류 중 하나이며, 주어진 문자열의 길이가 0이라면 True를 나타냅니다. 확실히 조금 낯설죠? 😅\n\n### JAR파일 경로 가져오기\n\n```shell\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\n```\n\n`ls` 명령을 통해 빌드 디렉토리의 파일 목록을 가져오고, `grep` 명령을 통해 `.jar` 파일만을 가져옵니다. 그다음 `head` 명령을 통해 단 하나의 파일만을 가져온 다음, `JAR_PATH` 변수에 저장합니다.\n\n### 어플리케이션 실행하기\n\n```shell\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n어플리케이션을 실행합니다.\n\n#### sudo -E\n\n`sudo` 명령 뒤에 붙은 `-E` 옵션은 유저가 설정한 환경변수를 `sudo` 명령에서도 공유하여 사용할 수 있도록 만드는 옵션입니다.\n\n#### nohup\n\n`nohup` 명령은 현재 **터미널 세션이 끊어져도 프로세스가 계속 살아있도록** 만들기 위해 사용되는 명령입니다.\n\n#### Background 프로세스\n\n그리고 명령 맨 뒤에 `&` 가 붙어있는데, 프로세스를 Foreground가 아닌 **Background에서 실행**하기 위해 붙여줍니다.\n\n## 한계점\n\n하지만, 이런 방식도 결국 한계점이 존재합니다. 특히나 달록과 같이 애자일한 조직에서는 최대한 작은 기능단위로 개발이 병렬적으로 진행되어, 메인 브랜치에 머지됩니다. 하루에 몇번이고 배포를 해야하는 상황이 발생할수도 있죠.\n그렇지 않아도 모든 개발자가 바쁘게 새로운 기능을 개발하기 바쁜데, 메인 브랜치에 병합된 시점마다 EC2 인스턴스에 접속해서 쉘 스크립트를 실행해야할까요?\n\n이런 한계점을 극복하고자, 달록팀은 앞으로 CI/CD 도구를 도입할 예정입니다. 세상에는 참 다양한 CI/CD 도구가 존재합니다. Jenkins, Github Actions, Travis CI, Circle CI, Gitlab CI/CD 등등...\n\n달록의 이번 스프린트의 배포 태스크에서는 이런 다양한 CI/CD 도구들의 장단을 분석하고 도입할 예정입니다. 많은 기대 부탁드립니다. 👏👏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다. 모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면…","fields":{"slug":"/data-jpa-auditing/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Auditing","tags":["Spring","Data JPA","Auditing"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n> auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다.\n\n모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면 모든 엔티티에 중복이 생기고 유지보수가 어려워진다. Spring Data JPA가 제공하는 `Auditing` 기능을 사용하면 이런 기능을 쉽고 빠르게 구현할 수 있다.\n\n## Spring Data JPA Auditing 적용하기\n\n`Auditing`을 적용하기 위해서는 우선 어노테이션을 적용해야 한다. `@Configuration` 어노테이션이 적용된 Config 클래스에 아래와 같이 `@EnableJpaAuditing`을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing\npublic class JpaConfig {\n}\n```\n\n⚠️ 주의 : 원활한 Slice 테스트를 위해 @SpringBootApplication 과 @EnableJpaAuditing 어노테이션 분리하기\n\n[https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing](https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing)\n\n### Spring Entity Callback Listener 적용하기\n\nAuditing entity listener class로 지정하기 위해 `@EntityListeners` 어노테이션을 Entity 클래스에 추가한다. 인자로는 `AuditingEntityListener.class`를 넘긴다. 이 설정을 통해 엔티티에 이벤트가 발생했을 때 정보를 캡처할 수 있다.\n\n```java\n@Entity\n@EntityListeners(AuditingEntityListener.class)\npublic class BaseEntity {\n    // ...\n}\n```\n\n### 생성일시, 수정일시 추적하기\n\n생성일시는 `@CreatedDate`, 수정일시는 `@LastModifiedDate` 어노테이션을 통해 추적할 수 있다. 생성일시의 경우 한 번 생성되면 변경되어선 안 되며, 항상 존재해야 하므로 `nullable = false`, `updatable = false`로 지정한다.\n\n추가적으로 BaseEntity를 다른 Entity들이 상속받아 사용할 수 있도록 `@MappedSuperclass` 어노테이션을 통해 해당 클래스를 Entity가 아닌 SuperClass로 지정했다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedDate\n    @Column(name = \"created_at\", nullable = false, updatable = false)\n    private LocalDateTime createdAt;\n\n    @LastModifiedDate\n    @Column(name = \"updated_at\")\n    private LocalDateTime updatedAt;\n\n    // ...\n}\n```\n\n### 생성한 사람, 수정한 사람 추적하기\n\n생성한 사람은 `@CreatedBy`, 수정한 사람은 `@LastModifiedBy` 어노테이션을 통해 추적할 수 있다. 해당 필드는 생성자, 수정자의 이름으로 채워진다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedBy\n    @Column(name = \"created_by\")\n    private String createdBy;\n\n    @LastModifiedBy\n    @Column(name = \"modified_by\")\n    private String modifiedBy;\n\n    // ...\n}\n```\n\n유저에 대한 정보는 SecurityContext's Authentication 인스턴스로부터 가져온다. 이 값을 커스텀하고 싶다면 `AuditorAware<T>` 인터페이스를 구현해야 한다.\n\n```java\npublic class AuditorAwareImpl implements AuditorAware<String> {\n\n    @Override\n    public String getCurrentAuditor() {\n        // your custom logic\n    }\n}\n```\n\n이렇게 만든 `AuditorAwareImpl`를 사용하려면 Config 클래스에 `AuditorAwareImpl` 인스턴스로 초기화되는 `AuditorAware` 타입의 빈을 설정해주어야 한다. 그리고 `@EnableJpaAuditing` 어노테이션에 `auditorAwareRef=\"auditorProvider\"` 설정을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing(auditorAwareRef=\"auditorProvider\")\npublic class JpaConfig {\n    //...\n\n    @Bean\n    AuditorAware<String> auditorProvider() {\n        return new AuditorAwareImpl();\n    }\n\n    //...\n}\n```\n\n---\n\n### References\n\n[https://www.baeldung.com/database-auditing-jpa](https://www.baeldung.com/database-auditing-jpa)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. Spring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자. 메서드를 보면 파라미터로  객체를 받는다.  객체는 Pagination을 위한 정보를 저장하는 객체다.  인터페이스의 구현체인 의 인스턴스를 생성하여…","fields":{"slug":"/data-jpa-slice-page/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Slice & Page","tags":["Spring","Data JPA","Slice","Page"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\nSpring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n\n// Service\nPageRequest pageRequest = PageRequest.of(page, size); // pageRequest 생성\nSlice<Member> slices = memberRepository.findSliceBy(pageRequest); // repository에서 페이지 가져오기\nPage<Member> pages = memberRepository.findPageBy(pageRequest);\n```\n\n메서드를 보면 파라미터로 `Pageable` 객체를 받는다. **`Pageable` 객체는 Pagination을 위한 정보를 저장하는 객체**다. `Pageable` 인터페이스의 구현체인 `PageRequest`의 인스턴스를 생성하여 메서드 인자로 넘겨줄 수 있다.\n\n![PageRequest static constructor](pagerequest.png)\n\n`PageRequest`는 정적 팩토리 메서드 `of`를 사용하여 인스턴스를 생성할 수 있다. `PageRequest`는 인자로 page, size, 필요하다면 sort까지 받을 수 있다.\n\n- `page` : 0부터 시작하는 페이지 인덱스 번호\n- `size` : 한 페이지에 반환할 데이터의 개수\n- `sort` : 정렬 방식\n\npage와 size를 쿼리 파라미터로 받아 `PageRequest`를 생성하여 Repository 메서드에 넘겨주는 것으로 간단하게 Pagination을 구현할 수 있는 것이다.\n\n## Slice VS Page\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n```\n\nRepository의 메서드를 보면 반환 값으로 `Slice` 혹은 `Page`를 받을 수 있다. 둘은 어떤 차이가 있을까?\n\n### Slice\n\n![slice](slice.png)\n\n`Slice`는 `Streamable`을 상속받는 인터페이스로 Pagination과 관련된 여러 메서드를 갖고 있다. 대표적인 메서드 몇 가지만 살펴보자.\n\n![slice methods](slice_methods.png)\n\n현재 페이지의 내용을 확인하거나 다음 페이지, 이전 페이지에 대한 정보를 가져올 수 있다. 그렇다면 `Page`는 무엇일까?\n\n### Page\n\n![page](page.png)\n\n`Page`는 `Slice`를 상속한다. 따라서 `Slice`가 가진 모든 메서드를 `Page`도 사용할 수 있다. 다만 `Page`가 다른 점은 **조회 쿼리 이후 전체 데이터 개수를 조회하는 쿼리가 한 번 더 실행된다는 것**이다.\n\nPage가 추가적으로 구현하고 있는 메서드 두 가지만 살펴보자.\n\n![page methods](page_methods.png)\n\nPage의 경우 전체 데이터 개수를 조회하는 쿼리가 추가적으로 실행되므로 Slice와 다르게 전체 데이터 개수나 전체 페이지 수까지 확인할 수 있다.\n\n> 추가적으로..\n\n```java\npublic interface PagingAndSortingRepository<T, ID> extends CrudRepository<T, ID> {\n\n    Iterable<T> findAll(Sort sort);\n\n    Page<T> findAll(Pageable pageable);\n}\n```\n\n`Page`를 반환받을 경우 아래처럼 **`Page`를 반환하는 `findAll` 메서드가 이미 존재**한다. 따로 Repository Interface에 메서드를 추가해주지 않아도 `findAll` 메서드에 `Pageable` 객체를 넘겨주면 Pagination을 사용할 수 있다. (Slice를 반환하려면 메서드를 정의해주어야 한다.)\n\n### Slice VS Page 어떤 걸 사용해야 할까?\n\n`Slice`는 전체 데이터 개수를 조회하지 않고 이전 or 다음 `Slice`가 존재하는지만 확인할 수 있다. 따라서 **`Slice`는 무한 스크롤 등을 구현하는 경우 유용**하다. `Page`에 비해 쿼리가 하나 덜 날아가므로 **데이터 양이 많을수록 `Slice`를 사용하는 것이 성능상 유리**하다.\n\n`Page`는 전체 데이터 개수를 조회하는 쿼리를 한 번 더 실행한다. 따라서 **전체 페이지 개수나 데이터 개수가 필요한 경우 유용**하다.\n\n> 알록에서는 카테고리 조회를 무한 스크롤로 구현하므로 Slice를 사용했다.\n\n## 컨트롤러에서 queryParameter를 Pageable 객체로 받는 방법\n\n앞에선 queryParameter를 통해 page, size를 받아 PageRequest를 만들어 넘겨주는 방법을 설명했다.\n\n```java\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n```\n\n하지만 이렇게 구현하니까 *“ModelAttribute를 통해 queryParameter를 DTO로 받는 것처럼, queryParameter를 Pageable 객체로 받을 수 없을까?”*하는 의문이 생겼다. 직접 ArgumentResolver를 구현할 뻔했지만 찾아보니 역시 똑똑한 JPA.. Pageable 객체를 인수로 설정하면 어노테이션 없이도 자동으로 객체를 만들어준다. `PageableHandlerMethodArgumentResolver` 가 이미 구현되어 있기 때문이다.\n\n```java\n// Controller\n// Pageable 객체를 바로 받을 수 있다.\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(Pageable pagealbe) {\n    // 생략\n}\n\n// Service\nSlice<Member> slices = memberRepository.findSliceBy(pageable); // PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있다.\n```\n\n이렇게 사용하면 PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있어 훨씬 편하다. 하지만 문제가 한 가지 발생한다. **프론트에서는 page index를 1부터 계산하는 것과 달리 Pageable은 page index를 0부터 계산한다.** 따라서 page의 인덱스를 1부터 시작하도록 설정할 필요가 있다.\n\n## page 인덱스를 1부터 시작하도록 설정하기\n\n### 방법 1. PageableHandlerMethodArgumentResolverCustomizer 커스터마이징 하기\n\n```java\n@Configuration\npublic class CustomPageableConfiguration {\n    @Bean\n    public PageableHandlerMethodArgumentResolverCustomizer customize() {\n        return p -> p.setOneIndexedParameters(true);\n    }\n}\n```\n\n따로 `CustomPageableConfiguration` 클래스를 만들어 `PageableHandlerMethodArgumentResolverCustomizer`를 커스터마이징 해주는 방법이 있다. 여기서 `setOneIndexedParameters` 메서드를 통해 인덱스를 1부터 시작하게 설정할 수 있다.\n\n### 방법 2. application-properties에 설정 추가하기\n\n```java\nspring.data.web.pageable.one-indexed-parameters=true\n```\n\n### 유의점\n\n이렇게 인덱스를 1부터 시작하도록 설정하더라도 **이후에 반환받은 `Slice`, `Page` 객체에서는 page 인덱스가 다시 0부터 시작**한다. 따라서 **`getNumber` 등의 메서드를 통해 page 번호를 받으면 -1씩 차이가 난다는 사실을 잊어선 안 된다.** 유의해서 사용하자.\n\n### References\n\n[https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/](https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/)\n\n[https://treasurebear.tistory.com/59](https://treasurebear.tistory.com/59)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. 달록에 적절한 패키지 구조 고민하기 우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 , 로 나눌 수 있다. 계층별 패키지 구조 계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로…","fields":{"slug":"/package-structure/"},"frontmatter":{"date":"July 18, 2022","title":"달록에 적절한 패키지 구조 고민하기","tags":["패키지 구조"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## 달록에 적절한 패키지 구조 고민하기\n\n우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 `계층별`, `기능별`로 나눌 수 있다.\n\n### 계층별 패키지 구조\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── application\n    │   │               ├── config\n    │   │               ├── domain\n    │   │               ├── dto\n    │   │               ├── exception\n    │   │               ├── presentation\n    │   │               └── DallogApplication.java\n    │   └── resources\n    │       └── application.yml\n```\n\n계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로젝트에 대한 이해도가 낮아도 각 계층에 대한 역할을 충분히 숙지하고 있다면 전체적인 구조를 빠르게 파악할 수 있다.\n\n하지만 단점도 존재한다. 하나의 패키지에 모든 클래스들이 모이게 되기 때문에 규모가 커지면 클래스의 개수가 많아져 구분이 어려워진다. 아래는 이전에 계층형 구조를 기반으로 작성한 프로젝트이다.\n\n```json\n└── presentation\n    ├── ChampionController.java\n    ├── CommentController.java\n    ├── PlaylistController.java\n    ├── RankingController.java\n    ├── SearchController.java\n    ├── UserController.java\n    ├── WardController.java\n    └── ...\n```\n\n비교적 적은 코드의 양이지만 규모가 커질수록 애플리케이션에서 presentation 계층에 해당하는 모든 객체가 해당 패키지에 모이게 될 것이다.\n\n### 기능별 패키지 구조\n\n기능별로 패키지를 나눠 구성한다. 기능별 패키지 구조의 장점은 해당 도메인에 관련된 코드들이 응집되어 있다는 점이다. 덕분에 `지역성의 원칙`를 잘 지킬 수 있다고 한다.\n\n> 컴퓨터 과학에서, 참조의 지역성, 또는 지역성의 원칙이란 프로세서가 짧은 시간 동안 동일한 메모리 공간에 반복적으로 접근하는 경향을 의미한다. 참조 지역성엔 두 가지 종류가 있다. 바로 시간적 지역성과 공간적 지역성이다. 시간적 지역성이란 특정 데이터 또는 리소스가 짧은 시간 내에 반복적으로 사용되는 것을 가리킨다. 공간적 지역성이란 상대적으로 가까운 저장 공간에 있는 데이터 요소들이 사용되는 것을 가리킨다.\n\n개발자 역시 복잡하고 거대한 프로젝트의 전체 구조를 모두 인지하는 것은 힘든일이다. 우선 특정 지역의 흐름을 파악할 수 있다면 해당 패키지에 대해서는 마치 캐시에 적재된 데이터에 접근 하듯 빠르게 인지가 가능하다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── category\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── schedule\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n하지만 각 계층이 기능별로 모여 있기 때문에 프로젝트에 대한 이해도가 낮으면 전체 구조를 파악하는데 오랜 시간이 걸린다.\n\n### 더 나아가기\n\n현재 구조에서는 도메인에 해당하는 다양한 기능들이 패키지 전반적으로 퍼져 있다. 각각의 도메인 기능이 밀집되어 있지 않은 구조를 가지고 있다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── domain\n    │   │               │   ├── auth\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   ├── category\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   └── schedule\n    │   │               │       ├── application\n    |   |               │       ├── domain\n    │   │               │       ├── dto\n    │   │               │       ├── exception\n    │   │               │       └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.properties\n```\n\n### domain\n\n실제 애플리케이션의 핵심이 되는 도메인 로직이 모여 있다. 애플리케이션의 주요 비즈니스 로직이 모여 있기 때문에 `외부와의 의존성을 최소화`해야 한다. 즉 `외부의 변경에 의해 도메인 내부가 변경되는 것을 막아야 한다는 것`을 인지해야 한다.\n\n### global\n\n`global`은 프로젝트 전반에서 사용하는 객체로 구성한다. 공통적으로 사용하는 dto나 error, config에 대한 것들이 모여 있다.\n\n### infrastructure\n\n`infrasturcture`는 외부와의 통신을 담당하는 로직들이 담겨 있다. 이번 프로젝트에서는 OAuth를 활용한 회원 관리를 진행하기 때문에 google의 인증 서버와 통신이 필요해진다. 이 패키지는 우리의 의지와 다르게 외부의 변화에 따라 변경될 여지를 가지고 있다. 즉 변화에 매우 취약한 구조이며 외부 서버에 의존적 이기 때문에 항시 변화에 대응할 수 있도록 대비해야 한다. 이것이 의미하는 바는 결국 `도메인 관련 패키지에서 infrastructure를 직접적으로 의존하는 것`은 도메인 로직을 안전하게 지킬 수 없다는 의미를 내포한다.\n\n## 정리\n\n각각의 방법은 서로 다른 장단점을 가지고 있기 때문에 정답은 없다고 생각한다. 현재 프로젝트의 규모와 요구사항을 고려하여 선택해야 한다. 다만 선택한 패키지 구조에 충분한 근거를 가져야하고, 객체와 패키지 사이의 의존성에 대해 충분히 고민해야 한다.\n\n달록은 현재 기능별 패키지 구조로 진행되고 있다. 모든 팀원들이 처음 부터 기획과 설계에 대한 고민을 진행했기 때문에 프로젝트의 구조에 대해 분석 하는 시간이 불필요했기 때문이다. 하지만 각각의 기능들이 난잡하게 퍼져 있기 때문에 [더 나아가기](#더-나아가기)에서 언급한 것 처럼 좀 더 밀접한 기능들을 모아둘 필요가 있다고 판단한다. 이것은 추후 팀원들과 충분한 논의를 통해 개선해갈 예정이다.\n\n## References.\n\n[Spring Guide - Directory](https://cheese10yun.github.io/spring-guide-directory)<br>\n[지역성의 원칙을 고려한 패키지 구조: 기능별로 나누기](https://ahnheejong.name/articles/package-structure-with-the-principal-of-locality-in-mind)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. 🎯 \"무한 스크롤을 구현해보세요!\" 어떻게 구현하실 건가요? 무한 스크롤을 처음 마주했을때 🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요. scroll event 사용하기 우테코 레벨1 유튜브 미션 처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다. 바로 스크롤 이벤트와 of…","fields":{"slug":"/infinite-scroll/"},"frontmatter":{"date":"July 18, 2022","title":"React에서 무한 스크롤 구현하기","tags":["react"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\n🎯 \"무한 스크롤을 구현해보세요!\"\n\n어떻게 구현하실 건가요?\n\n## 무한 스크롤을 처음 마주했을때\n\n🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요.\n\n```\n1. scroll 이벤트를 감지한다.\n\n2. 현재 스크롤 영역의 `위치를 계산`한다.\n\n3. 영역 계산을 통해 페이지 아래에 위치하면 API 요청을 진행한다.\n\n4. 받아온 데이터를 추가하여 다시 렌더링한다.\n\n5. 무한 반복\n```\n\n---\n\n## scroll event 사용하기\n\n[우테코 레벨1 유튜브 미션](https://github.dev/jhy979/javascript-youtube-classroom/tree/jhy979-step2)\n\n처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다.\n\n바로 스크롤 이벤트와 offset을 이용한 방식이죠!\n\n```js\n  scrollToBottom(callback) {\n    const isScrollBottom =\n      this.$videoList.scrollHeight - this.$videoList.scrollTop <=\n      this.$videoList.offsetHeight + EVENT.SCROLL.OFFSET;\n\n    if (isScrollBottom) {\n      callback(this.$searchInput.value);\n    }\n  }\n```\n\n메서드 네이밍을 통해서도 알 수 있듯이, 화면 하단까지 내려갔을 경우 (offset 정도를 감안하여) 인자로 받은 함수를 실행시켜주었습니다.\n\n아 물론, 스크롤 이벤트는 워낙 많이 발생하기 때문에 throttle을 걸어주었습니다. (이건 필수죠)\n\n😢 하지만, `documentElement.scrollTop`, `documentElement.scrollHeight`, `documentElement.offsetHeight`는 리플로우(Reflow)가 발생합니다.\n\n확실히 비효율적이겠죠!\n\n---\n\n## IntersectionObserver 사용하기\n\n달록에서는 무한 스크롤을 구현할 때 [Intersection Observer](https://developer.mozilla.org/ko/docs/Web/API/Intersection_Observer_API)를 사용했습니다.\n\n> Intersection Observer는 쉽게 말해 지정한 대상이 화면에 보이는지 감시하고 판단하는 도구입니다.\n\n브라우저 Viewport와 Target으로 설정한 요소의 교차점을 관찰하여 그 Target이 Viewport에 포함되는지 구별하는 기능을 제공합니다.\n\n<img src=\"https://velog.velcdn.com/images/jhy979/post/19500233-65fc-4ba9-b421-81516700c00b/image.png\" />\n\n### useIntersect 커스텀훅\n\n> 가장 먼저 useIntersect 라는 커스텀훅을 제작했습니다.\n\n이 커스텀훅은 `인자로 intersect시 실행할 함수`를 받고 `ref를 제공`하여 관찰할 대상을 지정할 수 있습니다.\n\n```ts\ntype IntersectHandler = (\n  entry: IntersectionObserverEntry,\n  observer: IntersectionObserver\n) => void;\n\n// 인자로 onIntersect와 options를 받습니다.\n// onIntersect는 intersect 발생 시 실행하고 싶은 함수입니다.\nfunction useIntersect(\n  onIntersect: IntersectHandler,\n  options?: IntersectionObserverInit\n) {\n  // 관찰하고 싶은 친구를 잡기 위해 ref를 만들어주세요.\n  const ref = useRef<HTMLDivElement>(null);\n\n  // intersect 시 실행할 함수를 만들어줍시다.\n  const callback = useCallback(\n    (entries: IntersectionObserverEntry[], observer: IntersectionObserver) => {\n      entries.forEach((entry) => {\n        if (entry.isIntersecting) {\n          onIntersect(entry, observer);\n        }\n      });\n    },\n    [onIntersect]\n  );\n\n  // 🔨 옵저버에게 일을 시켜봅시다.\n  useEffect(() => {\n    // 우리가 관찰하고 싶은 친구가 없으면 그냥 return 해버려요.\n    if (!ref.current) {\n      return;\n    }\n\n    // 관찰할 대상이 있으면 옵저버 데꼬 와야죠!\n    const observer = new IntersectionObserver(callback, options);\n\n    // 이 옵저버한테 감시를 시킵시다.\n    observer.observe(ref.current);\n\n    // 할 일 끝나면 고생한 옵저버도 쉬게 해줍시다!\n    return () => {\n      observer.disconnect();\n    };\n  }, [ref, options, callback]);\n\n  return ref;\n}\n\nexport default useIntersect;\n```\n\n### 실제 사용\n\nuseIntersect 커스텀훅을 잘 만들었으니 이제 이 커스텀훅을 무한 스크롤에 사용해 봅시다.\n\n![](https://velog.velcdn.com/images/jhy979/post/4643727c-852d-4f23-ab4f-44ce79e2e3b2/image.gif)\n\n다음은 카테고리 목록을 계속 불러와 리스트를 보여주는 컴포넌트입니다.\n\n```tsx\nfunction CategoryList({\n  categoryList,\n  getMoreCategories,\n  hasNextPage,\n}: CategoryListProps) {\n  // useIntersect 커스텀훅의 인자로 (교차 시) 실행할 함수를 넣어줍시다.\n  const ref = useIntersect(() => {\n    hasNextPage && getMoreCategories();\n  });\n\n  return (\n    <div css={categoryTable}>\n      <div css={categoryTableHeader}>\n        <span> 생성 날짜 </span>\n        <span> 카테고리 이름 </span>\n      </div>\n      {categoryList.map((category) => (\n        <CategoryItem key={category.id} category={category} />\n      ))}\n      // 페이지 하단까지 내리면 이 친구가 등장하여 옵저버에게 감지될 거예요.\n      <div ref={ref} css={intersectTarget}></div>\n    </div>\n  );\n}\n```\n\n💪 무한 스크롤함에 따라 props로 받아오는 categoryList가 길어지게 될텐데요, 다행히 React에서는 key값으로 변경 여부를 확인하기 때문에 새롭게 추가된 리스트들만 리렌더링해주었습니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. Git-flow git 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다. 이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답…","fields":{"slug":"/git-branch-strategy/"},"frontmatter":{"date":"July 12, 2022","title":"달록팀의 git 브랜치 전략을 소개합니다.","tags":["git","git-flow"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## Git-flow\n\ngit 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다.\n\n이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답이 될 수 없고, 지속적으로 개선할 예정이다.\n\n## Git Repository\n\n프로젝트에 적용하기 앞서 어떠한 형태로 Git Repository가 구성되는지 살펴보았다.\n\n![](./git-repository.png)\n\n### Upstream Remote Repository\n\n개발자가 공유하는 저장소로 최신 소스코드가 저장되어 있는 원격 저장소이다.\n\n#### 적용하기\n\n이러한 Remote Repository 생성을 위하여 github에 New organization을 사용히였다.\n\n![](./new-organization.png)\n\n다양한 기능을 제공하는 Team과 Enterprice는 월마다 일정 금액을 사용해야 한다. 하지만 간단한 프로젝트 진행을 위해 생성하였기 때문에 Free만 사용하여도 충분한 실습과 프로젝트를 진행할 수 있다.\n\n![](./fare.png)\n\norganization을 생성하게 되면 소속된 repository를 생성할 수 있다. 이것을 `Upstream Remote Repository`로 적용한다.\n\n### A's, B's, C's Origin Remote Repository\n\n`Upstream Repository`를 Fork한 원격 개인 저장소이다. Upstream Repository를 직접 clone하여 작업하는 것이 아니라 각각의 팀원들이 `Fork`를 하여 원격 저장소를 생성하고 그것을 clone하여 `Local Repository`를 생성하여 작업한다.\n\n이렇게 두 개의 remote repository로 나눈 이유는 Upstream repository의 경우 `팀원이 공유`하고 있는 Repository이기 때문에 다양한 시도를 하기에 큰 위험 부담을 가지고 있다. 각자의 개인 repository에서 `작업을 시도`한 후 적절한 기능 merge 하기 위해 `Pull Request`를 요청한다.\n\n### 운영 방식\n\ngit-flow는 기본적으로 5가지의 브랜치를 사용하여 운영한다.\n\n- `main`: 제품으로 출시될 수 있는 브랜치\n- `develop`: 다음 출시 버전을 개발하는 브랜치\n- `feature`: 기능을 개발하는 브랜치\n- `release`: 이번 출시 버전을 준비하는 브랜치\n- `hotfix`: 출시 버전에서 발생한 버그를 수정하는 브랜치\n\n![](./git-flow-dev.png)\n\n`main`과 `develop` 브랜치이다. 두 브랜치는 항시 운영되어야 하는 브랜치이다. `develop`는 개발을 위한 브랜치이고, `main`은 제품으로 출시될 수 있는 브랜치 이기 때문에 `항시 배포 가능한 상태`이어야 한다.\n\n`main`과 `develop`은 `Upstream remote repository`에서 운영한다.\n\n![](./git-flow-feature.png)\n\n`feature` 브랜치는 단위 기능을 개발하는 브랜치이다. 기능 개발이 완료되면 `develop` 브랜치와 `merge` 된다.\n\n`develop`은 모든 팀원이 `공유`하는 브랜치이다. feature는 각자 맡아 작성한 코드들이 들어 있는 브랜치이다. merge 작업 전에 팀원들 간의 `지속적인 코드 리뷰`가 필요하다.\n\n그렇기 때문에 `Pull Request`를 사용하여 `merge` 작업 전 리뷰어들에게 코드 리뷰를 받고 반영 사항을 수정하여 commit 후 merge 한다. 이 과정은 `협업에서 가장 중요한 부분`이라고 생각된다.\n\n![](./git-flow-release.png)\n\n`release` 브랜치는 배포를 하기 전에 충분한 검증을 위해 생성하는 브랜치이다. 배포 가능한 상태가 되면 `main` 브랜치로 `merge` 작업을 거친다. 또한 `develop`에도 반영사항을 모두 `merge` 시켜야 한다.\n\n![](./git-flow-hotfix.png)\n\n`hotifx` 브랜치는 배포 중 버그가 생겨 긴급하게 수정해야 하는 브랜치이다. 배포 이후에 이루어지는 브랜치이고, 반영 사항을 `main`과 `develop`에 모두 적용 시켜야 한다.\n\n앞서 말했듯이 `main`과 `develop`는 항시 운영되는 브랜치이다. 이 둘을 제외한 나머지 브랜치 들은 제 역할이 마무리 되어 `merge` 작업이 완료되면 브랜치를 삭제하여 정리한다.\n\n### 간단히 적용해보기\n\n`Upstream Remote Repository`를 기반으로 원격 개인 저장소에 `Fork` 해야 한다.\n\n![](./fork.png)\n\nOrganization에 생성한 repository에 Fork를 누르면 손쉽게 할 수 있다. Fork로 생성된 repository를 기반으로 `Local Repository`를 생성해야 한다.\n\n```bash\ngit clone https://github.com/{개인 github 이름}/{repository 이름}.git\n```\n\ngit clone을 사용하여 원격 저장소에 있는 repository를 손쉽게 clone할 수 있다.\n\n```bash\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\n```\n\nclone 받은 local repository를 git remote -v로 확인해보면 원격 저장소가 등록되어 있는 것을 확인 할 수 있다. 매번 최신 코드를 `fetch` 및 `rebase` 하기 위해서는 `Upstream`을 등록해야 한다.\n\n```bash\n$ git remote add upstream https://github.com/{organization 이름}/{repository 이름}.git\n\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\n```\n\n`git remote add upstream`을 통하여 upstream을 등록한다. 정상적으로 등록 된 것을 확인할 수 있다. 이제 작업할 때 마다 브랜치를 생성하고 최신 코드를 pull 받아야 한다.\n\n우리 팀원은 각각 개발해야 하는 기능을 github issue에 등록한 후 등록 번호를 기반으로 브랜치를 생성하기로 하였다.\n\n우선 간단한 예시를 위하여 이슈를 등록한다.\n\n![](./issue.png)\n\n3번 번호가 부여된 이슈라고 가정한다. 해당 번호를 기반으로 `local repository`에서 feature 브랜치를 생성한다.\n\n```bash\n$ git branch feature/3-init-setting\n$ git checkout feature/3-init-setting\n```\n\n이제 Upstream에 있는 remote repository에서 최신 소스코드를 받아 와야 한다.\n\n```bash\n$ git fetch upstream\n$ git rebase upstream/develop\n```\n\ngit pull을 사용하여 등록한 upstream develop에서 commit 기록을 병합한다. 이제 신나게 작업을 진행하고 자신의 원격 저장소인 `Origin remote repository`에 push한다.\n\n```bash\n$ git push origin feature/3-init-setting\n```\n\n그렇게 github repository를 살펴보면 `변경을 감지`하고 pull request를 생성할 것인지에 대한 탭을 확인할 수 있다.\n\n![](./upstream-repository.png)\n\n이제 fork한 개인 원격 저장소를 살펴보면 새롭게 작성한 브랜치를 감지하고 pull request 작성을 위한 버튼이 생성된다.\n\n![](./pull-request.png)\n\n`feature/3-init-setting` 브랜치를 develop에 merge하기 위한 pull request를 진행하는 예시이다. 작성한 코드를 리뷰해줄 팀원들을 선택하고, commit한 코드의 내용을 간단히 요약하여 작성한다. 이제 생성한 PR을 기반으로 `코드리뷰`를 진행한다. 변경 사항이 적용되면 develop에 반영하기 위해 merge한다.\n\n### 달록에 맞게 수정하기\n\ngit-flow는 빠르게 급변하는 웹 서비스에는 맞지 않은 git 브랜치 전략이다. 관리해야 할 브랜치가 늘어나기 때문에 개발자들이 신경써야 할 포인트가 늘어난다.\n\n빈번한 배포로 인해 급작스러운 이슈가 발생할 수 있다. 즉 예상치 못한 롤백이 자주일어날 수 있다. 또한 웹 서비스의 특성상 다양한 릴리즈 버전을 유지할 필요가 없다. 이러한 특성들로 인해 웹 서비스에는 다소 보수적인 git-flow 전략은 맞지 않을 수 있다.\n\n그럼에도 우리 달록팀은 git-flow를 선택했다. 우리는 실제 운영할 수 있는 서비스를 개발하며 다양한 경험을 습득해야 한다. 또한 대부분의 팀원들이 git에 익숙하지 않았으며 다양한 시도를 통해 빠르게 학습해야 한다.\n\n대신 git-flow를 정석적으로 사용하지 않고 필요한 부분을 수정하여 반영하려 한다. 현재 수준에서 `develop`에서 대부분의 빌드를 진행하기 때문에 `release` 브랜치의 필요성이 다소 옅어졌다. 결국 `release`를 제외한 `main`, `develop`, `feature`, `hotfix`만 사용하기로 결정하였다.\n\n### 달록이 집중한 것\n\n달록의 팀 각 구성원들은 맡은 이슈를 기반으로 브랜치를 생성한 뒤 작업을 진행할 것이다. 결국 다수의 브랜치가 아래와 같이 병렬적으로 커밋이 쌓이게 된다.\n\n![](./force-push.png)\n\n만약 팀원 중 한명이 작업을 끝내서 PR이 merge된 상황이라고 가정하자. develop 브랜치의 커밋 베이스는 변경됬으며 이전에 작업을 진행하던 브랜치들은 시작점이 뒤로 밀려나게 된다.\n\n여러 사람이 하나의 저장소를 기반으로 작업을 진행하기 때문에 함께 사용하는 공간의 코드들은 충돌을 야기할 가능성이 크다. 즉 지속적인 `fetch` + `rebase`를 통해 사전에 충돌에 대비하며 항상 `develop` 브랜치와 커밋 싱크를 맞춘다.\n\n정리하면 위 그림과 같이 항시 develop 브랜치의 끝 단에서 시작해야 한다. 이러한 방식은 코드의 충돌을 최소화할 수 있으며 순차적인 git 커밋 목록을 기반으로 쉽게 기능이 추가된 것을 확인할 수 있다.\n\n## 정리\n\n지금까지 간단히 `git-flow의 흐름`과 이것을 기반으로 `달록에 적용한 과정`들을 알아보았다. git-flow는 언급한 것 처럼 부가적인 브랜치로 인해 `관리에 대한 부담감`을 느낄 수 있다. 하지만 `upstream`과 `origin`을 분리한 환경은 좀 더 도전적인 과제들을 적용하기에 매우 좋은 환경을 구성해준다. 또한 `pull request`를 통한 코드 리뷰를 통해 보다 더 양질의 애플리케이션 개발에 힘쓸 수 있다.\n\n## References.\n\n[git flow; 환상과 현실 그 사이에 서비스](https://vallista.kr/git-flow;-%ED%99%98%EC%83%81%EA%B3%BC-%ED%98%84%EC%8B%A4-%EA%B7%B8-%EC%82%AC%EC%9D%B4%EC%97%90-%EC%84%9C%EB%B9%84%EC%8A%A4/)<br>\n[우린 Git-flow를 사용하고 있어요](https://woowabros.github.io/experience/2017/10/30/baemin-mobile-git-branch-strategy.html)<br>\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 배경 우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시  키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해  키워드를 사용합니다. 물론…","fields":{"slug":"/intellij-final-keyword/"},"frontmatter":{"date":"July 12, 2022","title":"IntelliJ에서 메소드 추출한 메소드의 파라미터에 final 키워드 자동 추가하기","tags":["intellij"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 배경\n\n우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시 `final` 키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해 `final` 키워드를 사용합니다. 물론 `final` 키워드 하나만으로 완전한 불변을 보장하도록 만들수는 없지만, 어느정도 예측 가능한 코드를 만드는데에는 도움이 됩니다.\n\n이는 메소드의 파라미터에도 적용됩니다. 아래의 코드는 `Memo` 객체를 생성하기 위한 생성자입니다. `value` 라는 String 값을 전달받아 객체 필드에 할당합니다.\n\n```java\npublic Memo(String value) {\n    validateLength(value);\n\n    value = \"hello\"; // 예상치 못한 코드\n\n    this.value = value;\n}\n```\n\n하지만 위처럼 예상치 못한 코드가 추가되면 어떻게 될까요? `value` 필드에는 개발자가 의도하지 못한 값이 할당될 것 입니다.\n\n```java\npublic Memo(final String value) {\n    validateLength(value);\n\n    value = 1; // error: final parameter value may not be assigned\n\n    this.value = value;\n}\n```\n\n이를 보완하기 위해서 위처럼 메소드 파라미터에 `final` 키워드를 붙이면, 재할당 시 컴파일 에러가 발생하여 예상치 못한 동작을 사전에 방지할 수 있을 것 입니다.\n\n## IntelliJ 설정하기\n\n하지만, 저희는 아직 메소드 파라미터에 `final` 키워드를 붙이는 습관이 들어있지 않았습니다. 따라서 IDE의 도움이 필요한데요, 다행히도 IntelliJ에서 메소드 추출 리팩토링을 할 때 생성되는 메소드 파라미터에 자동으로 `final` 키워드를 붙여주는 옵션을 발견하였습니다.\n\n![IntelliJ 설정 화면](./intellij.png)\n\n맥 기준으로 Preferences → Editor → Code Style → Java 페이지에서 Code Generation 탭을 클릭합니다. 해당 탭의 하단에 ‘Final Modifier’ 에서 ‘Make generated parameters final’ 을 체크해줍니다. 위와 같이 옵션을 변경하면 메소드 추출 시 파라미터에 자동으로 `final` 키워드가 생성되는 모습을 확인할 수 있습니다 😊\n"},{"excerpt":"이 글은 우테코 달록팀 크루 리버가 작성했습니다. Rest Docs Spring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다. Rest Docs와 Swagger 자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n S…","fields":{"slug":"/apply-rest-docs/"},"frontmatter":{"date":"July 07, 2022","title":"MockMvc를 사용한 Spring RestDocs","tags":["Spring","Rest API"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [리버](https://github.com/gudonghee2000)가 작성했습니다.\n\n## Rest Docs\n\nSpring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다.\n\n## Rest Docs와 Swagger\n\n자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n![](https://velog.velcdn.com/images/gudonghee2000/post/ffc0e7eb-3190-43ca-9a85-16f9b8bbbb4e/image.JPG)\n\nSwagger는 API 문서의 작성을 위해 프로덕션 코드에 추가적인 코드를 작성해야한다.\n그래서 Swagger의 사용은 프로덕션 코드의 가독성을 떨어트린다고 생각한다.\n\n반대로, Spring Rest Docs는 테스트 코드에 의존적이기 때문에 Spring Rest Docs를 사용하는것이 좋다고 생각한다.\n\n## MockMvc vs Rest Assured\n\nSpring Rest Docs를 사용하여 문서를 작성 하려면 테스트 코드가 필요하다.\n테스트 코드를 작성 할 때, 대표적으로 MockMvc와 Rest Assured를 사용한다.\n\nMockMvc를 사용하면 `@WebMvcTest`로 테스트 할 수 있다.\n그래서 Controller Layer만으로 테스트 하기 때문에 테스트 속도가 빠르다.\n\n반면, RestAssured는 `@SpringBootTest`로 수행해야한다. 그러면 전체 어플리케이션 컨텍스트를 로드하여 빈을 주입하기에 테스트 속도가 느리다.\n하지만, 실제 객체를 통한 테스트가 가능하기 때문에 테스트의 신뢰성이 높다.\n\n통합 테스트, 인수 테스트의 경우 RestAssuerd가 좋을 수 있지만, 문서를 작성하기 위한 테스트에는 MockMvc가 더 적절하다고 생각한다.\n\n**_💡 @WebMvcTest와 @SpringBootTest_**\n@WebMvcTest는 Application Context를 완전하게 Start하지 않고 Present Layer 관련 컴포넌트만 스캔하여 빈 등록한다.\n반면, @SpringBootTest의 경우 모든 빈을 로드하여 등록한다.\n\n## AsciiDoc\n\nSpring Boot Rest Docs는 Asciidoc를 문서 번역을 위한 텍스트 프로세서로 사용한다.\n\n## Rest Docs API 문서 생성 매커니즘\n\n우선, Rest Docs의 문서 생성 매커니즘을 살펴보자.\n\n1. MockMvc로 작성한 테스트 코드를 실행한다.\n\n2. 테스트가 통과하면 아래와 같이 `build/generated-snippets` 하위에 스니펫(문서조각)들이 생성된다.\n   ![](https://velog.velcdn.com/images/gudonghee2000/post/f4555336-cc43-4cc7-b7ca-8c1f903b2afd/image.png)\n\n   _❗❗ gradle은 build/generated-snippets에 스니펫이 생성된다._\n\n3. `build/generated-snippets` 하위에 생성된 스니펫들을 묶어서 HTML 문서를 만들기 위해서는, gradle의 경우 아래와 같이`src/docs/asciidoc` 하위에 스니펫들을 묶은 adoc문서를 만든다.![](https://velog.velcdn.com/images/gudonghee2000/post/bc769cd9-2fd2-483d-8c65-a4885f628e37/image.png)\n\n4. 스니펫을 이용해서 `src/docs/asciidoc` 하위에 adoc 파일을 생성했다면, `./gradlew build` 명령어를 통해 빌드를 해준다.\n   ![](https://velog.velcdn.com/images/gudonghee2000/post/f1948f04-9742-4267-8d19-3d962097f129/image.png)\n   빌드가 완료되면 위와 같이 `resources - static - docs` 하위에 HTML 문서가 생성된다.\n\n5. 어플리케이션을 실행 한 후, `http://localhost:8080/docs/{HTML 파일명}` 을 웹브라우저에 검색하면 생성한 REST API 문서를 확인 할 수 있다.\n\n   **❗❗ API문서 url은 코드를 통해 변경 가능하다.**\n\n### ❗유의할 점\n\nresources - static - docs 하위의 HTML 파일은 실제로는 build.gradle의 설정파일에 따라서 위와같이 build - docs - asciidoc 하위의 HTML 파일을 복사해온 파일이다.\n![](https://velog.velcdn.com/images/gudonghee2000/post/7b70d45e-15a7-4278-9e3a-033370c2a600/image.png)\n\n### 아이디어\n\nREST API 문서를 확인할 때, `http://localhost:8080/docs/{HTML 파일명}` 을 통해서 웹브라우저에 접근하지 않아도 확인하는 방법이 있다.\n![](https://velog.velcdn.com/images/gudonghee2000/post/8f5177d6-5ba0-4f3e-be3a-3cba37012c46/image.png)\nAsciiDoc 플러그인을 설치하면 위와같이, 인텔리제이 상에서도 REST API 문서를 실시간으로 확인할수 있다. (✔설치 추천)\n\n## Rest Docs 사용을 위한 빌드파일 설정\n\n```java\nplugins {\n    id 'org.asciidoctor.jvm.convert' version '3.3.2' // 1\n}\n\next {\n    snippetsDir = file('build/generated-snippets') // 2\n}\n\ntest {\n    outputs.dir snippetsDir // 3\n    useJUnitPlatform()\n}\n\nconfigurations {\n    asciidoctorExtensions\n}\n\nasciidoctor { // 4\n    configurations 'asciidoctorExtensions'\n    inputs.dir snippetsDir\n    dependsOn test\n}\n\ndependencies {\n    testImplementation 'org.springframework.restdocs:spring-restdocs-mockmvc' // 5\n    asciidoctorExtensions 'org.springframework.restdocs:spring-restdocs-asciidoctor' // 6\n}\n\ntask copyDocument(type: Copy) { // 7\n    dependsOn asciidoctor\n\n    from file(\"build/docs/asciidoc\")\n    into file(\"src/main/resources/static/docs\")\n}\n\nbootJar {\n    dependsOn copyDocument // 8\n}\n```\n\n1. gradle7부터 사용하는 플러그인으로 asciidoc 파일 변환, build 디렉토리에 복사하는 플러그인이다.\n\n2. 생성된 스니펫을 저장할 위치를 정의한다. gradle은 `build/generated-snippets`에 스니펫이 생성된다.\n\n3. 테스트 Task의 결과 아웃풋 디렉토리를 `build/generated-snippets`로 지정한다.\n\n4. asciidoctor Task가 사용할 인풋 디렉토리를 `build/generated-snippets`로 지정한다.\n   dependsOn test로 문서가 작성되기 전에 테스트가 실행되도록 한다.\n5. MockMvc를 테스트에 사용하기 위한 의존성을 추가 해준다.\n\n6. 일반 텍스트를 처리하고 HTML 파일을 생성하는 의존성을 추가 해준다.\n\n7. asciidoctor Task로 생성한 `build/docs/asciidoc`파일을 `src/main/resources/static/docs`로 복사한다.\n\n8. bootJar 실행시 copyDocument를 먼저 실행하도록 한다.\n\n---\n\n✅MockMvc를 사용한 Rest Docs 테스트 작성을 알아보기 전에 우선 MockMvc에 대해 알아보자.\n\n##MockMvc 기본 메서드\n어떠한 것들이 있는지 알아보고 밑에서 자세히 알아보자.\n\n### perform()\n\n가상의 request를 처리한다.\n\n```java\nmockMvc.perform(get(\"/api/schedules/?year=2022&month=7\"))\n```\n\n### andExpert()\n\nandExpert()\n\n예상값을 검증한다.\n\n```java\n.andExpect(status().isOk())\n// status 값이 정상인 경우를 기대하고 만든 체이닝 메소드의 일부\n\n.andExpect(content().contentType(\"application/json;charset=utf-8\"))\n//contentType을 검증\n```\n\n### andDo()\n\n요청에 대한 처리를 맡는다. print() 메소드가 일반적이다.\n\n```java\n.andDo(print())\n```\n\n### andReturn()\n\n테스트한 결과 객체를 받을 때 사용한다.\n\n```java\nMvcResult result = mockMvc.perform(get(\"/\"))\n.andDo(print())\n.andExpect(status().isOk())\n.andReturn();\n```\n\n## MockMvc 요청 만들기\n\n요청을 만들 때는 static 메서드인 get, post, put, delete, fileUpload 등을 이용해서 MockHttpServletRequestBuilder 객체를 생성하는 것에서 시작한다.\n\nMockHttpServletRequestBuilder는 ServletRequest를 구성하기에 필요한 다양한 메서드를 제공한다.\n![](https://velog.velcdn.com/images/gudonghee2000/post/ee7412c0-3698-4e26-9ad2-ce826495d20e/image.JPG)\n위 메서드들은 메서드 체이닝을 지원하기 때문에, 아래와 같이 요청 데이터를 연결해서 작성하면된다.\n\n```java\n@Test\n    void test() throws Exception {\n        MockHttpServletRequestBuilder builder = get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\");\n\n        mockMvc.perform(builder)\n                .andExpect(status().isOk());\n    }\n\n```\n\n_**❗❗ 유의 할 점**_\nMockMvc.perform() 의 파라미터 값이 MockHttpServletRequestBuilder의 상위 객체이다.\n\n그래서 perform() 파라미터로 아래와 같이 넣어주어도 작동된다.\n\n```java\n@Test\n    void test() throws Exception {\n        mockMvc.perform(get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 검증\n\nperform()은 반환 값으로 ResultActions가 반환된다.\nResultActions의 andExpect는 요청 실행 결과를 검증 하려면 ResultMatcher를 넘겨줘서 검증해야한다.\nResultMatcher는 다음의 MockMvcResultMatchers가 가지는 static 메서드를 통해서 얻는다.\n\nMockMvcResultMatchers는 다음의 static 메서드를 통해 다양한 ResultMatcher를 제공한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/f1d670a7-c355-4c1c-9d01-2d84ea6412b7/image.JPG)\n\n아래의 예시를 살펴보자.\n\n```java\n\t@Test\n    void test() throws Exception {\n        mockMvc.perform(builder)\n                .andExpect(handler().handlerType(ScheduleController.class))\n                .andExpect(handler().methodName(\"save\"))\n                .andExpect(forwardedUrl(\"index\"))\n                .andExpect(header().stringValues(\"Content-Language\", \"en\"))\n                .andExpect(model().attribute(\"message\", \"저장이 잘되었습니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 처리\n\n실행 결과를 출력할 떄는 andDo 메서드를 사용한다.\nandDo 메서드 의 인수에는 실행 결과를 처리 할 수 있는 ResultHandler를 지정한다.\nMockMvcResultHandlers는 다양한 ResultHandler를 제공하지만 print()를 주로 사용한다.\n\n## MockMvc를 사용한 Rest Docs 생성\n\n테스트 코드와 함께 MockMvc를 사용한 Rest Docs 생성을 알아보자.\n\n```java\n@WebMvcTest(ScheduleController.class)\n@AutoConfigureRestDocs // 1\nclass ScheduleControllerTest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Autowired\n    private ObjectMapper objectMapper;\n\n    @MockBean // 2\n    private ScheduleService scheduleService;\n\n    @Test\n    void save() throws Exception {\n        // given\n        ScheduleCreateRequest request = new ScheduleCreateRequest(\"제목\", LocalDateTime.now(), LocalDateTime.now(), \"메모\");\n\n        given(scheduleService.save(request))\n                .willReturn(1L); // 3\n\n        // when & then\n        mockMvc.perform(post(\"/api/schedules\")\n                        .content(objectMapper.writeValueAsString(request))\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .accept(MediaType.APPLICATION_JSON))\n                .andExpect(status().isOk())\n                .andDo(document(\"schedule-save\", // 4\n                        requestFields(\n                                fieldWithPath(\"title\").type(JsonFieldType.STRING).description(\"제목\"),\n                                fieldWithPath(\"startDateTime\").type(JsonFieldType.STRING)\n                                        .description(\"2022-07-04T13:00\"),\n                                fieldWithPath(\"endDateTime\").type(JsonFieldType.STRING).description(\"2022-07-05T07:00\"),\n                                fieldWithPath(\"memo\").type(JsonFieldType.STRING).description(\"메모입니다.\")\n                        )\n                ));\n    }\n}\n```\n\n1. target/generated-snippets dir 생성하고 테스트 코드를 통해 snippets를 추가해주는 애노테이션이다.\n\n2. `ScheduleService`를 mocking을 하기위해서 `@MockBean` 을 선언한다.\n\n3. mocking을 통해 `ScheduleService` 를 통해 받을 응답값을 설정한다.\n\n4. test 수행시 `andDo(document(\"xxx\"))`를 통해서 `./build/generated-snippets` 하위에 문서가 작성된다.\n\n---\n\n## 끝내면서\n\n이상 Rest Docs의 매커니즘, 설정 그리고 MockMvc를 활용한 Rest Docs 생성 방법을 살펴보았다.\n프로젝트에 RestAssuered를 사용한 Rest Docs를 적용하면서 테스트 격리에 문제를 경험하였는데,\n테스트 격리에 대해서 추후에 포스팅 해봐야겠다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 리버가 작성했습니다. JPA 등장배경 1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이…","fields":{"slug":"/appearance-background-of-jpa/"},"frontmatter":{"date":"July 07, 2022","title":"JPA 등장배경","tags":["JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [리버](https://github.com/gudonghee2000)가 작성했습니다.\n\n## JPA 등장배경\n\n1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이후에 Spring에서는 DB Connection을 좀 더 쉽게 관리하는 Spring JDBC API를 만들고 지원하였다. (이외에도 Query문을 XML파일을 통해 관리하게끔 도와주는 Mybatis도 등장하였음)\n하지만, 여전히 쿼리문을 개발자가 직접 작성해야하는 등 다양한 문제를 가지고 있었다.\n그래서 JAVA 진영에서는 개발자가 쿼리문을 직접 작성하지 않아도 프레임워크 내부에서 지원해주는 ORM(Object Relational Model)기술인 JPA가 등장하였다.\n그렇다면, JPA 이전에 개발자들이 직접 쿼리문을 작성하던 SQL 중심적인 개발의 단점은 무엇이 있을까?\n아래에서 살펴보자.\n\n## SQL 중심적인 개발의 단점\n\n#### 1. 쿼리문 무한 반복과 지루한 코딩\n\nJDBC API는 쿼리문을 개발자들이 직접 작성 해야한다.\n그래서 개발자들은 쿼리문을 작성하는 지루한 작업을 개발 과정에서 무한반복해야한다.\n\n#### 2. 객체의 필드가 추가되면 모든 쿼리문을 수정해야한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/488a7899-d55b-4447-aafc-bada8d840192/image.jpg)\n위 그림과 같이 SQL 중심적인 개발에서는 객체의 필드가 변경되면 해당하는 모든 쿼리문을 찾아 개발자가 수정해야한다.\n\n#### 3. 객체와 관계형 DB의 패러다임의 불일치\n\n객체라고 하면 떠오르는 키워드는 무엇이 있을까?\n캡슐화, 협력, 의존, 상속, 참조 등의 기술이 있다. 그런데, DB에서는 이러한 기술들이 없다.\n적용되는 기술들의 패러다임 불일치로 인해 개발자들은 SQL 지향적인 개발을 할 수 밖에 없다.\n아래에서 자세히 살펴보자.\n\n## 객체와 관계형 DB의 패러다임 차이\n\n객체와 관계형 DB는 연관관계를 통해 작업을 수행한다는 공통점을 가진다.\n하지만 연관관계를 맺는 패러다임의 차이를 가진다.\n\n객체는 상속, 참조를 통해 연관관계를 맺는다.\n반면 관계형 DB는 PK, FK를 통해 연관관계를 맺는다.\n이때, 연관관계를 맺는 방식의 차이로 발생하는 문제점을 코드와 함께 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 `Crew` 객체가 `Team` 객체를 필드로 가지고 참조한다고 하자.\n객체지향적인 관점에서, `Crew`와 `Team`의 관계를 위와 같이 표현하는것은 자연스럽다.\n\n하지만, DB에서는 `Crew`가 `Team`을 참조한다는 개념이 없다.\n그래서 위 객체들을 가지고 DB의 `Crew`테이블과 `Team`테이블의 관계를 맺을때, 아래와 같이 `PK` 값인 id를 `FK`로 가지도록 구현 하여야한다.\n![ERD](./erd.png)\n\n연관관계에 대해서 객체의 구조와 DB의 구조가 달라진다는 것이다.\n\n그렇다면 객체지향적인 연관관계를 가진 객체들을 DB에 저장 할 때,\nDB의 연관관계로 변경하는 것이 왜 문제가 될까?\n\n## 객체와 RDB의 연관관계 차이가 가져오는 문제\n\n위에서 봤던 `Crew`와 `Team`의 객체 모델링을 다시한번 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 모델링된 `Crew`와 `Team`을 DB에 저장한다고 한다면 다음의 과정이 필요하다.\nDB에 접근하고자 하는 Dao 객체는 `Crew`객체를 분해하고 각자 `Crew` 테이블과 `Team` 테이블에 대한 쿼리를 작성해야한다. 단순히 `Crew`의 객체정보를 저장하는데 3가지 과정을 거쳐야한다.\n\n이러한 복잡한 과정을 피하는 방법은 없을까?\n아래 코드를 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Long team_id;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 DB 테이블 구조에 맞추어 `Crew`와 `Team` 객체를 설계하는 방법이있다.\n이러한 객체 모델링은 Dao 객체를 통해 데이터를 DB에 저장 할 때, `Crew` 객체를 분해하는 과정을 삭제 할 수 있다.\n객체가 DB 구조에 맞추어 설계되어 있기 때문이다.\n\n**하지만, 객체 모델링을 할 때 객체가 서로 참조하는 객체지향적인 개발이 아닌\nDB 테이블구조에 맞추어 개발하는 SQL 중심적인 개발을 하게 된다는 문제를 가진다.**\n\n객체와 관계형 DB의 연관관계의 패러다임 차이는 객체를 객체답지 못하게 만든다는 것이다.\n그렇다면, 패러다임의 차이를 해결하는 방법은 없을까?\n\n## JPA\n\n객체와 관계형 DB의 패러다임의 차이로 인해 우리는 객체지향적인 프로그래밍을 하지못하고 DB에 종속적인 개발을 하게된다.\n이러한 문제를 해결하기 위해 JAVA진영에서는 JPA를 제공한다.\n\nJPA를 통해 개발자는 더이상 쿼리문을 반복적으로 작성하거나 유지보수하는 것을 신경쓰지 않아도 된다.\n왜냐하면 JPA가 쿼리문을 작성해주기 때문이다.\n그리고 SQL 중심적인 개발에서 벗어나 객체지향적인 개발을 할 수 있게 된다.\n왜냐하면 패러다임의 불일치를 JPA가 내부적으로 맵핑해주기 때문이다.\n\n다음 포스팅에서는 JPA의 작동 메커니즘을 자세히 살펴보자.\n"}]}},"pageContext":{}},"staticQueryHashes":[]}