{"componentChunkName":"component---src-pages-search-jsx","path":"/search/","result":{"data":{"allMarkdownRemark":{"nodes":[{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 🤢 문제 상황 며칠전 저는 데이터베이스 레플리케이션을 통한 쿼리 성능 개선 (feat. Mysql, SpringBoot) 포스팅에서 MySQL과 스프링부트 환경에서 레플리케이션을 적용하는 방법에 대해 다뤄보았습니다. DB 레플리케이션을 적용하여 읽기 성능을 향상시키기 위해서는 스프링부트에서 다수의 Data…","fields":{"slug":"/multi-datasource-issue-with-osiv/"},"frontmatter":{"date":"October 17, 2022","title":"대체 왜 DataSource 라우팅이 안되는거야!? (feat. OSIV)","tags":["replication","JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 🤢 문제 상황\n\n며칠전 저는 **[데이터베이스 레플리케이션을 통한 쿼리 성능 개선 (feat. Mysql, SpringBoot)](https://hudi.blog/database-replication-with-springboot-and-mysql/)** 포스팅에서 MySQL과 스프링부트 환경에서 레플리케이션을 적용하는 방법에 대해 다뤄보았습니다. DB 레플리케이션을 적용하여 읽기 성능을 향상시키기 위해서는 스프링부트에서 다수의 DataSource를 설정하고, 트랜잭션의 `readOnly` 옵션에 따라 DataSource를 잘 분기하는 것이 중요합니다.\n\n포스팅을 작성할 때에는 새로운 프로젝트를 만들어 학습을 진행하였으므로 큰 문제없이 레플리케이션 적용에 성공하였습니다. 문제는 달록에 이를 적용할 때 발생하였습니다. **분명 서비스 메소드의 트랜잭션 옵션을** `readOnly = false` **로 설정했는데, 읽기 전용 데이터베이스(replica)로 쿼리가 날아갔습니다**. 제대로 DataSource가 라우팅되지 않고 있는 상황이었죠. 대체 무엇이 문제였을까요?\n\n## 💡 실마리 발견\n\n### 로그인 상태에서만 발생하는 문제\n\n이것저것 시도해보다 이상한 점을 발견했습니다. 카테고리를 생성하거나, 일정을 생성하는 등 로그인 된 상태에서는 DataSource 분기가 제대로 이뤄지지 않았습니다. 서비스 메소드 `@Transactional` 어노테이션에 무슨 설정을 해도, 항상 DataSource는 `readOnly` 가 `true` 일때 선택되는 replica DB 였습니다. 하지만, **회원을 생성하는 로직에서는 이상하게 쓰기 전용(source) DB에 대한 DataSource가 정상적으로 선택**되었습니다.\n\n### 트랜잭션을 사용하는 HandlerMethodArgumentResolver\n\n달록은 스프링이 제공하는 `HandlerMethodArgumentResolver` 를 사용하여, Access Token 으로부터 member id를 꺼내오고, 회원의 실제 존재여부를 검증한 뒤, Controller의 파라미터에 member id를 담고있는 `LoginMember` 라는 DTO를 전달합니다. 그리고 이 로직은 로그인이 된 상태에서 호출되는 API에서만 동작하죠. 즉, 회원가입 로직에서는 실행될 필요가 없는 로직입니다.\n\n아래는 `AuthenticationPrincipalArgumentResolver` 의 코드 일부입니다.\n\n```java\n@Component\npublic class AuthenticationPrincipalArgumentResolver implements HandlerMethodArgumentResolver {\n\n    // ...\n\n    @Override\n    public Object resolveArgument(final MethodParameter parameter, final ModelAndViewContainer mavContainer,\n                                  final NativeWebRequest webRequest, final WebDataBinderFactory binderFactory) {\n        // ...\n\n        Long id = authService.extractMemberId(accessToken);\n        // AuthService 메소드 호출\n\n        return new LoginMember(id);\n    }\n}\n```\n\n위 코드를 보시다시피 `AuthenticationPrincipalArgumentResolver` 는 `AuthService` 의 `extractMemberId()` 메소드를 사용합니다. `extractMemberId()` 에서 위에서 이야기한 member id 추출, 회원 존재 여부 검증, `LoginMember` 생성등을 진행합니다. 아래와 같이 말이죠.\n\n```java\n/* AuthService 클래스 코드 */\n\n@Transactional(readOnly = true) // 실제로는 클래스에 선언되어 있습니다.\npublic Long extractMemberId(final String accessToken) {\n    Long memberId = tokenCreator.extractPayload(accessToken);\n    memberRepository.validateExistsById(memberId);\n    return memberId;\n}\n```\n\n즉, `AuthenticationPrincipalArgumentResolver` 의 코드는 트랜잭션을 사용합니다. 그리고 여러 시행착오 끝에 `AuthService.extractMemberId()` 메소드 트랜잭션의 `readOnly` 옵션에 따라 선택된 DataSource가 다른 트랜잭션에서도 그대로 사용되는 것을 알아냈습니다.\n\n실제로 `extractMemberId()` 메소드 트랜잭션의 `readOnly` 를 `false` 로 설정하니 이후 트랜잭션에서도 replica DB 대신 source DB로 쿼리를 날리는 것을 확인했습니다.\n\n## 🤔 트랜잭션 전파 때문인가…?\n\n처음에는 `AuthService.extractMemberId()` 와 그 이후에 실행되는 서비스 메소드간 트랜잭션 전파가 발생하여 DataSource 라우팅이 발생하지 않는다고 추측했습니다. 하지만 이상합니다. 분명 저는 컨트롤러 레이어에서는 트랜잭션 전파가 발생하지 않는다고 알고 있었기 때문입니다.\n\n하지만 그래도 혹시나 하는 마음에서 트랜잭션 전파 옵션을 `REQUIRES_NEW` 로 설정해보았습니다. 트랜잭션 전파 옵션을 `REQUIRES_NEW` 로 설정하면, 앞에서 이미 시작한 트랜잭션이 있든 없든 상관없이 항상 새로운 트랜잭션을 만듭니다.\n\n```java\n@Transactional(propagation = Propagation.REQUIRES_NEW)\n```\n\n기대와 달리 위와 같이 트랜잭션 전파 옵션을 변경하더라도 DataSource 라우팅이 정상적으로 동작하지 않았습니다. 즉, 컨트롤러 메소드에서 서비스 메소드의 트랜잭션 간 전파가 발생한 문제는 아니었던 것입니다.\n\n## 🧵 원인은 OSIV(open session in view)\n\n삽질 방향을 계속 스프링이 트랜잭션을 관리하는 방법에 초점을 맞추다보니 전혀 문제 해결에 대한 감을 잡지 못했습니다. 끝내 문제를 해결하지 못하고, 같은 우테코 크루인 루키에게 도움을 요청했습니다. 루키는 문제를 보자마자 단번에 **OSIV(open session in view) 문제**라는 것을 발견하였습니다. (루키 땡큐 🙏)\n\n그렇다면, 대체 OSIV가 무엇이고 왜 이런 문제의 원인이 되었을까요? OSIV는 JPA에서 사용되는 개념으로 **영속성 컨텍스트를 뷰와 컨트롤러 즉 프레젠테이션 레이어까지 열어둔다**는 의미입니다. 그렇다면 왜 JPA는 컨트롤러 까지 영속성 컨텍스트를 열어둘까요? 바로 **지연 로딩(lazy loading)을 사용하기 위함**입니다.\n\nOSIV가 활성화되어 있다면, 서비스 계층에서 컨트롤러 계층으로 직접 엔티티를 반환하더라도, 컨트롤러에서 지연 로딩을 통해 내부 필드를 읽어올 수 있습니다. 달록은 서비스 계층에서 무조건 DTO를 반환하여 엔티티나 도메인을 프레젠테이션 레이어까지 노출하지 않도록 규칙을 정해두어 한번도 OSIV를 사용해볼 일이 없었는데요. 그래서 저도 이번에 처음 알게된 개념입니다 😮\n\n### 요청 당 트랜잭션 방식의 OSIV\n\n![요청 당 트랜잭션 방식의 OSIV](./osiv-1.png)\n\n과거에 사용된 OSIV 방식입니다. 서블릿 필터 혹은 스프링 인터셉터에서 영속성 컨텍스트와 트랜잭션을 생성하고, 요청의 처음부터 끝까지 유지하는 방식입니다. 유저의 요청 내내 영속성 컨텍스트가 살아있으므로, 엔티티도 영속상태를 끝까지 유지합니다. 즉, 지연 로딩을 사용할 수 있죠.\n\n하지만 이 방식에는 문제점이 존재합니다. 영속성 컨텍스트가 컨트롤러와 뷰에서까지 유지되면, 엔티티도 영속상태를 유지한다고 했었죠. 이 상황에서 트랜잭션까지 함께 유지되다 보니 엔티티를 뷰나 컨트롤러에서 변경할 수 있게 됩니다.\n\n유저의 이름을 `***` 으로 가려서 클라이언트에게 보내줘야할 상황을 가정해봅시다. 컨트롤러는 아래와 같이 서비스 계층에서 전달받은 `Member` 엔티티를 수정합니다.\n\n```java\nclass MemberController {\n\n    public ResponseEnttiy<Member> findMember(final Long id) {\n        // ...\n        member.setName(\"***\");\n        // ...\n    }\n}\n```\n\n여기서 문제가 발생합니다. 요청 당 트랜잭션 방식의 OSIV는 뷰가 렌더링되면, 트랜잭션을 커밋합니다. 즉, 위 코드에서는 트랜잭션이 커밋되고, 영속성 컨텍스트가 플러시 되는데, JPA의 더티 체킹(dirty checking)으로 인해 `setName()` 메서드를 사용해 수정된 엔티티 내용이 실제 DB에 반영되게 됩니다. 정말 심각한 문제죠.\n\n따라서 (1) 엔티티 자체를 읽기전용 인터페이스로 제공하거나, (2) 엔티티를 한번 래핑하여 반환하거나, (3) DTO로 변환하여 반환하는 방법등으로 해결해야합니다. 이런 문제점으로 인하여 최근에는 요청 당 트랜잭션 방식의 OSIV 방식은 거의 사용되지 않는다고 합니다.\n\n### 스프링 OSIV\n\n그렇다면 최근에 사용되는 OSIV 방식은 무엇일까요? 바로 스프링 프레임워크에서 제공하는 스프링 OSIV입니다. 요청 당 트랜잭션 OSIV 방식의 가장 큰 문제점은 뷰나 컨트롤러 즉, 프레젠테이션 계층에서 엔티티를 수정할 수 있다는 문제점입니다.\n\n스프링 프레임워크가 제공하는 OSIV 방식은 무엇이 다를까요? 스프링 OSIV는 트랜잭션을 비즈니스 계층까지만 사용한다는 것 입니다. 아래 그림을 살펴볼까요?\n\n![스프링 OSIV](./osiv-2.png)\n\n중요한 것은 위 구조로 인해 프레젠테이션 레이어에서 더이상 엔티티를 수정할 수 없도록 개선이 되었다는 점 입니다.\n\n## 😡 그래서 왜 OSIV가 문제인데?\n\n이렇게 OSIV에 대해 구구절절 알아보았는데요, 그래서 결국 왜 OSIV가 DataSource 라우팅이 안되는 이슈의 원인이었을까요? 위에서 이야기했듯 OSIV를 사용하면 영속성 컨텍스트가 요청 내내 유지되는 것을 확인할 수 있죠.\n\n영속성 컨텍스트는 `EntityManager` 를 생성할 때 하나가 생성됩니다. 그리고 `EntityManager` 는 데이터베이스 연결이 필요한 시점에 커넥션을 획득해옵니다. 이때 커넥션 풀에서 커넥션을 획득해오니, `DataSource` 를 사용하겠죠?\n\n문제는 여기서 발생합니다. 저희가 `AbstractRoutingDataSource` 를 사용한 목적은 서비스 메소드마다 다른 `DataSource` 를 사용해서 다른 커넥션을 획득하기 위함이었습니다. 하지만 OSIV 덕에 영속성 컨텍스트는 프레젠테이션 레이어까지 유지되고, 한번 커넥션을 얻어온 `EntityManager` 는 또 다시 커넥션을 얻으려하지 않습니다. 즉, `AbstractRoutingDataSource` 를 통한 라우팅이 발생하지 않습니다. 애초에 새로운 커넥션을 가져올 필요가 없으니까요.\n\n### 해결 방법\n\n해결 방법 자체는 매우 매우 간단합니다. 아래와 같이 `application.yml` 에서 `open-in-view` 를 `false` 로 설정하면 됩니다. 더이상 OSIV를 사용하지 않겠다는 뜻이죠.\n\n```yaml\njpa:\n\topen-in-view: false\n```\n\n설정을 이렇게 변경하고, 다시 테스트해보면 이제 DataSource 라우팅이 정상적으로 동작합니다!\n\n## 마치며\n\nDataSource 라우팅이 이렇게 오래 걸릴 태스크가 아니라고 생각했는데 많이 얕봤던 것 같습니다. 특히, 이번 태스크를 진행하며 사용하는 프레임워크나 라이브러리에 대한 깊은 이해가 얼마나 중요한지 다시금 상기하게 되었습니다. 저는 OSIV라는 키워드 자체를 알지 못하고 있었거든요. 그래서 전혀 해결의 실마리를 잡지 못하고 있었습니다. JPA를 깊게 이해하고 있던 루키가 아니었다면, 아마 DB 레플리케이션을 포기했을지도 모릅니다 😅\n\n## 참고\n\n- 자바 ORM 표준 JPA 프로그래밍, 김영한\n- [https://incheol-jung.gitbook.io/docs/study/jpa/3](https://incheol-jung.gitbook.io/docs/study/jpa/3)\n- [https://github.com/woowacourse-teams/2021-nolto/issues/700](https://github.com/woowacourse-teams/2021-nolto/issues/700)\n- [https://github.com/woowacourse-teams/2021-gpu-is-mine/issues/399](https://github.com/woowacourse-teams/2021-gpu-is-mine/issues/399)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 문제 상황  어느날 실제 배포 서버의 DB를 확인해보았는데 start/end date time이 전부 +9시간되어 저장된 것을 확인했다.  분명 서비스에서는 00시 00분으로 잘 나오는데 왜 DB에만 9시간이 더해져서 저장되는 걸까? 원인 찾기 가정 1. MySQL Server timezone이 UTC여서…","fields":{"slug":"/db-time-2-plus-nine-hours/"},"frontmatter":{"date":"October 04, 2022","title":"달력 서비스의 DB 시간 다루기 2 - DB에 시간이 +9 되어 저장되는 문제","tags":["database","time","be","파랑"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 문제 상황\n\n![달록 캡쳐](dallog_schedules.png)\n\n어느날 실제 배포 서버의 DB를 확인해보았는데 start/end date time이 전부 +9시간되어 저장된 것을 확인했다.\n\n![나인 생일](nine_birthday.png)\n\n분명 서비스에서는 00시 00분으로 잘 나오는데 왜 DB에만 9시간이 더해져서 저장되는 걸까?\n\n## 원인 찾기\n\n### 가정 1. MySQL Server timezone이 UTC여서일 것이다. ❌\n\nKST에서 UTC로 바뀌면서 시간이 변한 거라면 시간이 +9가 아니라 -9가 되었어야 했다. 그리고 Time Zone을 변경해도 계속해서 +9가 되어 시간이 저장되는 것은 변함이 없었다.\n\n- Ubuntu의 Server Time Zone 변경하는 방법\n\n    ```bash\n    # Using timedatectl command (추천)\n    > sudo timedatectl set-timezone \"Asia/Seoul\"\n    > timedatectl  # 확인\n    \t\t\t\t\t\t\t Local time: Tue 2022-10-04 21:31:53 KST\n               Universal time: Tue 2022-10-04 12:31:53 UTC\n                     RTC time: Tue 2022-10-04 12:31:54\n                    Time zone: UTC (KST, +0900)\n    System clock synchronized: yes\n                  NTP service: active\n              RTC in local TZ: no\n    \n    # Using /etc/localtime Symlink\n    > sudo cp /usr/share/zoneinfo/Asia/Seoul /etc/localtime # 변경\n    > date # 현재 시간 확인\n    > sudo service mysql restart # mysql 재시작\n    \n    > sudo rm -f /etc/localtime # UTC로 되돌리기\n    ```\n\n- MySQL의 Time Zone 확인하기\n\n    ```bash\n    > SELECT @@time_zone, now(); # timezone과 현재 시간 확인\n    +-------------+---------------------+\n    | @@time_zone | now()               |\n    +-------------+---------------------+\n    | SYSTEM      | 2022-10-04 12:32:40 |\n    +-------------+---------------------+\n    \n    > SHOW VARIABLES WHERE Variable_name LIKE '%time_zone%'; # system timezone 확인\n    +------------------+--------+\n    | Variable_name    | Value  |\n    +------------------+--------+\n    | system_time_zone | UTC    |\n    | time_zone        | SYSTEM |\n    +------------------+--------+\n    ```\n\n\n> 물론 MySQL Server Time Zone 설정을 바꾸지 않아 생기는 문제도 있다. 모든 시간이 UTC 기준으로 저장되고 있기 때문이다. Time Zone 설정을 KST로 바꾸면 모든 시간이 전부 +9가 된다.\n\n하지만 DB에 시간이 +9가 되어 저장되는 문제의 원인은 아니었다.\n\n### 가정 2. yml 파일 설정의 `serverTimezone` 설정 때문이다. ⭕️\n\n```java\nspring:\n\tdatasource:\n    driver-class-name: com.mysql.cj.jdbc.Driver\n    url: jdbc:mysql://어쩌고저쩌고/dallog?serverTimezone=Asia/Seoul\n```\n\n설마하는 마음으로 설정을 제거해보니 시간이 정상적으로 들어가는 것을 확인할 수 있었다.\n\n![설정 제거 전후 비교](testTimeZone.png)\n\n그렇다면 이 문제는 왜 발생하는 것일까? 아래 그림을 보자.\n\n![내가 그린 기린 그림](fucking_serverTimeZone.png)\n\nJVM의 timezone은 기본값인 UTC로 설정된 상태다. 여기서 yml 설정을 통해 db의 serverTimezone을 Asia/Seoul이라고 알려줬다. 이때문에 DB에 시간이 저장되는 과정에서 자동으로 9시간을 더해준 것이다. DB에서 시간을 가져올 때는 반대로 -9시간을 적용하기 때문에 서비스 로직에는 아무 문제가 없었다.\n\n> 시간을 다룰 때 MySQL 서버 뿐만 아니라 JVM의 Time Zone도 고려해야 함을 잊지 말자!\n\n### (참고) DB server의 Time Zone 설정 우선순위\n\n그렇다면 왜 DB의 Time Zone 설정을 변경하는 것은 먹히지 않고 yml 파일의 serverTimezone 설정만 적용될까? MySQL의 Time Zone 설정 우선순위 때문이다. 우선순위는 아래와 같다.\n\n1. 클라이언트 수준에서의 설정\n2. DB 설정\n3. OS 설정\n\n1번이 바로 우리가 설정해준 yml의 serverTimezone 설정인 것이다. 그래서 1번을 지우지 않는 이상 아무리 DB server의 Time Zone을 변경해봤자 소용이 없었던 것이다.\n\n## 해결 방법\n\n내가 찾은 해결 방법은 두 가지다.\n\n### 1. serverTimezone 설정을 제거한다.\n\n원인을 찾았으니 원인인 serverTimezone 설정을 제거하면 문제를 해결할 수 있다.\n\n```java\nspring:\n\tdatasource:\n    driver-class-name: com.mysql.cj.jdbc.Driver\n    url: jdbc:mysql://어쩌고저쩌고/dallog # 설정 제거\n```\n\n### 2. JVM의 시간 변경\n\n설정을 제거하지 않고 문제를 해결하기 위해서는 JVM의 Time Zone과 mysql server의 Time Zone을 동일하게 맞춰주어야 한다.\n\njar 파일 실행 시 timezone 설정을 통해 Time Zone을 Asia/Seoul로 맞춰주면 문제를 해결할 수 있다.\n\n```bash\n# -Duser.timezone=Asia/Seoul\nnohup java -jar -Duser.timezone=Asia/Seoul -Dspring.profiles.active=${SPRING_PROFILE} /home/ubuntu/$JAR_NAME > /dev/null 2>&1 &\n```\n\n> 달록의 경우 2번을 통해 JVM 시간과 mysql의 server 시간을 모두 Asia/Seoul로 맞춰주기로 했다.\n\n#### Reference\n\n[https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html](https://dev.mysql.com/doc/connector-j/8.0/en/connector-j-time-instants.html)\n\n[https://tecadmin.net/linux-change-timezone/](https://tecadmin.net/linux-change-timezone/)\n\n[https://velog.io/@kyu/Timestamp-와-Datetime](https://velog.io/@kyu/Timestamp-%EC%99%80-Datetime)\n\n**Special Thanks To `애쉬` & `차리`**\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 문제 상황 달록은 스케줄의 시작 일시와 종료 일시의 타입이  로 지정되어 있다. 우리가 따로 날짜 범위에 대한 검증을 해주고 있지 않아 특정 날짜 범위를 벗어나면 아래와 같은 예외가 발생했다.  그제서야 날짜의 최대 최소 값을 알아보았다.  java의 경우 거의 무제한이라고 봐도 무방했다. 문제는 mysq…","fields":{"slug":"/db-time-1-timestamp-2038/"},"frontmatter":{"date":"October 04, 2022","title":"달력 서비스의 DB 시간 다루기 1 - TimeStamp 2038 문제","tags":["database","time","be","파랑"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 문제 상황\n\n```sql\nCREATE TABLE IF NOT EXISTS schedules (\n  ...\n  start_date_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  end_date_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  created_at timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  updated_at timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\t...\n);\n```\n\n달록은 스케줄의 시작 일시와 종료 일시의 타입이 `timestamp` 로 지정되어 있다. 우리가 따로 날짜 범위에 대한 검증을 해주고 있지 않아 특정 날짜 범위를 벗어나면 아래와 같은 예외가 발생했다.\n\n![DataIntegrityViolationException 발생](error.png)\n\n그제서야 날짜의 최대 최소 값을 알아보았다.\n\n![표](datetime.png)\n\njava의 경우 거의 무제한이라고 봐도 무방했다. 문제는 mysql이었다.\n\nTimeStamp는 min, max 값이 너무 작았다. 이대로 계속 TimeStamp를 쓰게 되면 우리 달록 서비스의 수명은 최대 2038년이 되는 것이었다. 생각보다 이 문제가 유명해서 이를 `2038 문제`라 부른다고 한다.\n\n[https://namu.wiki/w/2038년 문제](https://namu.wiki/w/2038%EB%85%84%20%EB%AC%B8%EC%A0%9C)\n\n이는 캘린더 서비스를 제공하는 달록에게는 치명적인 부분이었다. ~~달록이 그때까지 살아 있으면 소원이 없겠..~~\n\n## TimeStamp와 DateTime의 차이점\n\nTimeStamp가 DateTime과 다른 점은 **시스템의 Time Zone 설정에 따라 시간이 변경**된다는 것이다. 이런 특성 때문에 글로벌 서비스를 필요로 하는 경우 TimeStamp를 유용하게 사용할 수 있다.\n\n하지만 달록은 아직 글로벌 서비스를 염두에 두고 있지 않아 Time Zone에 따라 자동으로 시간이 바뀌는 것에 대한 필요성을 느끼지 못했다. 오히려 달력 서비스임에도 일정이 2038년까지만 등록되는 것이 더 큰 문제로 느껴졌다.\n\n> 따라서 달록은 일정의 start date time, end date time의 타입을 TimeStamp에서 DateTime으로 바꿔주기로 했다.\n\n## 타입을 변경해도 서비스 로직에 문제가 생기지 않는지 확인\n\n실제로 타입을 변경하기 전, local에서 schema.sql을 변경하여 start_date_time, end_date_time의 타입을 변경해도 서비스 로직이 잘 동작하는지 확인해보았다.\n\n```sql\n// schema.sql\nCREATE TABLE IF NOT EXISTS schedules (\n  ...\n  start_date_time datetime NOT NULL,\n  end_date_time datetime NOT NULL,\n  ...\n);\n```\n\n기본은 `datetime(6)` 이며 괄호 안의 숫자는 datetime의 소수점 자리수를 나타낸다. 최대 6자리까지 지정할 수 있다. 하지만 달록에서는 소수점까지는 사용하지 않기 때문에 `datetime` 으로 지정했다.\n\n![mysql capture](select_schedules.png)\n\n![dallg_capture](dallog.png)\n\n그 결과 타입을 datetime으로 변경해도 서비스 로직이 잘 동작하는 것을 확인할 수 있었다.\n\n### Reference\n\n[https://kciter.so/posts/deep-dive-into-datetime](https://kciter.so/posts/deep-dive-into-datetime)\n\n[https://dev.mysql.com/doc/refman/8.0/en/datetime.html](https://dev.mysql.com/doc/refman/8.0/en/datetime.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. 달록팀은 와 로 이루어져 있다. 프론트엔드와 백엔드의 경우 서로의 영역을 침범할 일이 적지만 각각의 영역에서는 대부분의 코드를 공유하여 으로 태스크를 진행하게 된다. 아래는 실제 달록에서 어떠한 과정을 통해 태스트를 나누고 진행하고 있는지 설명하기 위해 백엔드에서 구성한 패키지 구조이다. 크게 3개의 기준…","fields":{"slug":"/cyclic-dependency/"},"frontmatter":{"date":"September 30, 2022","title":"cyclic dependency","tags":["매트","BE","cyclic dependency","순환 의존"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n달록팀은 `2명의 프론트엔드 개발자(티거, 나인)`와 `4명의 백엔드 개발자(파랑, 리버, 후디, 매트)`로 이루어져 있다. 프론트엔드와 백엔드의 경우 서로의 영역을 침범할 일이 적지만 각각의 영역에서는 대부분의 코드를 공유하여 `병렬적`으로 태스크를 진행하게 된다.\n\n아래는 실제 달록에서 어떠한 과정을 통해 태스트를 나누고 진행하고 있는지 설명하기 위해 백엔드에서 구성한 패키지 구조이다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── domain\n    │   │               │   ├── auth\n    │   │               │   │   └── ...\n    │   │               │   ├── subscription\n    │   │               │   │   └── ...\n    │   │               │   ├── schedule\n    │   │               │   │   └── ...\n    │   │               │   └── member\n    │   │               │       └── ...\n    │   │               │    \n    │   │               ├── global\n    │   │               │   └── ...\n    │   │               │     \n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n크게 3개의 기준을 가지고 패키지 구조를 분리하였다.\n\n- `domain`: 실제 애플리케이션의 핵심이 되는 도메인 로직이 모여 있다. 애플리케이션의 주요 비즈니스 로직이 모여 있으며 `외부와의 의존성을 최소화`해야 한다. 도메인 패키지 내부에는 달록의 요구사항에 맞춰 도출된 도메인 별 패키지(e.g. auth, category, schedule 등)가 위치하고 있다. 해당 패키지 내부에는 각 도메인에서 계층 혹은 역할 별로 나뉘게 된다.\n- `global`: 프로젝트 전반에서 사용하는 객체로 구성한다.\n- `infrastructure`: 외부와 통신을 담당하는 로직이 담겨 있다.\n\n> 각각의 도메인 별 패키지(e.g. auth, category, schedule 등)를 또 한번 `domain` 패키지로 묶은 이유는 `global`과 `infrastructure`와의 의존성을 편리하게 확인하기 위한 의도가 담겨 있다. 다만 불필요하게 패키지 depth가 늘어난 것이 우려되어 팀원 과의 충분한 상의 이후 개선할 예정이다.\n> \n\n## 달록 작업 과정 시나리오\n\n먼저 파랑이 `schedule`과 관련된 태스크를 부여받은 뒤 열심히 작업을 진행하고 있다. 달록과 지원 플랫폼 근로에 소속된 파랑은 일치감치 달록의 태스크를 마무리한 뒤 지원 플랫폼 근로를 진행하기 위해 달록 서비스 운영 코드에 작업 내역을 반영한다.\n\n그와 동시에 후디도 `subscription`과 관련된 태스크를 부여 받았다. 후디는 현재 달록에만 소속되어 있기 때문에 파랑이 지원 플랫폼 근로로 이동한 후에도 계속해서 작업을 진행하고 있다. \n\n위 시나리오만 살펴보면 서로 다른 패키지에서 태스크를 진행하기 때문에 크게 문제 되는 부분은 없어보인다. 하지만 여기에는 함정이 숨어있다. 바로 `schedule`과 `subscription`이 `패키지 순환 참조`를 하고 있기 때문이다.\n\n## 의존성 비순환 원칙\n\n> ***하루 종일 일해서 무언가를 작동하게 만들어 놓고 퇴근했는데, 이틑날 출근해 보면 전혀 돌아가지 않는 경험을 해본적 있지 않은가? 왜 작동하지 않게 되었나? 왜냐하면 누군가 당신보다 더 늦게까지 일하면서 당신이 `의존`하고 있던 무언가를 수정했기 때문이다. 나는 이러한 현상을 ‘숙취 증후군$_{the morning after syndrome}$’이라고 부른다.***  - ***클린 아키텍처 116p***\n> \n\n객체 세계에서 패키지가 패키지를 의존하는 것은 너무나 자연스러운 일이다. 각각의 객체는 서로가 필요한 순간에 적절히 의존하여 메시지를 전달해야 한다. 하지만 `의존이 순환하는 것`은 큰 파장을 불러일으킨다. 이것은 패키지 사이에도 동일하게 적용된다.\n\n![](./image-1.png)\n\n의존성을 `비순환`하도록 개선하는 것만으로도 문제 발생의 포인트 지점을 쉽게 파악하고 관리할 수 있다. 이상적인 의존 구조는 `비순환 방향 그래프`의 형태를 띄게 될 것이다. 이것을 적절히 지키기 위한 원칙을 `의존성 비순환 원칙`이라 한다. \n\n## 순환 참조 개선\n\n이처럼 순환 참조 구조는 특정 패키지/객체에 변화가 생기면 연쇄적으로 영향을 받을 수 있기 때문에 유연하게 대응할 수 없다. 때문에 객체의 의존이 `단방향`으로 흐를 수 있도록 개선해야 한다. \n\nIntelliJ에서는 패키지 간의 순환 참조를 확인하기 위한 기능을 제공한다.\n\n> 상단탭 `Code` → `Analyze Code` → `Cyclic Dependencies…` 클릭\n\n![](./image-2.png)\n\n자 이제 달록의 패키지 순환 참조 유무를 확인해보자. 나름대로 `객체 사이의 의존성`을 고려하여 진행 했다고 자부하지만 그럼에도 `패키지 사이의 의존성` 까지는 고려하지 못했다. 아래 사진을 보면 패키지 간의 `순환 참조 발생 지 점`을 확인할 수 있다.\n\n![](./image-3.png)\n\n자 이제 순환 참조가 발생하는 코드를 간단히 살펴보자.\n\n`schedule` 패키지에 소속된 `IntegrationSchedule` 객체는 특정 행위를 달성하기 위해 `subscription` 패키지를 의존하고 있다.\n\n```java\npackage com.allog.dallog.domain.schedule.domain;\n...\nimport com.allog.dallog.domain.subscription.domain.Subscription; // subscription 패키지 의존\n...\npublic class IntegrationSchedule {\n    ...\n    public boolean isSameCategory(final Subscription subscription) {\n        Category category = subscription.getCategory();\n        return category.getId().equals(categoryId);\n    }\n    ...\n}\n```\n\n`subscription` 패키지에 소속된 `Subscriptions` 객체도 `IntegrationSchedule`를 의존하는 형태를 띄고 있다. \n\n```java\npackage com.allog.dallog.domain.subscription.domain;\n...\nimport com.allog.dallog.domain.schedule.domain.IntegrationSchedule; // schedule 패키지 의존\n...\npublic class Subscriptions {\n    ...\n    public Color findColor(final IntegrationSchedule schedule) {\n        return subscriptions.stream()\n                .filter(schedule::isSameCategory)\n                .findAny()\n                .orElseThrow(() -> new NoSuchCategoryException(\"구독하지 않은 카테고리 입니다.\"))\n                .getColor();\n    }\n    ...\n}\n```\n\n이제 문제가 되는 부분을 개선해보자!\n\n먼저 기존에 `IntegrationSchedule`에서 의존하던 `Subscription`을 제거한다.\n\n```java\npackage com.allog.dallog.domain.schedule.domain;\n...\npublic class IntegrationSchedule {\n    ...\n    public boolean isSameCategory(final Category category) {\n        Long categoryId = category.getId();\n        return this.categoryId.equals(categoryId);\n    }\n    ...\n}\n```\n\n변경된 메서드에 맞춰 사용하던 부분까지 적절히 수정한다. \n\n```java\npackage com.allog.dallog.domain.subscription.domain;\n...\nimport com.allog.dallog.domain.schedule.domain.IntegrationSchedule;\n...\npublic class Subscriptions {\n    ...\n    public Color findColor(final IntegrationSchedule schedule) {\n        return subscriptions.stream()\n                .filter(subscription -> schedule.isSameCategory(subscription.getCategory()))\n                .findAny()\n                .orElseThrow(() -> new NoSuchCategoryException(\"구독하지 않은 카테고리 입니다.\"))\n                .getColor();\n    }\n    ...\n}\n```\n\n이제 `subscription` 패키지가 `schedule` 패키지를 `단방향 의존`하고 있다. \n\n다시 한번 IntelliJ를 통해  `Cyclic Dependencies…`를 통해 패키지 순환 구조를 확인해보자.\n\n![](./image-4.png)\n\n패키지 순환 구조가 깔끔히 제거된 것을 확인할 수 있다.\n\n## 정리\n\n간단한 시나리오를 기반으로 순환 참조하는 구조가 어떠한 문제를 야기하는지 알아보았다. 순환 참조는 문제의 발생 지점을 명확하게 파악하기 어렵다. 즉 의존 방향이 `단방향`으로 향하도록 해야 한다.\n\nIntelliJ에 다양한 기능을 활용하면 손쉽게 의존 구조를 파악할 수 있다. 주기적으로 패키지/객체 사이의 의존을 확인하며 프로젝트를 진행하다 보면 보다 더 견고한 서비스를 만들 수 있을 것이라 기대한다.\n\n## References.\n\n- 로버트 C.마틴 지음, 송준이 옮김, 『클린 아키텍처』 (인사이트, 2019), 116p.\n- [https://jojoldu.tistory.com/603?category=1011740](https://jojoldu.tistory.com/603?category=1011740)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 배경 우아한테크코스 5번째 데모데이의 필수 요구사항중 하나는 톰캣의 설정 중 , ,  를 적절한 값으로 설정하고 그 이유를 공유하는 것 입니다. 이 3가지 설정은 톰캣이 한번에 얼마나 많은 요청을 처리하고, 얼마나 많은 커넥션을 생성하고, 운영체제로 하여금 얼마나 많은 TCP 커넥션을 대기시킬지를 결정합니…","fields":{"slug":"/preparing-for-performance-test/"},"frontmatter":{"date":"September 21, 2022","title":"톰캣 튜닝을 위한 달록의 서버 성능 테스트 준비 과정","tags":["부하테스트","jmeter"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 배경\n\n우아한테크코스 5번째 데모데이의 필수 요구사항중 하나는 톰캣의 설정 중 `maxConnections`, `maxThreads`, `acceptCounts` 를 적절한 값으로 설정하고 그 이유를 공유하는 것 입니다. 이 3가지 설정은 톰캣이 한번에 얼마나 많은 요청을 처리하고, 얼마나 많은 커넥션을 생성하고, 운영체제로 하여금 얼마나 많은 TCP 커넥션을 대기시킬지를 결정합니다.\n\n이런 설정은 서비스마다 최적의 값이 다를 것 입니다. 따라서 성능 테스트를 통해 적절한 설정 값을 찾아낼 필요가 있습니다. 따라서 달록팀은 성능 테스트에 대해서 학습하고, 독립된 성능 테스트 환경을 구축하였습니다. 그 과정을 공유합니다.\n\n## 성능, 부하, 스트레스 테스트\n\n![출처: https://www.bmc.com/blogs/load-testing-performance-testing-and-stress-testing-explained/](./performance-test.png)\n\n### 성능 테스트 (Performance Test)\n\n성능 테스트는 특정 상황에서 시스템이 어느 수준의 퍼포먼스를 내는지 측정하기 위한 테스트 방식입니다. 가상의 유저를 시뮬레이션하여 실제와 비슷한 환경을 만들고, 서버에 일정 규모의 부하를 주는 방식으로 테스트합니다. 일반적으로 애플리케이션의 성능을 측정하고, 최적화를 하기 위해 사용합니다.\n\n사실 성능 테스트는 후술할 부하 테스트와 스트레스 테스트를 포함하는 포괄적인 의미입니다.\n\n### 부하 테스트 (Load Test)\n\n부하 테스트는 서비스에 부하를 점진적으로 한계치까지 증가시키는 방식의 성능 테스트입니다. 부하 테스트를 통해 애플리케이션, 데이터베이스 등의 최대 성능을 파악할 수 있습니다. 간단히 말해, 부하 테스트를 수행하면 우리 서비스가 견딜 수 있는 최대치의 요청을 테스트할 수 있습니다.\n\n### 스트레스 테스트 (Stress Test)\n\n스트레스 테스트는 극단적인 상황을 상정하고, 서비스가 극도의 부하 상황에서 어떻게 동작하고 어떻게 복구되는지 확인하기 위해 사용하는 성능 테스트입니다. 정상적인 상황에서 발생할 수 없는 버퍼 오버플로우, 메모리 누수 등의 문제를 발견하고 해결하기 위해 사용합니다. 스트레스 테스트를 통해 애플리케이션이 예기치 않은 트래픽 급증에 다운되지 않도록 대비할 수 있습니다.\n\n현재 달록의 목표는 애플리케이션의 성능을 측정하고, 그 측정 결과를 통해 최적의 톰캣 세팅을 얻는 것 입니다. 따라서 이 경우 부하 테스트, 스트레스 테스트라기 보단 성능 테스트에 해당한다고 볼 수 있겠죠?\n\n## 성능 테스트 도구\n\n### 📏 JMeter (Star 6.5k)\n\n**[JMeter](https://jmeter.apache.org/)** 는 아파치 재단에서 개발한 성능 테스트를 위한 도구입니다. 순수 100% Java로 개발된 어플리케이션 입니다. 웹 애플리케이션 서버 성능 테스트를 위해 개발되었지만, 현재는 FTP, 데이터베이스, TCP 등 여러 프로토콜의 성능을 테스트할 수 있도록 발전하였습니다.\n\nGUI가 Java의 Swing으로 개발되어 그다지 수려하다고 이야기하기는 어렵습니다. 다만, 예전 버전에 비해서는 그 투박한 정도가 많이 개선된 것 같습니다. 개인적으로 디자인을 많이 따지는데 사용하는데 큰 거부감은 없었습니다 😓. non-GUI 환경에서도 실행될 수 있지만, 찾아보니 그닥 많이 사용되는 방법은 아닌 것 같군요.\n\nJMeter는 거의 20년전에 첫 출시하여 지금까지 쭉 릴리즈되고, 많은 사람들에게 사랑받는 입증된 성능 테스트 도구입니다. 거기에 가장 [최근 릴리즈](https://github.com/apache/jmeter/releases/tag/rel%2Fv5.5)가 2022년 6월인것을 보면 안정성과 유지보수는 믿고 가도 되겠죠 😄\n\n### 🪨 nGrinder (Star 1.6k)\n\n[nGrinder](https://github.com/naver/ngrinder)는 서버 성능 테스트를 위해 Naver가 'The Grinder'를 기반으로 개발한 오픈소스 성능 테스트 도구입니다.\n\nnGrinder는 한국 기업 네이버가 개발하고 유지보수하는 만큼 도구 자체에서 한국어를 지원합니다. 다만, (제가 찾지 못한 것 일수도 있지만) 한국어로 작성된 문서는 없습니다. **[nabble](http://ngrinder.373.s1.nabble.com/ngrinder-user-kr-f113.html)** 이라는 한국어 포럼이 존재는 합니다만, 유의미하게 활성화된 것은 아닌 것 같습니다.\n\nnGrinder는 Controller, Agent, Target으로 구성됩니다. Controller는 WAS 기반으로 웹 브라우저로 접속하여 GUI로 사용할 수 있습니다. Agent는 직접 부하를 발생시키는 머신입니다. Controller의 지휘하에 동작합니다. Target은 부하가 발생할 대상 서버를 의미합니다.\n\nWAS 기반으로 동작하기 때문에 젠킨스나 소나큐브 대시보드와 같이 개발자 각각의 계정을 가질 수 있고, 계정 별 부하 테스트 히스토리를 관리할수도 있는 점이 장점입니다.\n\n### 🦗 Locust (Star 19.8k)\n\nLocust는 파이썬으로 개발된 성능 테스트 도구입니다. JMeter나 nGrinder와는 다르게 기본으로 파이썬 코드를 작성하여 성능 테스트를 수행하도록 만들어져있습니다.\n\n### 🤔 달록의 선택\n\nnGrinder는 한국 기업이 만든 점에서 우선 친근감이 드네요 😄. 다만, 낮은 인지도와 커뮤니티가 아쉽다는 생각이 듭니다. 무엇보다 WAS 기반으로 동작하기 때문에 EC2 인스턴스에 설치해야 하는데, 이것이 하나의 허들로 작용하였습니다. 또한 보안그룹 문제로 Agent를 제대로 사용하지 못하는 문제가 존재했습니다. 그렇다고 로컬에서 직접 돌리기엔 JMeter와 차이점이 없게 됩니다.\n\nLocust는 스크립트 기반으로 성능 테스트가 진행됩니다. 스크립트를 작성해서 성능 테스트를 구성하는 것은 개발자 입장에서 꽤 매력적인 부분입니다. 명시적인 코드로 테스트 시나리오를 확인할 수 있고, 시나리오를 형상관리하기도 용이할 것입니다. 다만, 저희 달록팀 백엔드 인원 모두 자바 개발자이다 보니 파이썬 코드로 스크립트를 작성한다는 점이 아쉬웠습니다. 물론, 파이썬은 비교적 언어 학습 비용이 낮지만, 여러 현실적인 여건을 고려하였을 때 크게 적절하다는 판단은 들지 않았습니다.\n\nJMeter는 기본적으로 GUI로 제공되기 때문에 학습 비용이 낮고, 빠르게 배워 테스트 시나리오를 작성할 수 있다는 것이 장점입니다. 특히, 지금과 같이 미션과 프로젝트를 병행하고 있는 지금 학습 비용이 높은 테스트 도구를 사용하는 것이 현실적으로는 좋은 선택은 아니라고 생각됩니다.\n\n또한 JMeter가 가진 깊은 역사로 인한 넓고 단단한 생태계를 누릴 수 있습니다. 비교적 많은 레퍼런스를 찾을 수 있습니다. 또한, 플러그인 생태계도 무시할 수 없습니다. JMeter가 직접 제공하지 않는 기능은 플러그인을 설치하여 사용하면 됩니다. 자체적으로 Plugin Manager 내장하고 있어, 별도의 설정 없이 플러그인을 설치할 수 있습니다.\n\n따라서 저희 달록 팀은 **안정성이 입증되어 있고, 높은 점유율로 트러블 슈팅과 확장성이 용이하고, 초기 학습 비용이 낮아 바로 배워 사용할 수 있는 JMeter를 사용**하였습니다.\n\n> 더 깊이 찾아보지는 못했지만, Gatling(Star 5.7k)나, Vegeta(Star 20.2.k)도 위와 비슷한 이유로 선택되지 못했습니다.\n\n## JMeter 설치\n\nJMeter는 **[Apache JMeter 공식 웹사이트](https://jmeter.apache.org/download_jmeter.cgi)** 에서 바이너리를 다운로드 받을 수 있습니다. 다운로드 받고, `bin` 디렉토리에 있는 파일을 운영체제 환경에 맞게 실행해주시면 됩니다.\n\n맥의 경우 Brew를 사용하여 손쉽게 설치할 수 있습니다. Brew가 설치되어 있는 맥 컴퓨터에서 아래 명령을 실행하여 설치합니다.\n\n```shell\n$ brew install jmeter\n```\n\n아래 명령으로 실행합니다.\n\n```shell\n$ brew install jmeter\n```\n\n## 성능 테스트 환경 설정\n\n![현재 달록의 아키텍처](./full-architecture.png)\n\n부하 테스트는 어디서 수행해야 할까요? 당연히 실제 유저가 존재하는 프로덕션 환경은 안됩니다. 그렇다고 개발용 서버에서 테스트는 할 수 있을까요?\n\n개발전용 서버는 `develop` 브랜치에 병합된 버전의 애플리케이션이 실행되고 있는 환경입니다. 병합된 프론트엔드의 변경사항과 백엔드의 변경사항이 서로 문제를 일으키지 않고 잘 동작하는데 확인하는데 사용됩니다. 이 환경에서 성능 테스트를 수행하기도 곤란합니다.\n\n또한 현재 개발 서버에는 NGINX와 스프링 애플리케이션이 한 인스턴스에서 실행되고 있으며 데이터베이스는 H2를 사용하고 있어 실제 프로덕션과 유사한 성능 테스트 리포트를 얻지 못할수도 있습니다. 즉, 테스트의 신뢰도가 떨어집니다.\n\n![독립된 성능 테스트 환경](./architecture.png)\n\n따라서 달록은 현재 잘 사용중인 프로덕션과 개발서버에 영향을 끼치지 않으면서, 프로덕션과 동일한 환경을 별도로 구축하여 성능 테스트를 수행하기로 하였습니다.\n\n## TPS (Transaction Per Seconds)\n\n자, 이렇게 성능 테스트할 환경이 모두 세팅되었습니다. 그런데 우리는 어떤 지표를 보고 '성능이 개선되었다' 라고 판단할 수 있을까요? 찾아보니 많은 지표가 있었습니다. 그 중 가장 많이 사용되는 지표가 TPS(Transaction Per Seconds) 입니다.\n\nTPS가 무엇일까요? 말 그대로 초당 처리된 트랜잭션 수를 의미합니다. 웹 애플리케이션 맥락에서 트랜잭션은 Request와 Response의 한 쌍이 처리됨을 말합니다. TPS는 정확히 어떻게 계산할까요?\n\n$$\nTPS = VirtualUser/Average Response Time\n$$\n\nVirtual User는 가상 유저, response time은 요청 이후 응답이 되돌아오는 시간을 의미합니다. 만약, 100명의 유저가 접속하고, 평균 repsonse time이 10이라면, 이 서비스의 TPS는 10이 됩니다.\n\n앞으로 달록은 이 TPS라는 지표를 토대로 서비스의 성능이 개선되었음을 판단할 예정입니다.\n\n## JMeter 사용\n\n![](./jmeter.png)\n\n달록은 일단 간단하게 위와 같이 성능 테스트 환경을 설정하였습니다. `jpgc-graphs-basic` 플러그인을 설치하면 핵심 지표인 TPS를 편하게 그래프로 확인할 수 있습니다. JMeter 사용에 대한 자세한 내용과 이를 사용한 톰캣 튜닝 과정은 다음 포스트에서 다뤄보도록 하겠습니다.\n\n## 참고\n\n- https://www.blazemeter.com/blog/performance-testing-vs-load-testing-vs-stress-testing\n- https://www.guru99.com/performance-vs-load-vs-stress-testing.html\n- https://loadium.com/blog/jmeter-vs-locust-part1\n- https://wiki.loadium.com/test-settings/what-is-tps\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 글을 쓰게된 계기 우아한 테크코스 레벨4 Tomcat 구현하기 미션을 진행하면서 직접만든 Tomcat과 실제 Tomcat이 다르게 작동하는 부분이 있었다.\nServlet과 ServletContainer, ServletLifeCycle를 알아보고\n왜 다르게 작동하는지에 대한 의문점을 공유하고자 글을 작성…","fields":{"slug":"/servlet-life-cycle/"},"frontmatter":{"date":"September 17, 2022","title":"서블릿 생명주기와 직접만든 톰캣을 통한 의문점","tags":["tomcat","servlet"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## 글을 쓰게된 계기\n우아한 테크코스 레벨4 Tomcat 구현하기 미션을 진행하면서 직접만든 Tomcat과 실제 Tomcat이 다르게 작동하는 부분이 있었다.\nServlet과 ServletContainer, ServletLifeCycle를 알아보고\n왜 다르게 작동하는지에 대한 의문점을 공유하고자 글을 작성해본다.\n\n## Servlet이란?\n자바로 HTTP 요청을 처리하는 프로그램을 만들 때 사용하는 인터페이스이다.\nServlet은 자바 표준으로 `Jakarta EE`의 표준 API이다.\nServlet은 클라이언트의 `Request`를 받아서 일련의 작업을 처리하고 `Response`에 작업의 결과물을 담아준다.\n\n## Servlet Container란?\n서버에서 만들어진 `Servlet`들을 **관리**하고 **중계**하는 역할을 한다.\n\nServletContainer는 클라이언트의 **요청**을 받았을 때, 요청에 대응할 수 있는 Servlet을 인스턴스화 해주고 **요청**에 대한 **응답**을 해준다.\n\n## ServletContainer의 역할\n> 1. 웹서버와의 통신 지원\nServletContainer는 `Servlet`과 `WebServer`가 손쉽게 통신 할 수 있게 중계 해준다. \nServletContainer는 `ServerSoket`을 만들고 `listen()`, `accept()` 등의 API로 클라이언트 요청이 오면 적절한 `Servlet`에게 요청을 처리하게끔 한다.\n\n> 2. Servlet 생명주기 관리\nServletContainer는 `Servlet`을 관리한다.\n-  Servlet 클래스를 로딩하여 인스턴스화\n-  Servlet 초기화\n- 요청에 적절한 Servlet 할당\n- Servlet 제거\n\nServlet 생명주기를 어떻게 관리하는지 좀 더 자세히 살펴보자.\n\n## Servlet 생명주기를 관리하는 메서드\n``` java\npublic class ExampleServlet extends HttpServlet {\n\n    public static final String 인코딩 = \"인코딩\";\n\n    @Override\n    public void init(final ServletConfig config) throws ServletException {\n    }\n\n    @Override\n    protected void service(final HttpServletRequest request, final HttpServletResponse response) throws IOException {\n    }\n\n    @Override\n    public void destroy() {\n    }\n}\n```\nServletContainer는 `Servlet`들의 생명주기를 그림과 같이 `init()`, `service()`, `destroy()` 메서드를 통해 관리해준다.\n아래에서 전체적인 과정을 간략하게 살펴보고 `Servlet 생명주기`를 자세히 살펴보자.\n\n\n## Servlet 생명주기 과정 알아보기\nServletContainer가 `Servlet`을 어떻게 사용하고 관리하는지 그림을 통해 정리해보자.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/1993301b-561f-419d-9082-a5e668e0c13f/image.png)\n\n\n1. 톰캣이 가지고 있는 Connector 중 하나를 통해 `Request`를 전달 받는다.\n\n2. 톰캣은 `Request`의 `url`을 기반으로 적절한 `Servlet`을 찾아내 매핑한다.\n\n3. 요청이 적절한 `Servlet`에게 매핑되면 메모리에 해당 `Servlet` 인스턴스가 올라와있는지 확인한다. 만약 존재하지 않으면 `Servlet` 인스턴스를 생성한다.\n\n4. 톰캣이 `init()` 메서드를 호출하여 `Servlet`을 초기화한다.\n\n5. `Servlet`이 초기화 되면 톰캣은 `service()` 메서드를 호출하여 `Request`를 처리한다.\n\n6. 마지막으로 톰캣을 종료하거나 `Servlet`을 수정하게되면 `destroy()` 메서드를 호출하여 메모리에 올라와있던 `Servlet`을 제거한다.\n\n## Servlet 생명주기 조금 더 자세히 살펴보기\n![](https://velog.velcdn.com/images/gudonghee2000/post/02b01768-d52a-4820-b829-9361d1328b4c/image.JPG)\n\nServlet은 처음 호출되어 메모리에 올라가면 해당 인스턴스를 재활용한다.\n**그래서 객체가 생성되는 순간부터 자원이 해제되는 순간까지가 서블릿의 라이프사이클이다.** \n\n과정을 하나씩 자세히 살펴보자.\n\n#### 1. Servlet 객체 생성\n먼저 Servlet이 최초에 호출되면 톰캣에서 서블릿 파일(.java)을 불러와 컴파일한 후 메모리에 로드시킨다. 톰캣은 서블릿 파일이 변경되지 않는 이상 서버가 종료될 때까지 계속해서 해당 객체를 재활용한다.\n\n#### 2. init() 메서드 호출 (객체 생성시 최초 1회만 실행)\nServlet에서 기본 상속받는 HttpServlet의 부모 추상 클래스인 GenericServlet에 아래와 같이 정의하고 있는 메서드이다.\n\n``` java\npublic abstract class GenericServlet implements Servlet, ServletConfig, java.io.Serializable {\n\n\t...\n    \n\t@Override\n    public void init(ServletConfig config) throws ServletException {\n        this.config = config;\n        this.init();\n    }\n}\n\n```\nServlet 객체 생성시 초기화 작업을 수행하는데, `ServerConfig` 객체를 넣어 실행한다.\n\n`ServerConfig`객체는 아래에서 다시살펴보자.\n\n\n#### 3. service() 메서드 호출\n클라이언트의 요청에 따라 생성된 `Request`객체를 통해 요청을 처리하고 `Response` 객체에 적절한 응답을 추가해준다.\n이때, 쓰레드풀의 쓰레드 하나를 사용한다.\n\n#### 4. destroy() 메서드 호출 (자원 해제시에 1번만 실행)\nServlet에 수정사항이 생겨 Servlet을 다시 로드해야하거나 서버가 정상적으로 종료되는 경우에 호출된다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/93a0c18e-4db4-4538-ad85-4b291322b430/image.JPG)\n실제 톰캣의 Servlet을 호출할 때의 로그이다.\n처음 Servlet이 호출됬을 때만 `init()` 메서드가 호출됨을 알 수 있다.\n\n## 직접 구현한 톰캣과 달랐던 의문점들\n\n우아한 테크코스 레벨4 저번미션을 진행하면서 직접 Tomcat을 구현해보았었다.\n직접만든 Tomcat과 달랐던 부분이있어서 이를 공유하고자 한다.\n\n### 1. Servlet 실행시점에 Servlet 객체 생성\n\n직접 구현해본 톰캣에서는 다음과 같이 `Tomcat` 실행시점에 `Servlet` 인스턴스를 생성하였다.\n\n``` java\npublic class Tomcat {\n\t...\n\n    public void start() {\n        ControllerContainer container = new ControllerContainer(configuration);\n        var connector = new Connector(container);\n      \t...\n    }\n}\n```\n`ControllerContainer`가 ServletCotainer이고 `ControllerContainer` 생성자 파라미터의 `Configuration` 객체에 `Servlet`\n 인스턴스 생성에 대한 코드가 담겨있다.\n\n그런데, 실제 톰캣은 클라이언트의 `Request`가 들어오고 매핑할 `Servlet`이 필요해질 때 `Servlet` 인스턴스를 생성한다😮😮\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/93a0c18e-4db4-4538-ad85-4b291322b430/image.JPG)\n\n실제 테스트 코드에서 `쉐어드 서블릿`이라는 객체에 요청을 보냈을 때 확인해 본 결과이다.\n\n이런 결과를 보고 \n\n**톰캣을 실행 할 때, Servlet 객체를 모두 생성 해놓으면 클라이언트 요청이 들어올때 더 빨리 응답을 해줄 수 있지않나? 라는 의문이 들었다.**\n\n아마, `Servlet`에 변경사항이 있을 때, 실제 Server를 종료시키지 않고 `Servlet` 변경사항을 적용하기 위해서 클라이언트 **요청**이 들어올 때, `Servlet` 인스턴스를 생성하도록 구현 한게 아닐까 조심스럽게 추측한다🙄 \n\n### 2. Servlet 객체 생성과 init()을 따로하는 이유\n두번째는 `Servlet`을 생성 하고 `init()` 메서드를 호출한다는 점이다.\n**Servlet을 생성 할 때 초기화도 같이 하면 되는거 아닌가? 왜 과정을 나눠놨지? 라는 의문이 들었다.**\n\n찾아본 자료에서 몇가지 이유를 보았는데 일리가 있다고 생각하는 이유는 다음과 같다.\n\n- Interface는 생성자를 가질수 없기 때문이다.\n`Servlet` 초기화를 위해서는 앞에서 이야기한 `ServletConfig` 객체를 `Servlet`에게 전달해주어야 하는데 Interface는 생성자를 가질 수 없다.\n그래서 `Java EE Servlet API` 표준에 `Servlet` 객체를 초기화하기 위해서는 `ServletConfig` 객체가 필요함을 명시해주기 위해서 `init()`메서드를 통해 초기화 하도록 한것이 아닐까 한다.   \n> \n**💡ServletConfig란?**\n하나의 Servlet 초기화에 필요한 정보를 전달하기 위한 Config 객체를 의미한다.\n\n- 생성자는 복잡한 논리/비즈니스 처리가 없어야 하기때문이다. `ServletConfig`객체를 생성하는 것은 비용이 비싼작업이기 때문에 생성자가 아닌 `init()` 메서드를 통해 처리해야 한다는 의견도 있다.\n\n## 마치면서\n`Servlet`과 `ServletContainer`, `Servlet` 생명주기에 대해서 살표보았다. \n`Servlet` 생명주기에 대해서 같은 의문점이 있었다면 의문을 해소하는데 도움이 되었으면 좋겠다😊\n\n## 참고한 자료\nhttps://stackoverflow.com/questions/7580771/can-we-replace-the-purpose-of-init-method-to-the-servlet-constructor\n\nhttps://codevang.tistory.com/193\n\nhttps://velog.io/@jihoson94/Servlet-Container-%EC%A0%95%EB%A6%AC\n\nhttps://bashbeta.com/ko/%EC%84%9C%EB%B8%94%EB%A6%BF%EC%97%90%EC%84%9C-%EC%83%9D%EC%84%B1%EC%9E%90%EB%A5%BC-%ED%98%B8%EC%B6%9C%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9E%85%EB%8B%88%EA%B9%8C/"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 순환 참조란, 서로 다른 여러 빈이 상호 간의 의존성을 가져 의존성이 순환하고 있다는 것을 의미한다. 예를 들면 아래와 같은 상황이다. 우리 달록은 Member가 생성되는 시점에 개인의 일정을 저장하기 위한 Category와 개인 Cateogry에 대한 Subscription 정보가 추가되어야 하는 요…","fields":{"slug":"/circular-reference-1/"},"frontmatter":{"date":"September 10, 2022","title":"순환 참조 문제 개선기 (1)","tags":["매트","BE","순환 참조","circular reference","의존성","트러블 슈팅"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n순환 참조란, 서로 다른 여러 빈이 상호 간의 의존성을 가져 의존성이 순환하고 있다는 것을 의미한다. 예를 들면 아래와 같은 상황이다.\n\n우리 달록은 Member가 생성되는 시점에 개인의 일정을 저장하기 위한 Category와 개인 Cateogry에 대한 Subscription 정보가 추가되어야 하는 요구사항을 가지고 있다. 이것을 코드로 표현하면 아래와 같다.\n\n```java\n@Service\npublic class MemberService {\n\n    private static final String MY_SCHEDULE = \"내 일정\";\n\n    private final CategoryService categoryService; // 순환 참조\n    private final SubscriptionService subscriptionService;\n    private final MemberRepository memberRepository;\n\n    public MemberService(final CategoryService categoryService, final SubscriptionService subscriptionService,\n                         final MemberRepository memberRepository) {\n        this.categoryService = categoryService;\n        this.subscriptionService = subscriptionService;\n        this.memberRepository = memberRepository;\n    }\n\n    @Transactional\n    public Member save(final String email, final String displayName) {\n        Member newMember = memberRepository.save(new Member(email, displayName));\n\n        Category newCategory = categoryService.save(newMember.getId(), MY_SCHEDULE);\n        subscriptionService.save(newMember.getId(), newCategory.getId());\n\n        return newMember;\n    }\n\n    public Member findById(final Long id) {\n        return memberRepository.findById(id)\n                .orElseThrow(NoSuchElementException::new);\n    }\n}\n```\n\n앞서 언급한 것처럼 Member 생성 시 아래와 같이 내 일정을 저장하기 위한 Category와 내 일정을 바로 Subscription 하기 위한 로직이 필요하다. 즉 `MemberService`는 `CategoryService`와 `SubscriptionService`를 필드로 가지며 `의존`하는 형태를 띄게 된다. \n\n```java\n@Service\npublic class CategoryService {\n\n    private final MemberService memberService; // 순환 참조\n    private final CategoryRepository categoryRepository;\n\n    public CategoryService(final MemberService memberService, final CategoryRepository categoryRepository) {\n        this.memberService = memberService;\n        this.categoryRepository = categoryRepository;\n    }\n\n    @Transactional\n    public Category save(final Long memberId, final String name) {\n        Member member = memberService.findById(memberId);\n        Category newCategory = new Category(name, member);\n        return categoryRepository.save(newCategory);\n    }\n\n    public Category findById(final Long id) {\n        return categoryRepository.findById(id)\n                .orElseThrow(NoSuchElementException::new);\n    }\n}\n```\n\n위 코드는 Category 저장을 위한 로직을 담은 `CategoryService`이다. `CategoryService`의 경우 요청된 `Member Id` 정보를 기반으로 Member를 조회한 뒤 Category를 저장하게 된다.\n\n이러한 구조 덕분에 `MemberService`는 `CategoryService`를 의존하고, `CategoryService`는 `MemberService`를 의존하게 되어 `순환 참조`가 발생하게 된다.\n\n```shell\n***************************\nAPPLICATION FAILED TO START\n***************************\n\nDescription:\n\nThe dependencies of some of the beans in the application context form a cycle:\n\n┌─────┐\n|  categoryService defined in file [/Users/hyeonic/study/blog-code/circular-reference/build/classes/java/main/io/github/hyeonic/circularreference/category/CategoryService.class]\n↑     ↓\n|  memberService defined in file [/Users/hyeonic/study/blog-code/circular-reference/build/classes/java/main/io/github/hyeonic/circularreference/member/MemberService.class]\n└─────┘\n\n\nAction:\n\nRelying upon circular references is discouraged and they are prohibited by default. Update your application to remove the dependency cycle between beans. As a last resort, it may be possible to break the cycle automatically by setting spring.main.allow-circular-references to true.\n```\n\n## 의존성 살펴보기\n\n객체가 다른 객체의 의존성을 가진다는 것은 해당 객체의 변화에 직접적인 영향을 받을 수 있다는 것이다. 위 예시를 다시 한번 살펴보자.\n\n```java\n@Service\npublic class MemberService {\n\n    private static final String MY_SCHEDULE = \"내 일정\";\n\n    private final CategoryService categoryService; // 순환 참조\n    ...\n\n    @Transactional\n    public Member save(final String email, final String displayName) {\n        Member newMember = memberRepository.save(new Member(email, displayName));\n        \n        // Category save 메서드의 시그니처나 반환 타입이 변경 된다면..?\n        Category newCategory = categoryService.save(newMember.getId(), MY_SCHEDULE);\n        subscriptionService.save(newMember.getId(), newCategory.getId());\n\n        return newMember;\n    }\n    ...\n}\n```\n\n만약 Category에 대한 요구사항이 변경되어 `save` 메서드의 시그니처나 반환 타입에 대한 변경에 `MemberService`는 직접적인 코드 수정으로 변화에 대응해야 한다.\n\n그만큼 객체가 객체를 의존한다는 것은 의존 객체의 변경에 영향을 받을 수 있다는 것을 의미한다. 이러한 의존이 순환되어 연결되어 있다고 생각해보자. 순환 참조에 해당하는 객체 모두 연쇄적으로 변경에 영향을 받을 수 있다는 것을 의미한다.\n\n![](./circular-reference.png)\n\n사실 순환 참조를 해결하기 위한 방법으로는 `@Lazy` 애노테이션을 활용하거나 `setter` 주입을 사용하는 등 다양한 방법이 존재한다. 하지만 결국 이러한 순환 참조를 야기하는 설계는 근본적으로 각 객체에 대한 책임이 적절하지 않다는 의미를 내포할 수 있다. \n\n다음 시간에는 앞서 발생한 순환 참조를 달록에서 어떻게 개선하고 있는지 시리즈로 작성할 예정이다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. @JsonProperty, @JsonNaming 구글 측에서 OpenID connect Sever flow를 살펴보면 사용자 인증을 진행할 경우 아래와 같은 흐름으로 진행되는 것을 확인할 수 있다. Create an anti-forgery state token: 위조 방지 상태 토큰 만들기 Send a…","fields":{"slug":"/json-property-json-naming/"},"frontmatter":{"date":"September 10, 2022","title":"@JsonProperty, @JsonNaming","tags":["매트","역직렬화","BE","JsonProperty","JsonNaming"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## @JsonProperty, @JsonNaming\n\n구글 측에서 [OpenID connect Sever flow](https://developers.google.com/identity/protocols/oauth2/openid-connect#server-flow)를 살펴보면 사용자 인증을 진행할 경우 아래와 같은 흐름으로 진행되는 것을 확인할 수 있다.\n\n1. Create an anti-forgery state token: 위조 방지 상태 토큰 만들기\n2. Send an authentication request to Google: Google에서 인증 요청 보내기\n3. Confirm anti-forgery state token: 위조 방지 상태 토큰 확인\n4. `Exchange code for access token and ID token: code를 액세스 토큰 및 ID 토큰으로 교환`\n5. Obtain user information from the ID token: ID 토큰에서 사용자 정보 가져오기\n6. Authenticate the user: 사용자 인증\n\n`4`를 살펴보면 서버가 액세스 토큰 및 ID 토큰을 교환받기 위해 일회성 승인 코드인 `code`를 활용하여 아래와 같이 요청을 보내야 한다.\n\n```\nPOST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\ncode=4/P7q7W91a-oMsCeLvIaQm6bTrgtp7&\nclient_id=your-client-id&\nclient_secret=your-client-secret&\nredirect_uri=https%3A//oauth2.example.com/code&\ngrant_type=authorization_code\n```\n\n위 요청을 보낸 뒤 성공적으로 처리되면 아래와 같은 응답을 확인할 수 있다.\n\n```json\n{\n    \"access_token\": ...,\n    \"expires_in\": ...,\n    \"id_token\": ...,\n    \"token_type\": ...,\n    \"refresh_token\": ...\n} \n```\n\n주목해야 할 것은 google 측에서 token 정보를 `snake_case` 형식으로 제공하고 있다는 것을 기억하자.\n\n## snake_case to CamelCase\n\ngoogle 측에서 제공된 token 정보의 경우 `snake_case`로 되어 있기 때문에 해당 데이터를 Java 객체로 변환하기 위해서는 아래와 같이 필드명을 작성해야 정상적으로 변환이 가능하다.\n\n```java\npublic class GoogleTokenResponse {\n\n    private String access_token;\n    private String refresh_token;\n    private String id_token;\n    private String expires_in;\n    private String token_type;\n    private String scope;\n\n    private GoogleTokenResponse() {\n    }\n    ...\n    \n    public String getAccess_token() {\n        return access_token;\n    }\n    ...\n}\n```\n\n그러나 Java에서는 기본적으로 `CamelCase`를 사용하고 있다. 코드 내부에 `CamelCase`와 `snake_case`가 함께 뒹굴고 있으면 일관성이 떨어진 코드를 마주하게 될 것이다. 우리는 `snake_case`로 제공 받은 데이터를 `역직렬화`할 때 `CamelCase`로 변환하는 방법에 대해 고민하게 되었다.\n\n> 직렬화: Java 객체 → 전송 가능한 데이터 <br> \n> 역직렬화: 전송 가능한 데이터 → Java 객체\n\n### @JsonProperty\n\n`@JsonProperty`는 필드나 메서드 위에 선언되어 `직렬화/역직렬화`될 때 매핑 하기 위한 `property 명을 지정`한다. 아래와 같이 적용이 가능하다.\n\n```java\npublic class GoogleTokenResponse {\n\n    @JsonProperty(\"access_token\")\n    private String accessToken;\n\n    @JsonProperty(\"refresh_token\")\n    private String refreshToken;\n\n    @JsonProperty(\"id_token\")\n    private String idToken;\n\n    @JsonProperty(\"expires_in\")\n    private String expiresIn;\n\n    @JsonProperty(\"token_type\")\n    private String tokenType;\n    \n    private String scope;\n\n    private GoogleTokenResponse() {\n    }\n    ...\n}\n```\n\n다만 변환해야 하는 필드 값이 늘어날 경우 필드마다 해당 애노테이션을 명시해야 한다. 또한 문자열로 작성하기 때문에 오타로 인해 정상적으로 변환하지 못할 가능성 또한 존재한다.\n\n### @JsonNaming\n\ngoogle 측에서 제공되는 token 정보의 경우 `snake_case`의 일정한 전략을 활용하여 응답하고 있다. 이때 변환하고자 하는 클래스 위에 `@JsonNaming`을 통해 일괄적으로 적용이 가능하다.\n\n```java\n@JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class)\npublic class GoogleTokenResponse {\n\n    private String accessToken;\n    private String refreshToken;\n    private String idToken;\n    private String expiresIn;\n    private String tokenType;\n    private String scope;\n\n    private GoogleTokenResponse() {\n    }\n    ...\n}\n```\n\n## TODO\n\n간단히 역직렬화 시점에 `snake_case`로 명시된 JSON 데이터를 `CamelCase` 필드로 변환하는 방식에 대해 알 수 있었다. 하지만 이 밖에도 직렬화/역직렬화를 진행할 때 Jackson에서 제공되는 애노테이션의 종류는 다양하다. 추후 어떠한 시점에 해당 애노테이션이 작용되며 직렬화와 역직렬화 시점에 필수적으로 필요한 것들에 대해 알아보려 한다. \n\n## References.\n\n[OpenID Connect](https://developers.google.com/identity/protocols/oauth2/openid-connect)<br>\n[Jackson Annotation Examples](https://www.baeldung.com/jackson-annotations)\n"},{"excerpt":"Flyway 란? Flyway는 오픈소스 데이터베이스 마이그레이션 툴 입니다. 데이터베이스 마이그레이션 툴이란 데이터베이스의 변경 사항을 추적하고, 업데이트나 롤백을 보다 쉽게 할 수 있도록 도와주는 도구입니다. 데이터베이스 마이그레이션을 왜 해야할까요?  프로젝트를 진행하다 보면, 데이터베이스가 위 그림처럼 여러 환경에서 존재하게 됩니다. 각자의 로컬 …","fields":{"slug":"/dallog-flyway/"},"frontmatter":{"date":"September 08, 2022","title":"달록의 데이터베이스 마이그레이션을 위한 Flyway 적용기","tags":["DevOps"]},"rawMarkdownBody":"\n## Flyway 란?\n\nFlyway는 오픈소스 **데이터베이스 마이그레이션 툴** 입니다. 데이터베이스 마이그레이션 툴이란 데이터베이스의 **변경 사항을 추적하고, 업데이트나 롤백을 보다 쉽게** 할 수 있도록 도와주는 도구입니다.\n\n데이터베이스 마이그레이션을 왜 해야할까요?\n\n![](./flyway.png)\n\n프로젝트를 진행하다 보면, 데이터베이스가 위 그림처럼 **여러 환경**에서 존재하게 됩니다. 각자의 로컬 환경에 데이터베이스가 존재하고, 테스트 서버에도 프로덕션 서버에도 각각 별도의 데이터베이스가 존재합니다. 각자의 로컬 환경에서 개발을 하게 되면, 엔티티의 구조가 변경되고 이로 인해 데이터베이스의 **스키마도 변경**될 것 입니다.\n\n소프트웨어의 소스 코드와 같은 경우에는 Git과 같은 형상관리 툴을 사용하여 이런 문제를 비교적 잘 해결해왔습니다. 하지만, 데이터베이스는 그렇지 않았습니다.\n\n예를 들어 JPA를 사용하는 환경에서 엔티티 구조가 변경되면 이 변경된 구조를 다른 배포 환경의 데이터베이스에도 적용해야합니다. 즉, **일일히 스키마 수정을 위한 DDL을 각 환경별로 모두 실행**해줘야합니다.\n\n물론, 로컬 개발 환경이나 개발 서버에서는 Hibernate 설정 중 `ddl-auto` 을 `create` , `create-drop` , `update` 등으로 설정하여 DDL을 변경된 엔티티 구조에 맞춰 실행할 수 있지만, **프로덕션에서는 이와 같은 설정이 불가능**합니다.\n\n```java\n@Table(name = \"members\")\n@Entity\npublic class Member extends BaseEntity {\n\n    // ...\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    @Column(name = \"id\")\n    private Long id;\n\n    @Column(name = \"email\", nullable = false)\n    private String email;\n\n    @Column(name = \"display_name\", nullable = false)\n    private String displayName;\n\n    @Column(name = \"profile_image_url\", nullable = false)\n    private String profileImageUrl;\n\n    @Enumerated(value = EnumType.STRING)\n    @Column(name = \"social_type\", nullable = false)\n    private SocialType socialType;\n```\n\n위 코드는 달록의 `Member` 엔티티 코드입니다. 만약 달록이 이후에 서비스 가입 시 휴대전화 번호도 추가로 제공받게 된다면, 아래와 같은 필드가 엔티티에 추가될 것 입니다.\n\n```java\n@Column(name = \"contact\", nullable = false)\nprivate String contact;\n```\n\n사실 로컬 개발 환경이나, 개발용 서버에서는 큰 문제가 없습니다. 테이블을 DROP 하고 엔티티 구조에 맞게 CREATE 하면 되니까요. 하지만, **프로덕션 서버에서 테이블을 DROP 하는 것은 미친짓**이죠. 실제 유저들의 데이터가 저장되어있기 때문입니다.\n\n그렇다면 프로덕션 서버에서는 어떻게 테이블 스키마가 변경된 것에 대응할 수 있을까요? `ALTER TABLE` 과 같이 테이블 스키마를 변경하는 **DDL을 사용**해야합니다.\n\n```sql\nALTER TABLE members ADD COLUMN contact VARCHAR(255) NOT NULL;\n```\n\n엔티티 구조가 별로 변경되지 않았다면, 사람 손으로 위와 같은 DDL을 작성하고 프로덕션 서버에서 실행할 수는 있겠습니다. 하지만, 굉장히 번거로운 작업이며 **사람의 실수(human error)**가 발생하기 쉽습니다. 또한 **형상관리도 어렵습니다.**\n\n포스팅에서 설명할 Flyway와 같은 데이터베이스 마이그레이션 툴을 사용하면, 위와 같은 문제를 해결할 수 있습니다. 이번 포스팅에서는 Flyway에 대한 기본적인 개념과 달록이 프로젝트에 적용한 과정에 대해 정리해봅니다.\n\n## 실습 준비\n\n### 실습 환경\n\n실습은 아래와 같은 환경에서 진행합니다. JPA를 사용하는 점 참고해주세요.\n\n- SpringBoot\n- Spring Data JPA (Hibernate)\n- MySQL 8.0\n\n### JPA 엔티티\n\n```java\n@Entity\npublic class Member {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    private String name;\n    private String email;\n    private String password;\n\n\t\t// ...\n```\n\n위와 같은 JPA 엔티티가 존재한다고 가정합니다.\n\n## Flyway 실습 환경 구축\n\n원래라면 Gradle에 스크립트를 작성하는 등 명령줄 인터페이스에서 Flyway를 사용해야 합니다 ([참고](https://flywaydb.org/documentation/getstarted/firststeps/gradle)). 하지만, **Spring Boot는 기본적으로 Flyway와 Liquibase라는 고수준의 데이터베이스 마이그레이션 도구를 지원**합니다. ([참고](https://docs.spring.io/spring-boot/docs/2.1.1.RELEASE/reference/html/howto-database-initialization.html#howto-use-a-higher-level-database-migration-tool)) 따라서 Spring Boot 에서는 더 쉽게 Flyway를 프로젝트에 적용할 수 있습니다. 이번 포스팅에서는 Spring Boot에서 Flyway를 적용하는 방법에 대해서 다룹니다.\n\n### MySQL 컨테이너 띄우기\n\n> 꼭 도커를 사용할 필요는 없습니다. 본 포스팅에서는 빠른 실습 환경 구축을 위해 도커를 사용합니다.\n\n```yaml\nversion: \"3\"\nservices:\n  mysql-db:\n    image: mysql:8.0\n    volumes:\n      - ./mysql:/var/lib/mysql\n    ports:\n      - 3306:3306\n    environment:\n      MYSQL_ROOT_PASSWORD: root\n      MYSQL_DATABASE: flyway-study\n    platform: linux/x86_64\n```\n\n`docker-compose.yml` 파일을 생성하고, 위 내용으로 채워줍니다. 아래 명령으로 MySQL 도커 컨테이너를 간단하게 띄울 수 있습니다.\n\n```bash\n$ docker-compose up -d\n```\n\n### 의존성 추가\n\n`build.gradle` 의 `dependencies` 에 아래 내용을 추가합니다.\n\n```sql\nimplementation 'org.flywaydb:flyway-core'\n```\n\n만약 여러분의 DBMS가 MySQL 8.X 버전이거나, MariaDB를 사용하신다면, 아래와 같이 종속성을 추가해야합니다.\n\n```sql\nimplementation 'org.flywaydb:flyway-mysql'\n```\n\n### DataSource 설정\n\n`application.yml` (혹은 `application.properties`) 에 DataSource 관련 설정을 추가 해야합니다. Spring Boot는 **DataSource 설정으로 Flyway를 자동으로 연결**하기 때문입니다.\n\n```yaml\nspring:\n  datasource:\n    driver-class-name: com.mysql.cj.jdbc.Driver\n    url: jdbc:mysql://localhost:3306/flyway-study\n    username: root\n    password: root\n```\n\n### JPA 설정\n\n```yaml\nspring:\n\t# ...\n\tjpa:\n    hibernate:\n      ddl-auto: validate\n```\n\n`ddl-auto` 옵션을 `validate` 로 설정하면, 실제 데이터베이스 스키마와 JPA 엔티티의 구조가 서로 같은지 비교하고, 같지 않다면 어플리케이션을 실행하지 못하도록 합니다. 이 옵션을 사용하여 Flyway를 사용했을 때 올바르게 데이터베이스 마이그레이션이 진행되었는지 확인해볼 것입니다.\n\n### Flyway 활성화\n\n```yaml\nspring:\n\t# ...\n\tflyway:\n\t    enabled: true\n```\n\n위 설정을 추가하여 Spring Boot에서 Flyway를 활성화 합니다.\n\n## 첫번째 마이그레이션 스크립트 작성\n\nFlyway는 마이그레이션 스크립트의 버전 순서대로 SQL 스크립트를 실행합니다. 우리는 아직 아무런 마이그레이션 스크립트를 작성하지 않았습니다. 최초로 실행될 마이그레이션 스크립트를 아래와 같이 작성합니다. 파일 경로는 `resources/db/migration` 이며 파일명은 `V1__init.sql` 로 생성해주세요. 이때, **언더스코어**(`_`)**가 2개임을 주의**합니다.\n\n```sql\nCREATE TABLE member (\n    id BIGINT AUTO_INCREMENT,\n    name VARCHAR(255),\n    email VARCHAR(255),\n    password VARCHAR(255),\n    PRIMARY KEY (id)\n);\n```\n\n마이그레이션의 버전과 명명법은 조금 있다가 다루겠습니다.\n\n## JPA 엔티티 구조 변경\n\n시간이 흘러 애플리케이션이 기능 변경을 거듭하고, 회원가입 시 연락처 정보도 제공받게 되었습니다. JPA 엔티티 구조는 아래와 같이 변경되겠죠?\n\n```java\n@Entity\npublic class Member {\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n\n    private String name;\n    private String email;\n    private String password;\n    private String contact; // 추가된 필드\n```\n\n그리고, 애플리케이션을 실행해봅니다. 예상하신것처럼 아래와 같이 JPA에서 에러가 발생하며 애플리케이션이 실행되지 않습니다.\n\n```\nSchema-validation: missing column [contact] in table [member]\n```\n\n`ddl-auto` 를 `validate` 로 설정했기 때문입니다. 실제 테이블에는 `contact` 라는 컬럼이 존재하지 않았는데, `Member` 엔티티에는 `contact` 필드가 추가되어 스키마가 불일치해서 발생하는 에러입니다.\n\n## 새로운 버전의 마이그레이션 스크립트 작성\n\n새로운 버전의 마이그레이션 스크립트를 생성하여, 현재 데이터베이스 테이블의 스키마를 엔티티와 일치하도록 만들어봅시다. `V2__add_contact.sql` 라는 이름으로 동일하게 `resources/db/migration` 디렉토리에 추가하고, 아래의 내용을 채워넣습니다.\n\n```sql\nALTER TABLE member ADD COLUMN contact VARCHAR(255);\n```\n\n`ALTER TABLE` 은 이미 존재하는 테이블의 컬럼을 추가, 제거, 수정 등의 작업을 하기 위해 사용되는 DDL 입니다.\n\n## 마이그레이션 스크립트 명명 규칙\n\n![](./migration-naming-convention.png)\n\n기본적으로 Flyway의 마이그레이션 스크립트의 파일 이름 명명법은 위를 따릅니다.** 숫자가 작은 버전의 마이그레이션부터 숫자가 큰 버전 순서대로 스크립트가 실행**됩니다.** 1부터 2, 3, 4, … 처럼 순차적으로 늘어나는 방식으로 사용해도 좋고, 20220101, 20220115, … 처럼 날짜 형태로 사용해도 좋습니다.**\n\n단, 버전은 정수로 인식되므로 `3.10` 과 `3.2` 중 `3.2` 가 먼저 실행됨을 주의하셔야합니다.\n\n### Versioned Migrations\n\nFlyway의 핵심 기능입니다. 마이그레이션 스크립트의 최신 버전과 현재 데이터베이스의 스키마 버전을 비교하고,**차이점이 있다면 마이그레이션 스크립트를 순차적으로 실행하여 최신 스키마와 격차를 좁혀** 나갑니다.\n\n마이그레이션 버전이 1부터 9까지 있다고 가정해봅시다. 현재 개발환경의 데이터베이스 스키마가 5버전일 때, 최신 버전에 대해 마이그레이션을 실행하면, 마이그레이션 스크립트 6 ~ 9 버전이 순차적으로 실행됩니다. 개발 환경의 최신 스키마가 9버전이면, 아무 스크립트도 실행되지 않습니다.\n\n최신 마이그레이션 버전의 숫자보다 작은 숫자의 버전으로 마이그레이션 스크립트를 추가한다면, 그 마이그레이션 스크립트는 무시됩니다. 예를 들어 최신 버전이 `V10` 인데, `V9` 를 이후에 추가하는 경우입니다. 즉, 마이그레이션 스크립트를 추가할 때에는 **항상 최신 마이그레이션 스크립트의 버전보다 큰 숫자로 버전을 설정**해야합니다.\n\n### Undo Migrations\n\n이 기능은 Flyway Teams 라는 유료 버전에서만 사용할 수 있습니다. Undo 기능은 때때로 위험할 수 있으므로 주의해서 사용해야한다고 합니다 ([참고](https://flywaydb.org/documentation/command/undo)).\n\n### Repeatable Migrations\n\n모든 마이그레이션 스크립트가 실행된 이후 실행되는 스크립트 입니다. Repeatable Migrations 끼리는 description 순서대로 실행됩니다. **한번 실행되며, 파일이 변경되어 체크섬이 변경되면 또 실행**됩니다.\n\n## flyway\\_schema\\_history\n\n![](./flyway_schema_history.png)\n\n데이터베이스를 확인해보면, 우리가 생성하지 않은 테이블이 하나 생성된 것을 확인할 수 있습니다. `flyway_schema_history` 라는 이름의 테이블인데요, Flyway는 이 테이블을 사용하여, 마이그레이션에 대한 버전 관리를 합니다. 한번 `V2__add_contact.sql` 을 제거하고, `V1__init.sql` 에 `contact` 컬럼을 추가해볼까요? `V1__init.sql` 을 아래와 같이 변경하고, 애플리케이션을 실행해봅시다.\n\n```sql\nCREATE TABLE member (\n    id BIGINT AUTO_INCREMENT,\n    name VARCHAR(255),\n    email VARCHAR(255),\n    password VARCHAR(255),\n    contact VARCHAR(255),\n    PRIMARY KEY (id)\n);\n```\n\n애플리케이션이 잘 실행되나요? Flyway에서 아래와 같은 에러가 발생할 것입니다.\n\n```\nCaused by: org.flywaydb.core.api.exception.FlywayValidateException: Validate failed: Migrations have failed validation\nMigration checksum mismatch for migration version 1\n-> Applied to database : -714805521\n-> Resolved locally    : 1143495658\nEither revert the changes to the migration, or run repair to update the schema history.\n```\n\nFlyway는 각 마이그레이션 스크립트 별로 체크섬을 비교하여 유효성을 검사합니다. 따라서, 스키마에 대한 모든 변경은 반드시 새로운 버전의 마이그레이션 스크립트를 추가하는 방법으로 진행해야합니다.\n\n## 주의점\n\nSpring Boot는 기본적으로 `schema.sql` 과 `data.sql` 을 통해 데이터베이스 스키마를 초기화하고, 데이터를 추가할 수 있는 기능을 제공합니다. 하지만, 이 기능은 당연하지만 Flyway와 함께 사용해서는 안됩니다 ([참고](https://docs.spring.io/spring-boot/docs/current/reference/html/howto.html#howto.data-initialization.using-basic-sql-scripts)).\n\n또한, 엔티티의 구조가 변경되었을때 반드시 마이그레이션 스크립트를 작성해야합니다. 일반적으로 프로덕션에서는 `ddl-auto` 를 `validate` 로 설정합니다.\n\n## 참고\n\n- [https://www.code4copy.com/java/spring-boot-flyway-db-migration-integration-example/](https://www.code4copy.com/java/spring-boot-flyway-db-migration-integration-example/)\n- [https://www.techgeeknext.com/spring-boot/spring-boot-flyway-example](https://www.techgeeknext.com/spring-boot/spring-boot-flyway-example)\n- [https://blog.gangnamunni.com/post/introducing-flyway/](https://blog.gangnamunni.com/post/introducing-flyway/)\n- [https://flywaydb.org/documentation/](https://flywaydb.org/documentation/)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 달록 서브모듈 도입기 Spring Boot를 활용하여 애플리케이션을 개발할 때 ,  통해 다양한 설정 정보를 지정할 수 있다. 우리 달록도 을 통해 애플리케이션 구동 시 필요한 정보들을 명시하여 사용하고 있다. 하지만 이러한 설정 정보는 애플리케이션이 실행되는 환경에 따라 분리되어야 하며 private…","fields":{"slug":"/submodule/"},"frontmatter":{"date":"August 26, 2022","title":"달록 서브모듈 도입기","tags":["매트","BE","submodule","서브모듈"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## 달록 서브모듈 도입기\n\nSpring Boot를 활용하여 애플리케이션을 개발할 때 `application.properties`, `application.yml` 통해 다양한 설정 정보를 지정할 수 있다. 우리 [달록](https://github.com/woowacourse-teams/2022-dallog)도 `application.yml`을 통해 애플리케이션 구동 시 필요한 정보들을 명시하여 사용하고 있다.\n\n하지만 이러한 설정 정보는 애플리케이션이 실행되는 환경에 따라 분리되어야 하며 private한 정보까지 명시해야 했다. 우리 달록은 이것을 이루기 위해 환경 변수를 활용하여 설정 정보를 명시하였다.\n\n```yaml\nspring:\n  sql:\n    init:\n      mode: always\n\n  datasource:\n    url: jdbc:h2:~/dallog;MODE=MYSQL;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE\n    username: sa\n\n  jpa:\n    properties:\n      hibernate:\n        format_sql: true\n    show-sql: true\n\n    hibernate:\n      ddl-auto: create\n    defer-datasource-initialization: true\n\n  data:\n    web:\n      pageable:\n        max-page-size: 100\n\n  h2:\n    console:\n      enabled: true\n\nlogging:\n  level:\n    root: ${LOGGING_LEVEL}\n  file:\n    name: \"./logs/dallog.log\"\n  logback:\n    rollingpolicy:\n      file-name-pattern: \"./logs/dallog-%d{yyyy-MM-dd}-%i.log\"\n      max-file-size: 5MB\n      total-size-cap: 100MB\n\noauth:\n  google:\n    client-id: ${GOOGLE_CLIENT_ID}\n    client-secret: ${GOOGLE_CLIENT_SECRET}\n    redirect-uri: ${GOOGLE_REDIRECT_URI}\n    oauth-end-point: https://accounts.google.com/o/oauth2/v2/auth\n    response-type: code\n    scopes:\n      - https://www.googleapis.com/auth/userinfo.profile\n      - https://www.googleapis.com/auth/userinfo.email\n    token-uri: ${GOOGLE_TOKEN_URI}\n    grant-type: authorization_code\n\nreport:\n  webhook:\n    discord: ${DISCORD_WEBHOOK_URI}\n\nsecurity:\n  jwt:\n    token:\n      secret-key: ${JWT_SECRET_KEY}\n      expire-length: ${JWT_EXPIRE_LENGTH}\n```\n\n환경 변수는 `${}`와 같은 형식으로 작성된다. 가장 큰 장점은 빠르게 적용이 가능하다는 것이다. 하지만 모든 팀원과 개발 및 운영 서버에 환경 변수를 물리적으로 세팅해두어야 한다. 만약 개발 중 환경 변수가 추가된다면 모든 팀원이 동일하게 추가해야 하며 개발 및 운영 환경에도 동일하게 세팅해주어야 하는 단점이 있다.\n\n## 서브모듈 도입\n\n앞서 언급한 단점들로 우리는 환경 변수를 통한 설정 정보 관리의 불편함을 느끼게 되었다. 이러한 불편함을 개선하기 위해 서브모듈을 도입하기로 결정하였다. 서브모듈을 통해 필요한 환경 정보를 유연하게 관리하고, repository를 통한 형상 관리 까지 진행할 수 있게 되었다. 또한 서브모듈을 활용하게 되면 `JWT의 Secret Key`와 같은 private한 정보들을 private repository를 통해 안전하게 관리할 수 있다. \n\n### private repository 생성\n\n![](./config-repository.png)\n\n먼저 환경 별 설정 정보와 private한 설정 정보를 모아두기 위한 `private repository`를 생성한다. repository의 이름을 `config`라고 설정한 것은 추후 언급할 예정이다. 필요한 파일을 해당 repository에 작성 후 `commit & push`를 진행한다.\n\n### main repository에 서브모듈 추가\n\n```bash\n$ git add submodule https://github.com/dallog/{private repository}.git\nor\n$ git add submodule https://github.com/dallog/{private repository}.git {연결한 디렉토리} \n```\n\n`{private repository}`에 앞서 생성한 private repository의 이름을 명시한다. 만약 뒤에 추가로 디렉토리의 위치를 인자로 전달하면 해당 디렉토리와 서브모듈이 연결된다.\n\n위 명령어를 작성하면 `.gitsubmodules` 파일이 생성된다.\n\n```\n[submodule \"backend/src/main/resources/config\"]\n        path = backend/src/main/resources/config\n        url = https://github.com/dallog/config.git\n```\n\n우리 달록은 `backend/src/main/resources/config`로 위치를 지정하였다. 이제 새롭게 추가된 서브모듈 설정을 `commit & push` 한다. 새로운 기능이 추가되었기 때문에 PR을 시도한다.\n\nPR에 commit 내역을 살펴보면 아래와 같은 commit 내역을 확인할 수 있다.\n\n![](./commit-1.png)\n![](./commit-2.png)\n\n잘 연결된 것을 확인할 수 있다.\n\n## submodule 적용 시키기\n\n위 PR이 merge가 되면 main repository에 서브모듈 디렉토리가 추가될 것이다. 만약 최초로 main repository를 clone하는 경우 아래와 같이 진행한다.\n\n```bash\n$ git clone --recurse-submodules https://github.com/woowacourse-teams/2022-dallog.git\n```\n\n *  `--recurse-submodules`: clone 시점에 서브모듈을 자동으로 `init`하고 `update`한다.\n\n위 과정은 아래와 같다.\n\n```bash\n$ git clone https://github.com/woowacourse-teams/2022-dallog.git\n$ git submodule init\n$ git submodule update\n\nor\n\n$ git clone https://github.com/woowacourse-teams/2022-dallog.git\n$ git submodule update --init\n```\n\n * `git submodule update --init`: 현재 main repository에 연결되어 있는 sub repository의 정보를 가지고 update를 진행한다. main repository의 commit 당시의 snap shot을 가져오는 것이다. 즉 매번 최신의 private repository commit을 가져오는 것이 아니라 main repository의 기준으로 반영된 commit을 가져온다.\n\n## 작업 중간에 서브모듈이 추가되어 반영해야 하는 경우\n\n우리 달록은 fetch + rebase를 기반으로 프로젝트를 최신화하고 있다.\n\n```bash\n$ git pull --rebase upstream develop\n```\n\nmain repository를 pull 한다고 서브모듈의 repository까지 최신화 되지 않는다. 각 서브모듈을 일괄적으로 업데이트 해야 한다. 각각의 sub repository에서 fetch + merge 혹은 pull을 진행해도 되지만 아래와 같이 한 번에 진행할 수 있다.\n\n```bash\ngit submodule update --init\n```\n\n해당 명령어를 통해 main repository에 추가 반영된 서브모듈 repository 정보를 가져올 수 있다.\n\n## 주의사항\n\n서브모듈이 수정되면 남은 팀원에게 변경사항에 대해 전달해야 한다. 우리는 서브모듈을 하나의 main 브랜치로 관리하고 있기 때문에 동시에 수정할 경우 충돌될 가능성이 크다. 왠만하면 서브모듈에 대한 작업은 동시에 진행하지 않도록 한다.\n\n서브모듈이 변동될 경우 나머지 팀원들은 앞서 작성한 명령어를 통해 일괄적으로 서브모듈의 변경점을 반영해야 한다. 서브모듈을 변경한 사람은 자신이 어떤 이유로 수정했는지 명시한 뒤 PR 작성 시 나머지 팀원이 잘 적용할 수 있도록 가이드 라인을 명시해둔다.\n\n## [번외] 왜 config인가?\n\n> Spring Boot will automatically find and load `application.properties` and `application.yaml` files from the following locations when your application starts:\n\nSpring Boot 애플리케이션은 시작될 때 다음 위치에서 자동으로 `application.properties` 및 `application.yml` 파일을 찾아 로드한다. \n\n * From the classpath\n    * The classpath root\n    * The classpath `/config` package\n * From the current directory\n    * The current directory\n    * The `/config` subdirectory in the current directory\n    * Immediate child directories of the `/config` subdirectory\n\n> The list is ordered by precedence (with values from lower items overriding earlier ones). Documents from the loaded files are added as PropertySources to the Spring Environment.\n\n목록은 우선 순위에 따라 정렬된다(하위 항목의 값이 이전 항목보다 우선이다). 로드된 파일의 문서는 스프링 환경에 속성 소스로 추가된다. 정리하면 아래로 갈수록 높은 우선순위를 가진다.\n\n```java\npublic class ConfigFileApplicationListener implements EnvironmentPostProcessor, SmartApplicationListener, Ordered {\n    ...\n    private static final String DEFAULT_SEARCH_LOCATIONS = \"classpath:/,classpath:/config/,file:./,file:./config/*/,file:./config/\";\n    ...\n}\n```\n\nMaven, Gradle를 활용하여 Spring 프로젝트를 생성하면 기본적으로 리소스 파일은 `src/main/resources`에 저장된다. `.java`와 같이 컴파일 대상이 아닌 리소스 파일은 resources 디렉토리에 저장하고 관리한다. 즉 리소스 파일의 Classpath는 기본적으로 `src/main/resources`가 된다. 만약 해당 위치에 설정 관련 리소스가 존재하지 않는 경우 다음 우선 순위인 `src/main/resources/config`를 찾아 설정 정보를 로드한다.\n\n> Classpath는 JVM이 실행할 때 class 파일을 찾는데 기준이 되는 경로를 의미한다. Spring은 이러한 classpath를 통해 필요한 resources를 가져온다.\n\n물론 외부에서 jar 파일을 실행할 때 아래와 같이 `spring.config.location` 설정을 통해 기본 위치를 변경할 수 있다.\n\n```bash\n$ java -jar myproject.jar --spring.config.location=classpath:custom-config/\n```\n\n이렇게 애플리케이션 실행 시점에 의존하여 주입하는 것 보다 기본으로 제공하는 우선 순위에 따라 조정하는 것이 더욱 편하다 판단하여 `config`라는 이름을 가진 `private repository`를 생성하여 반영했다.\n\n## 정리\n\n우리 달록은 다양한 환경과 private한 정보 관리를 위해 서브모듈을 도입하게 되었다. 환경 변수를 사용할 때 보다 적절한 형상 관리와 팀원 간의 정보 공유를 통해 편리하게 설정 정보를 관리할 수 있게 되었다.\n\n장점만 있는 것은 아닌다. private repository와 서브모듈에 대한 관리 포인트가 늘어나게 되었다. 모든 팀원들이 서브모듈에 대한 배경지식이 없다면 최악의 경우 commit 기록이 꼬이게 되어 설정 정보가 온전하게 적용되지 않을 수 있다. 또한 `외부 환경(jenkins 등)`에서 `CD`를 진행할 때도 서브모듈을 적용한 repository도 함께 pull 받기 위해 추가적인 설정이 필요해진다.\n\n서브모듈은 편리한 부분도 많지만 결국 관리해야 할 포인트가 늘어난다는 단점도 있다. 팀원과의 충분한 협의와 프로젝트의 규모를 잘 고민한 뒤 적용하는 것이 바람직하다 판단한다.\n\n### References.\n\n[git submodule로 중요한 정보 관리하기](https://tecoble.techcourse.co.kr/post/2021-07-31-git-submodule/)<br>\n[7.2. Externalized Configuration](https://docs.spring.io/spring-boot/docs/2.7.0/reference/htmlsingle/#features.external-config)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. Google은 Refresh Token을 쉽게 내주지 않는다. 우리 달록은 캘린더를 손쉽게 공유할 수 이다. 현재에는 우리 서비스 내에서만 일정이 등록 가능한 상태이다. 추후 확장성을 고려하여 와 연동하기 위해 Google에서 제공하는 token 정보를 관리해야 하는 요구사항이 추가 되었다. code를 활…","fields":{"slug":"/google-refresh-token/"},"frontmatter":{"date":"August 22, 2022","title":"Google은 Refresh Token을 쉽게 내주지 않는다.","tags":["매트","BE","OAuth","OpenId","refresh token"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## Google은 Refresh Token을 쉽게 내주지 않는다.\n\n우리 [달록](https://github.com/woowacourse-teams/2022-dallog)은 캘린더를 손쉽게 공유할 수 `구독형 캘린더 공유 서비스`이다. 현재에는 우리 서비스 내에서만 일정이 등록 가능한 상태이다. 추후 확장성을 고려하여 `Google Calendar API`와 연동하기 위해 Google에서 제공하는 token 정보를 관리해야 하는 요구사항이 추가 되었다.\n\n## code를 활용한 AccessToken 및 IdToken 발급\n\nGoogle은 OAuth 2.0 요청 때 적절한 `scope(e.g. openid)`를 추가하면 `OpenID Connect`를 통해 Google 리소스에 접근 가능한 `Access Token`, AccessToken을 재발급 받기 위한 `Refresh Token`, 회원의 정보가 담긴 `IdToken`을 발급해준다. \n\n`Access Token`의 경우 짧은 만료 시간을 가지고 있기 때문에 google `Access Token` 재발급을 위한 `Refresh Token`을 저장하고 관리해야 한다. `Refresh Token`은 `Access Token`보다 긴 만료 시간을 가지고 있기 때문에 보안에 유의해야 한다. 그렇기 때문에 프론트 측에서 관리하는 것 보다 달록 DB에 저장한 뒤 관리하기로 결정 하였다. 참고로 Google은 보통 아래와 같은 이유가 발생할 때 `Refresh Token`을 만료시킨다고 한다.\n\n[Refresh Token 만료](https://developers.google.com/identity/protocols/oauth2#expiration)\n\n * 사용자가 앱의 액세스 권한을 취소한 경우\n * Refresh Token이 6개월 동안 사용되지 않은 경우\n * 사용자가 비밀번호를 변경했으며 Gmail scope가 포함된 경우\n * 사용자가 계정에 부여된 Refresh Token 한도를 초과한 경우\n * 세션 제어 정책이 적용되는 Google Cloud Platform 조직에 사용자가 속해있는 경우\n\n정리하면 `Refresh Token`은 만료 기간이 비교적 길기 때문에 서버 측에서 안전하게 보관하며 필요할 때 리소스 접근을 위한 `Access Token`을 발급 받는 형태를 구상하게 되었다.\n\n우리 [달록](https://github.com/woowacourse-teams/2022-dallog)은 아래와 같은 형태로 인증이 이루어진다. \n\n![](./oauth-flow.png)\n\n> _달록팀 후디 고마워요!_\n\n프론트 측에서 `OAuth 인증`을 위해서는 달록 서버에서 제공하는 `OAuth 인증을 위한 페이지 uri`을 활용해야 한다. 달록 서버는 해당 uri를 생성하여 전달한다. 로직은 아래 코드로 구현되어 있다.\n\n```java\n@Component\npublic class GoogleOAuthUri implements OAuthUri {\n\n    private final GoogleProperties properties;\n\n    public GoogleOAuthUri(final GoogleProperties properties) {\n        this.properties = properties;\n    }\n\n    @Override\n    public String generate() {\n        return properties.getOAuthEndPoint() + \"?\"\n                + \"client_id=\" + properties.getClientId() + \"&\"\n                + \"redirect_uri=\" + properties.getRedirectUri() + \"&\"\n                + \"response_type=code&\"\n                + \"scope=\" + String.join(\" \", properties.getScopes());\n    }\n}\n```\n\n이제 브라우저에서 해당 uri에 접속하면 아래와 같은 페이지를 확인할 수 있다.\n\n![](./google-oauth-uri.png)\n\n계정을 선택하면 `redirect uri`와 함께 `code` 값이 전달되고, google의 token을 발급 받기 위해 백엔드 서버로 `code` 정보를 전달하게 된다. 아래는 실제 code 정보를 기반으로 google token을 생성한 뒤 `id token`에 명시된 정보를 기반으로 회원을 생성 or 조회한 뒤 `달록 리소스에 접근하기 위한 access token`을 발급해주는 API이다.\n\n```java\n@RequestMapping(\"/api/auth\")\n@RestController\npublic class AuthController {\n\n    private final AuthService authService;\n\n    public AuthController(final AuthService authService) {\n        this.authService = authService;\n    }\n    ...\n    @PostMapping(\"/{oauthProvider}/token\")\n    public ResponseEntity<TokenResponse> generateToken(@PathVariable final String oauthProvider,\n                                                       @RequestBody final TokenRequest tokenRequest) {\n        TokenResponse tokenResponse = authService.generateToken(tokenRequest.getCode());\n        return ResponseEntity.ok(tokenResponse);\n    }\n    ...\n}\n```\n\n * `authService.generateToken(tokenRequest.getCode())`: code 정보를 기반으로 google 토큰 정보를 조회한다. 메서드 내부에서 [code을 액세스 토큰 및 ID 토큰으로 교환](https://developers.google.com/identity/protocols/oauth2/openid-connect#exchangecode)에서 제공된 형식에 맞춰 google에게 code 정보를 전달하고 토큰 정보를 교환한다. \n\n실제 Google에서 토큰 정보를 교환 받는 클라이언트를 담당하는 `GoogleOAuthClient`이다. 핵심은 인가 코드를 기반으로 `GoogleTokenResponse`를 발급 받는 다는 것이다.\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final GoogleProperties properties;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(final GoogleProperties properties, final RestTemplateBuilder restTemplateBuilder,\n                             final ObjectMapper objectMapper) {\n        this.properties = properties;\n        this.restTemplate = restTemplateBuilder.build();\n        this.objectMapper = objectMapper;\n    }\n\n    @Override\n    public OAuthMember getOAuthMember(final String code) {\n        // code을 액세스 토큰 및 ID 토큰으로 교환\n        GoogleTokenResponse googleTokenResponse = requestGoogleToken(code);\n        String payload = getPayload(googleTokenResponse.getIdToken());\n        UserInfo userInfo = parseUserInfo(payload);\n\n        String refreshToken = googleTokenResponse.getRefreshToken();\n        return new OAuthMember(userInfo.getEmail(), userInfo.getName(), userInfo.getPicture(), refreshToken);\n    }\n\n    private GoogleTokenResponse requestGoogleToken(final String code) {\n        HttpHeaders headers = new HttpHeaders();\n        headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n        MultiValueMap<String, String> params = generateTokenParams(code);\n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(params, headers);\n        return fetchGoogleToken(request).getBody();\n    }\n\n    private MultiValueMap<String, String> generateTokenParams(final String code) {\n        MultiValueMap<String, String> params = new LinkedMultiValueMap<>();\n        params.add(\"client_id\", properties.getClientId());\n        params.add(\"client_secret\", properties.getClientSecret());\n        params.add(\"code\", code);\n        params.add(\"grant_type\", \"authorization_code\");\n        params.add(\"redirect_uri\", properties.getRedirectUri());\n        return params;\n    }\n\n    private ResponseEntity<GoogleTokenResponse> fetchGoogleToken(\n            final HttpEntity<MultiValueMap<String, String>> request) {\n        try {\n            return restTemplate.postForEntity(properties.getTokenUri(), request, GoogleTokenResponse.class);\n        } catch (RestClientException e) {\n            throw new OAuthException(e);\n        }\n    }\n\n    private String getPayload(final String jwt) {\n        return jwt.split(JWT_DELIMITER)[1];\n    }\n\n    private UserInfo parseUserInfo(final String payload) {\n        String decodedPayload = decodeJwtPayload(payload);\n        try {\n            return objectMapper.readValue(decodedPayload, UserInfo.class);\n        } catch (JsonProcessingException e) {\n            throw new OAuthException(\"id 토큰을 읽을 수 없습니다.\");\n        }\n    }\n\n    private String decodeJwtPayload(final String payload) {\n        return new String(Base64.getUrlDecoder().decode(payload), StandardCharsets.UTF_8);\n    }\n    ...\n}\n```\n\n이제 Google에게 제공 받은 `Refresh Token`을 저장해보자.\n\n## Refresh Token에 채워진 null\n\n이게 무슨 일인가, 분명 요청 형식에 맞춰 헤더를 채워 디버깅을 해보면 계속해서 `null` 값으로 전달되고 있는 것이다. 즉, Google 측에서 Refresh Token을 보내주지 않고 있다는 것을 의미한다.\n\n![](./google-refresh-token-null.png)\n\n다시 한번 [액세스 토큰 새로고침 (오프라인 액세스)](https://developers.google.com/identity/protocols/oauth2/web-server#offline)를 살펴보았다.\n\n![](./google-refresh-token-docs.png)\n\n정리하면 [Google OAuth 2.0 서버로 리디렉션](https://developers.google.com/identity/protocols/oauth2/web-server#redirecting)할 때 query parameter에 `access_type`을 `offline`으로 설정해야 한다는 것이다. 다시 되돌아 가서 Google 인증 요청을 위한 uri를 생성하는 메서드를 아래와 같이 수정하였다.\n\n```java\n@Component\npublic class GoogleOAuthUri implements OAuthUri {\n\n    private final GoogleProperties properties;\n\n    public GoogleOAuthUri(final GoogleProperties properties) {\n        this.properties = properties;\n    }\n\n    @Override\n    public String generate() {\n        return properties.getOAuthEndPoint() + \"?\"\n                + \"client_id=\" + properties.getClientId() + \"&\"\n                + \"redirect_uri=\" + properties.getRedirectUri() + \"&\"\n                + \"response_type=code&\"\n                + \"scope=\" + String.join(\" \", properties.getScopes()) + \"&\"\n                + \"access_type=offline\"; // 추가된 부분\n    }\n}\n```\n\n이제 다시 요청을 진행해보자! 분명 `refresh token`이 정상적으로 교환될 것이다.\n\n## 또 다시 Refresh Token에 채워진 null\n\n분명 문서에 명시한 대로 설정을 진행했지만 아직도 동일하게 `null` 값이 채워져 있다.\n\n![](./google-refresh-token-null-2.png)\n\n> _해달라는 데로 다해줬는데..._\n\n## 엄격한 Google\n\nGoogle은 OAuth 2.0을 통해 인증을 받을 때 Refresh Token을 굉장히 엄격하게 다룬다. 사용자가 로그인을 진행할 때 마다 Refresh Token 정보를 주는 것이 아니라, Google에 등록된 App에 최초 로그인 할 때만 제공해준다. 즉, 재로그인을 진행해도 Refresh Token은 발급해주지 않는다. \n\nGoogle의 의도대로 동작하려면 내가 우리 서비스에 최초로 로그인을 진행하는 시점에만 Refresh Token을 발급받고 서버 내부에 저장한 뒤 필요할 때 꺼내 사용해야 한다.\n\n하지만 우리 서버는 모종의 이유로 최초에 받아온 Refresh Token을 저장하지 못할 수 있다. 이때 [Google OAuth 2.0 서버로 리디렉션](https://developers.google.com/identity/protocols/oauth2/web-server#redirecting)할 때 `prompt`를 `consent`로 설정하게 되면 매 로그인 마다 사용자에게 동의를 요청하기 때문에 강제로 `Refresh Token`을 받도록 지정할 수 있다.\n\n이제 진짜 마지막이다. 아래와 같이 수정한 뒤 다시 디버깅을 진행하였다.\n\n```java\n@Component\npublic class GoogleOAuthUri implements OAuthUri {\n\n    private final GoogleProperties properties;\n\n    public GoogleOAuthUri(final GoogleProperties properties) {\n        this.properties = properties;\n    }\n\n    @Override\n    public String generate() {\n        return properties.getOAuthEndPoint() + \"?\"\n                + \"client_id=\" + properties.getClientId() + \"&\"\n                + \"redirect_uri=\" + properties.getRedirectUri() + \"&\"\n                + \"response_type=code&\"\n                + \"scope=\" + String.join(\" \", properties.getScopes()) + \"&\"\n                + \"access_type=offline\"\n                + \"prompt=consent\"; // 추가된 부분\n    }\n}\n```\n\n![](./google-refresh-token-success.png)\n\n정상적으로 발급 되는 것을 확인할 수 있다!\n\n## 문제점\n\n하지만 여기서 문제가 하나 있다. 단순히 `prompt`를 `consent`로 설정할 경우 우리 서비스에 가입된 사용자는 Google OAuth 2.0 인증을 진행할 때 매번 재로그인을 진행해야 한다. 이것은 사용자에게 매우 `불쾌한 경험`으로 다가올 수 있다. 즉 우리는 매번 `재로그인`을 통해 Refresh Token을 발급 받는 것이 아닌, 최초 로그인 시 `Refresh Token`을 발급 받은 뒤 적절한 저장소에 저장하고 관리해야 한다.\n\n그렇다면 실제 운영 환경이 아닌 테스트 환경에서는 어떻게 해야 할까? 운영 환경과 동일한 `Google Cloud Project`를 사용할 경우 최초 로그인을 진행할 때 `내 권한 정보가 등록`된다. 즉 Refresh Token을 재발급 받을 수 없다는 것을 의미한다. \n\n우리 달록은 운영 환경과 테스트 환경에서 서로 다른 `Google Cloud Project`를 생성하여 관리하는 방향에 대해 고민하고 있다. 이미 Spring Profile 기능을 통해 각 실행 환경에 대한 설정을 분리해두었기 때문에 쉽게 적용이 가능할 것이라 기대한다. 정리하면 아래와 같다.\n\n * `운영 환경`: Refresh Token 발급을 위해 `accept_type`을 `offline`으로 설정한다. 단 최초 로그인에만 Refresh Token을 발급 받기 위해 `prompt`는 명시하지 않는다.\n * `개발 환경`: 개발 환경에서는 매번 DataBase가 초기화 되기 때문에 Refresh Token을 유지하여 관리할 수 없다. 테스트를 위한 추가적인 `Google Cloud Project`를 생성한 뒤, `accept_type`을 `offline`으로, `prompt`는 `consent`로 설정하여 매번 새롭게 Refresh Token을 받도록 세팅한다.\n\n## 정리\n\n영어를 번역기로 해석한 수준의 문장으로 인해 많은 시간을 삽질하게 되었다. 덕분에 Google에서 의도하는 Refresh Token에 대한 사용 방식과 어디에서 저장하고 관리해야 하는지에 대해 좀 더 깊은 고민을 할 수 있게 되었다. 만약 나와 같은 상황에 직면한 사람이 있다면 이 글이 도움이 되길 바란다!\n\n## References.\n\n[dallog repository](https://github.com/woowacourse-teams/2022-dallog)<br>\n[https://github.com/devHudi](https://github.com/devHudi)<br>\n[passport.js에서 구글 OAuth 진행 시 Refresh Token을 못 받아오는 문제 해결](https://m.blog.naver.com/dldbdgml99/222013891067)<br>\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 코드 커버리지란? 코드 커버리지란, 테스트 코드가 프로덕션 코드를 얼마나 실행했는지를 백분율로 나타내는 지표입니다. 즉, 테스트 코드가 실제로 프로덕션 코드를 얼마나 몇 퍼센트 검증하고 있는지를 나타냅니다. 코드 커버리지를 통해 현재 작성된 테스트 코드의 수가 충분한것인지 논의할 수 있습니다. 더 자세한 …","fields":{"slug":"/dallog-jacoco/"},"frontmatter":{"date":"August 16, 2022","title":"달록의 Jacoco 적용기 (feat. Gradle)","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 코드 커버리지란?\n\n코드 커버리지란, 테스트 코드가 프로덕션 코드를 얼마나 실행했는지를 백분율로 나타내는 지표입니다. 즉, 테스트 코드가 실제로 프로덕션 코드를 얼마나 몇 퍼센트 검증하고 있는지를 나타냅니다. 코드 커버리지를 통해 현재 작성된 테스트 코드의 수가 충분한것인지 논의할 수 있습니다.\n\n더 자세한 내용은 제가 작성한 [코드 커버리지(Code Coverage)란?](https://hudi.blog/code-coverage/) 포스팅을 참고해주세요! 😊\n\n## Jacoco란?\n\nJacoco는 자바 진영에서 프로젝트의 코드 커버리지를 분석하고, 보고서를 만들어주는 역할을 하는 코드 커버리지 프레임워크입니다. 달록팀은 이번에 Jacoco를 도입하여 코드 커버리지를 분석하고, 코드 커버리지가 일정 수준 이하라면 빌드가 실패하도록 설정하였습니다. 이 과정을 여러분께 공유드립니다.\n\n## 프로젝트에 Jacoco 적용하기\n\n```groovy\nplugins {\n    // ...\n    id 'jacoco'\n}\n\n// ...\n\njacoco {\n    toolVersion = \"0.8.7\"\n}\n```\n\n위와 같이 `build.gradle` 파일에서 Jacoco를 플러그인으로 가져오고, 버전을 설정해줍니다.\n\n## 커버리지 보고서 생성하기\n\n### 바이너리 보고서 생성\n\nJacoco 플러그인을 불러왔다면, 프로젝트의 테스트 코드를 한번 실행해봅시다.\n\n![](./test.exec.png)\n\n그럼 위와 같이 **build → jacoco** 에서 `test.exec` 파일이 생성된 것을 확인할 수 있습니다. 해당 파일은 Jacoco가 테스트 코드를 실행하고, 코드 커버리지를 분석하여 만들어준 보고서 파일입니다.\n\n이 파일은 바이너리 파일이라서 일반적인 방법으로 우리가 읽을 수 없습니다.\n\n### XML, CSV, HTML 파일로 보고서 생성\n\n하지만 Jacoco는 위에서 생성한 바이너리 커버리지 (`test.exec`) 파일을 사람이 읽을 수 있는 XML, CSV, HTML 파일로도 생성하는 기능을 제공합니다.\n\nJacoco Gradle 플러그인은 `jacocoTestReport` 라는 태스크가 있습니다. 이 태스크는 바이너리 보고서를 사람이 읽기 좋은 형태로 출력해주는 역할을 합니다. 아래와 같이 XML 파일을 생성하도록 태스크를 작성합니다.\n\n```groovy\njacocoTestReport {\n    reports {\n        xml.enabled true\n        csv.enabled true\n        html.enabled true\n\n        xml.destination file(\"${buildDir}/jacoco/index.xml\")\n        csv.destination file(\"${buildDir}/jacoco/index.csv\")\n        html.destination file(\"${buildDir}/jacoco/index.html\")\n    }\n}\n```\n\n> 위에서 `${buildDir}` 은 빌드 디렉토리 경로를 의미합니다.\n\n테스트가 이미 실행되어 `test.exec` 파일이 생성되었다고 가정하고, 아래 명령을 통해 `jacocoTestReport` 태스크를 실행하고 XML, CSV, HTML 파일을 얻어봅니다.\n\n```bash\n$ ./gradlew jacocoTestReport\n```\n\n![](report-files.png)\n\nbuild → jacoco 에서 `index.xml` `index.csv` 파일, 그리고 `index.html` 디렉토리가 생성된 것을 확인할 수 있습니다. `html` 파일은 `index.html` 디렉토리 내부 존재합니다.\n\nXML과 CSV파일은 소나큐브등과 연동할 때 주로 사용되며, HTML 파일은 사람이 직접 커버리지를 확인할 때 사용됩니다. HTML 파일을 열어보면 아래 사진과 같이 보고서를 눈으로 확인할 수 있습니다.\n\n![](./report.png)\n\n> jacocoTestReport에 대한 자세한 내용은 [https://docs.gradle.org/current/dsl/org.gradle.testing.jacoco.tasks.JacocoReport.html](https://docs.gradle.org/current/dsl/org.gradle.testing.jacoco.tasks.JacocoReport.html) 를 참고해주세요.\n\n### 테스트가 끝난뒤 바로 jacocoTestReport 실행하기\n\ntest 태스크가 끝난 다음에 jacocoTestReport 를 실행하는 2번의 과정은 조금 번거롭습니다. 아래와 같이 설정하면, 테스트가 끝나면 곧바로 jacocoTestReport가 실행되게 만들수 있습니다. 아래와 같이 test 태스크에 추가합니다.\n\n```groovy\ntest {\n    // ...\n    finalizedBy 'jacocoTestReport'\n}\n```\n\nGradle에서 `finalizedBy` 를 사용하면, 해당 태스크가 끝나고 성공 여부와 관계없이 명시한 태스크를 이어 실행하도록 설정할 수 있습니다.\n\n## 빌드가 성공하는 커버리지 기준 설정\n\nJacoco Gradle 플러그인에서는 `jacocoTestCoverageVerification` 이라는 태스크가 존재합니다. 이 태스크에서 규칙을 설정하여 프로젝트의 코드 커버리지가 규칙을 통과하지 않으면 빌드가 실패하도록 만들 수 있습니다. 아래는 달록팀의 설정입니다.\n\n```groovy\njacocoTestCoverageVerification {\n    violationRules {\n        rule {\n            enabled = true\n            element = 'CLASS'\n\n            limit {\n                counter = 'LINE'\n                value = 'COVEREDRATIO'\n                minimum = 0.75\n            }\n        }\n\n        // ...\n\n        rule {\n            // 규칙을 여러개 추가할 수 있습니다.\n        }\n    }\n}\n```\n\n`rule` 블럭 내부에서 아래와 같은 속성으로 규칙을 정의할 수 있습니다.\n\n### enabled\n\n규칙의 활성화 여부를 나타냅니다. 기본 값은 `true` 입니다.\n\n### element\n\n커버리지를 체크할 단위를 설정합니다. 아래와 같은 옵션이 있습니다.\n\n- `BUNDLE` : 패키지 번들 (전체 프로젝트)\n- `CLASS` : 클래스\n- `GROUP` : 논리적 번들 그룹\n- `METHOD` : 메소드\n- `PACKAGE` : 패키지\n- `SOURCEFILE` : 소스파일\n\n기본값은 `BUNDLE` 입니다.\n\n### counter\n\n코드 커버리지를 측정할 때 사용되는 지표입니다. 아래와 같은 옵션이 있습니다.\n\n- `LINE` : 빈 줄을 제외한 실제 코드의 라인 수\n- `BRANCH` : 조건문등의 분기 수\n- `CLASS` : 클래스 수\n- `METHOD` : 메소드 수\n- `INSTRUCTION` : 자바 바이트코드 명령 수\n- `COMPLEXITY` : 복잡도\n\n기본값은 `INSTRUCTION` 입니다.\n\n## 보고서 생성 후 커버리지 만족 검사\n\n조금 전, `finalizedBy` 속성으로 `test` 태스크가 실행되고 `jacocoTestReport` 가 실행되도록 설정했습니다.\n\n이어서 `jacocoTestReport` 태스크가 실행된 이후 `jacocoTestCoverageVerification` 가 바로 실행되어 테스트가 실행되고, 커버리지가 기준 미달이면 빌드가 실패되도록 흐름을 만들어봅시다.\n\n```groovy\njacocoTestReport {\n    // ...\n\n    finalizedBy 'jacocoTestCoverageVerification'\n}\n```\n\n간단히 위처럼 `jacocoTestReport` 에 추가해주면 끝입니다.\n\n## 코드 커버리지 분석 대상 제외\n\n예외 클래스, DTO 클래스 등 굳이 테스트를 하지 않아도 되는 클래스들이 있습니다. 이런 클래스까지 포함하여 코드 커버리지를 계산하면 코드 커버리지가 낮게 나올 것 입니다. 이런 클래스를 분석 대상에서 제외할 수 있습니다.\n\n### jacocoTestReport\n\n`jacocoTestReport` 태스크에서는 보고서에 표시되는 클래스 중 일부를 제외할 수 있습니다.\n\n```groovy\njacocoTestReport {\n    // ...\n    afterEvaluate {\n        classDirectories.setFrom(\n                files(classDirectories.files.collect {\n                    fileTree(dir: it, excludes: [\n                            '**/*Application*',\n                            '**/*Exception*',\n                            '**/dto/**',\n                            // ...\n                    ])\n                })\n        )\n    }\n    // ...\n}\n```\n\n제외 대상 파일의 경로를 [Ant 스타일](https://ant.apache.org/manual/dirtasks.html)로 작성합니다.\n\n### jacocoTestCoverageVerification\n\n`jacocoTestCoverageVerification` 에서는 코드 커버리지를 만족하는지 확인할 대상 중 일부를 제외할 수 있습니다. `jacocoTestReport` 에서 작성한 것과 다르게 파일의 경로가 아닌 패키지 + 클래스명을 적어주어야 합니다. 와일드 카드로 `*` (여러 글자) 와 `?` (한 글자) 를 사용할 수 있습니다.\n\n```groovy\njacocoTestCoverageVerification {\n    violationRules {\n        rule {\n            // ...\n\n            excludes = [\n                    '*.*Application',\n                    '*.*Exception',\n                    '*.dto.*',\n                    // ...\n            ]\n        }\n    }\n}\n```\n\n## 참고\n\n- [https://techblog.woowahan.com/2661](https://techblog.woowahan.com/2661/)\n- [https://bottom-to-top.tistory.com/36](https://bottom-to-top.tistory.com/36)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 문제 상황 현재 달록팀 젠킨스 설정상 프론트엔드와 백엔드 배포를 한 인스턴스에서 진행합니다.  브랜치에 소스코드가 병합되면, 이것이 트리거가 되어 프론트엔드와 백엔드 배포 파이프라인 스크립트가 실행됩니다. 아직 별도로 분기처리는 하지 않아 프론트엔드와 백엔드 배포가 동시에 시작됩니다. 하지만, 달록팀이 젠…","fields":{"slug":"/jenkins-build-optimization/"},"frontmatter":{"date":"August 09, 2022","title":"젠킨스 빌드 최적화를 향한 여정","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 문제 상황\n\n현재 달록팀 젠킨스 설정상 프론트엔드와 백엔드 배포를 한 인스턴스에서 진행합니다. `develop` 브랜치에 소스코드가 병합되면, 이것이 트리거가 되어 프론트엔드와 백엔드 배포 파이프라인 스크립트가 실행됩니다. 아직 별도로 분기처리는 하지 않아 프론트엔드와 백엔드 배포가 동시에 시작됩니다.\n\n하지만, 달록팀이 젠킨스를 실행하기 위해 사용하고 있는 EC2 인스턴스는 **t4g.micro** 입니다. 이 인스턴스는 기본 제공되는 **물리 메모리(RAM)가 1GB** 가량입니다. 사실 프론트엔드와 백엔드 배포를 동시에 진행하기에는 턱없이 모자란 사양입니다.\n\n![스프링부트 애플리케이션 빌드에 9분 가량 소요된다](./build-9m.png)\n\n이런 상황에서 프론트엔드와 백엔드 빌드가 각각 10분 가까이 걸리는 경우도 존재하고, 빌드 도중에 아예 서버에 간헐적으로 접속할 수 없게되는 경우도 심심치 않게 목격했습니다.\n\n![서버: 살... 려줘...](./dead-server.png)\n\n굳이, 서버가 죽지 않더라도 빌드가 트리거되고 끝나기까지는 젠킨스 웹페이지 이동도 어려울 정도로 성능 저하가 극심했습니다.\n\n## 문제 원인 추정\n\n![빌드 중의 젠킨스 인스턴스 CPU 이용률](./cpu-utilization.png)\n\n일단 먼저 EC2 모니터링 탭에서 CPU 사용량을 살펴보았습니다. 예상은 했지만, CPU 사용량이 문제의 원인은 아니었습니다. 그렇다면 메모리는 어떨까요?\n\n현재 달록 젠킨스 서버는 물리 메모리 1gb와 스왑 영역으로 확보해둔 가상 메모리 2gb, 더해서 총 **3gb의 메모리로 운용**되고 있습니다. 이 환경에서 백엔드와 프론트엔드 빌드가 동시에 시작되면 여유 메모리는 어느정도 수준까지 낮아질까요? `free` 명령을 통해 알아 보았습니다.\n\n![백엔드, 프론트엔드 동시 빌드 중 여유 메모리](./lack-of-memory.png)\n\n물리 메모리의 여유 공간이 **고작 30~40mb** 정도의 메모리만 남았습니다. 가상 메모리도 약 **400mb가량** 남아있는 상황입니다. 상황에 따라 다르겠지만, 메모리가 상당히 부족한 것을 보아 역시 메모리의 문제였던 것 같습니다.\n\n메모리가 부족한 것이 정확히 어떤 문제를 일으키게 된건지 확실하지는 않지만 **CPU 스레싱**이 가장 큰 원인이 아닐까 추정됩니다.\n\n### CPU 스레싱\n\n리눅스에서 프로세스가 동작할 때 프로세스 전부를 물리 메모리에 띄우지 않고 **가상 메모리**라는 곳에 보관합니다. 젠킨스 서버도 스왑 영역을 2gb로 지정해서 이 영역만큼을 가상 메모리로 활용하고 있습니다. 스왑 영역은 하드디스크와 같은 저장장치에 위치합니다.\n\nCPU가 작업 처리를 위해 특정 데이터를 메모리에 요청을 했는데, CPU가 원하는 데이터가 물리 메모리가 아니라 가상 메모리에 저장되어 있을 수 있습니다. 이런 상황을 **페이지 부재(page fault)**라고 합니다. 페이지 부재가 발생하면, 스왑 영역에서 물리 메모리로 데이터를 옮겨야 하는 상황, 즉 페이지를 교체해야하는 상황이 발생합니다.\n\n알다시피 하드디스크와 같은 장비는 I/O 속도가 굉장히 느립니다. 그런데 만약 물리 메모리가 매우 한정되어 있어 페이지 부재가 자주 발생하게 되고, 이런 상황이 반복되어 **CPU가 작업을 처리하는 속도보다 페이지를 교체하는 시간이 훨씬 많이 들것**입니다. 이런 현상을 **CPU 스레싱(thrashing)**이라고 합니다.\n\n달록의 젠킨스 서버도 스레싱 현상이 발생해서 빌드 처리 속도가 극도로 낮아진 것이 아닐까 추정됩니다.\n\n## 문제 해결\n\n결국 서버 스펙상 프론트엔드 빌드와 백엔드 빌드가 동시에 실행되어 메모리가 부족해지는 것이 문제라면, 둘을 동시에 실행되지 않게 만들면 손쉽게 문제를 해결할 수 있을것 입니다.\n\n### 젠킨스 노드\n\n젠킨스의 **노드(node)**는 파이프라인 등을 실행할 수 있는 머신을 의미합니다. 노드는 다수의 **실행기(executor)**를 갖습니다. 노드는 실행기의 수 만큼 동시에 빌드를 실행할 수 있습니다.\n\n확인해보니 젠킨스에는 기본 노드 1개가 존재했고, 해당 노드에는 실행기가 2개로 설정되어 있었습니다. 이를 1로 변경하여 더이상 병렬적으로 백엔드, 프론트엔드 빌드가 실행되지 않도록 해줍시다.\n\n![](./number-of-executors.png)\n\n**Jenkins 관리 > 노드 관리 > 노드 이름 > 설정 > Number of executors** 를 1로 설정해줍니다. 이제 병렬적으로 빌드가 트리거되어도 하나의 빌드만 진행되고, 나머지 빌드는 빌드 대기 목록에서 자신의 차례를 기다리게 될 것 입니다.\n\n![](./waiting.png)\n\n## 최적화 결과\n\n![](./super-strong-jenkins.png)\n\n![](./optimized.png)\n\n빌드를 동시에 실행하지 않도록 설정하였을 뿐인데, 빌드 성능이 **10배 가까이 상승**했습니다. 아주 간단한 설정만 해주었는데 드라마틱하게 성능이 개선되다니 놀랍네요!\n\n코드가 병합되고 나서 10분은 지나야 빌드가 완료되었다는 알림을 받았었는데, 이제 코드가 병합되고 몇 분내로 프론트엔드와 백엔드 빌드 완료 알림을 받게 되었습니다. 한번에 하나의 빌드만 실행되니 빌드중에도 젠킨스 웹페이지를 접속하는데도 큰 문제가 없게 되었습니다.\n\n많은 삽질을 했지만 그만큼 재밌던 경험이었습니다. 이만 글 줄이겠습니다!\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 테스트는 독립적이어야 하며 공유 자원을 사용하면 안된다. 격리되지 못한 테스트는 테스트에서 Entity 객체를 상수로 두면 안 되는 이유 같이 많은 문제를 야기할 수 있다. 테스트를 격리하는 방법에는 여러가지가 있다. 우리 달록에서는 크게 두 가지 방식을 적용해봤고, 각 방식에 대한 장단점을 확인해보려…","fields":{"slug":"/test-isolation/"},"frontmatter":{"date":"August 09, 2022","title":"테스트 격리","tags":["달록","매트","테스트 격리","BE"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n테스트는 독립적이어야 하며 공유 자원을 사용하면 안된다. 격리되지 못한 테스트는 [테스트에서 Entity 객체를 상수로 두면 안 되는 이유](https://dallog.github.io/test-fixture-constant/) 같이 많은 문제를 야기할 수 있다.\n\n테스트를 격리하는 방법에는 여러가지가 있다. 우리 [달록](https://github.com/woowacourse-teams/2022-dallog)에서는 크게 두 가지 방식을 적용해봤고, 각 방식에 대한 장단점을 확인해보려 한다.\n\n## TRUNCATE\n\n테스트 격리에 대한 방식에 대해 알아보기 전에 간단한 SQL 쿼리의 특성에 대해 알아보려 한다.\n\n`TRUNCATE`는  테이블의 모든 행을 삭제하는 `데이터 정의 언어 (Data Definition Language)`이다. `TRUNCATE`는 `DELETE`와 거의 유사하지만 부분 삭제가 불가능하다. 또한 `TRUNCATE`는 테이블 구조를 유지하기 때문에 `모든 행을 삭제하는데 가장 빠르고 효율적인 방법`이 될 수 있다.\n\n우리는 이러한 특성을 활용하여 테스트 간의 격리를 위해 활용 하였다. 각각의 테스트 이전에 직접 작성한 SQL 스크립트를 통해 매번 테이블을 초기화 하는 방식으로 말이다.\n\n아래는 테스트 격리를 위한 `truncate.sql`이다.\n\n```sql\nSET foreign_key_checks = 0;\nTRUNCATE TABLE subscriptions;\nTRUNCATE TABLE categories;\nTRUNCATE TABLE members;\nSET foreign_key_checks = 1;\n```\n\n- `foreign_key_checks`: `MySQL`에서 외래키 제약 조건을 키고 끄기 위한 설정이다. `0`으로 설정하면 제약 조건을 해제하고, `1`로 설정하면 제약 조건을 설정한다.\n- `TRUNCATE 테이블명`: 앞서 언급한 것 처럼 테이블 구조를 유지하고 테이블에 모든 행을 삭제한다.\n\n### 방법 1: @Sql(”truncate.sql”)\n\n```java\n@Target({ElementType.TYPE, ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@Repeatable(SqlGroup.class)\npublic @interface Sql {\n\n\t@AliasFor(\"scripts\")\n\tString[] value() default {};\n\n\t@AliasFor(\"value\")\n\tString[] scripts() default {};\n\t...\n}\n```\n\n`테스트 클래스`나 `테스트 메서드`에 작성할 수 있는 애노테이션이다. 해당 애노테이션에 SQL 스크립트를 명시하면 메서드 실행 시점에 해당 스크립트가 수행된다.\n\n아래와 같이 손쉽게 적용할 수 있다.\n\n```java\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Sql(\"truncate.sql\")\n@Import(TestConfig.class)\nclass AcceptanceTest {\n\n    @LocalServerPort\n    private int port;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n    }\n}\n```\n\n하지만 이 방식은 `Entity`가 추가될 경우 직접 `truncate.sql` 파일에 접근하여 테이블을 추가해줘야 하는 불편함이 있다.\n\n### 방법 2: EntityManager를 통한 TRUNCATE\n\n앞서 언급한 단점을 해결하기 위한 방법이다. `EntityManager`를 통해 동적으로 현재 등록된 `Entity`를 조회한 뒤 각각의 `Entity`의 클래스 정보를 활용하여 테이블 이름을 획득한 뒤 쿼리문을 실행하도록 할 수 있다. \n\n하지만 단순히 Entity의 이름을 가져올 경우 아래와 같이 클래스 이름을 가져올 뿐이다.\n\n```java\n@DataJpaTest\nclass DatabaseCleanerTest {\n\n    @Autowired\n    private EntityManager entityManager;\n\n    @Test\n    void entities() {\n        entityManager.getMetamodel()\n                .getEntities()\n                .stream()\n                .map(EntityType::getName)\n                .forEach(System.out::println);\n    }\n}\n```\n\n```bash\nMember\nCategory\nSubscription\nSchedule\n```\n\n하지만 우리 `달록`은 `MySQL`에 `schedule 예약어`로 인해 `schedules`와 같이 복수형으로 표현하기로 통일하였다. 하지만 클래스 이름을 활용하여 다양한 조건을 가진 복수 형태를 만드는 것은 한계가 있었다. 예를 들면 `Member`는 `members`, `Category`는 `categories`와 같이 표현하고 있다.\n\n우리는 이러한 테이블명은 `Entity` 클래스에 `@Table` 애노테이션을 통해 명시하기로 약속하였고, 이것을 활용하는 방안을 고민하였다.\n\n```java\n@Table(name = \"categories\")\n@Entity\npublic class Category extends BaseEntity {\n\n    public static final int MAX_NAME_LENGTH = 20;\n\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    @Column(name = \"id\")\n    private Long id;\n\n    @Column(name = \"name\", nullable = false)\n    private String name;\n\n    @ManyToOne(fetch = FetchType.LAZY)\n    @JoinColumn(name = \"members_id\", nullable = false)\n    private Member member;\n\t\t...\n}\n```\n\n해답은 바로 Java의 `Reflection API`를 활용한 방법이다. 아래는 Java의 Reflection을 활용하여 Table 애노테이션에 명시한 name 값을 추출하여 `tableNames`로 활용한 예시이다.\n\n```java\n@Component\npublic class DatabaseCleaner {\n\n    private final EntityManager entityManager;\n    private final List<String> tableNames;\n\n    public DatabaseCleaner(final EntityManager entityManager) {\n        this.entityManager = entityManager;\n        this.tableNames = entityManager.getMetamodel()\n                .getEntities()\n                .stream()\n                .map(Type::getJavaType)\n                .map(javaType -> javaType.getAnnotation(Table.class))\n                .map(Table::name)\n                .collect(Collectors.toList());\n    }\n\n    @Transactional\n    public void execute() {\n        entityManager.flush();\n        entityManager.createNativeQuery(\"SET foreign_key_checks = 0\").executeUpdate();\n\n        for (String tableName : tableNames) {\n            entityManager.createNativeQuery(\"TRUNCATE TABLE \" + tableName).executeUpdate();\n        }\n\n        entityManager.createNativeQuery(\"SET foreign_key_checks = 1\").executeUpdate();\n    }\n}\n```\n\n- `getMetamodel()`: persistence unit의 메타 모델에 접근하기 위한 `Metamodel` 인터페이스를 반환한다.\n- `getEntities()`: metamodel에 Entity Type을 반환한다.\n- `getJavaType()`: Java Type을 반환한다. 반환 타입은 `Class<T>`이다.\n- `getAnnotation(Table.class)`: `@Table` 애노테이션을 가져온다.\n- `Table::name`: 해당 애노테이션에 명시한 이름 정보를 가져온다.\n\n우리는 이제 작성한 `Entity`의 모든 테이블명을 가지게 되었다. 이제 앞서 작성한 `TRUNCATE` 관련 쿼리를 `entityManager`가 실행할 수 있도록 작성 해준다.\n\n자 이제 우리가 만든 `DatabaseCleaner` 객체를 주입 받아 각각의 메서드 실행 이전에 호출할 수 있도록 개선하자.\n\n```java\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Import(TestConfig.class)\nclass AcceptanceTest {\n\n    @LocalServerPort\n    private int port;\n\n    @Autowired\n    private DatabaseCleaner databaseCleaner;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n        databaseCleaner.execute();\n    }\n}\n```\n\n- `@Autowired private DatabaseCleaner databaseCleaner`: 앞서 작성한 `DatabaseCleaner`을 주입 받는다.\n- `databaseCleaner.execute()`: `@BeforeEach`를 통해 각 테스트 메서드가 실행되기 이전에 DB 테이블의 행을 모두 비워준다.\n\n이제 우리는 직접 파일을 수정하지 않아도 `Entity`의 추가에 유동적으로 대응할 수 있게 되었다.\n\n## 테스트 격리 방식 비교\n\n가장 간편하게 테스트를 격리하는 방법은 각각의 분리된 테스트 마다 Spring Context를 새롭게 로드하는 것이다. `@DirtiesContext`를 활용하면 아래와 같이 적용이 가능하다.\n\n```java\n@DirtiesContext(classMode = ClassMode.BEFORE_EACH_TEST_METHOD)\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Import(TestConfig.class)\nclass AcceptanceTest {\n\n    @LocalServerPort\n    private int port;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n    }\n}\n```\n\n- `@DirtiesContext(classMode = ClassMode.BEFORE_EACH_TEST_METHOD)`: 테스트 메서드를 실행하기 전에 context를 새롭게 로드한다.\n\n하지만 이 방식은 테스트하는데 매우 오랜 시간이 걸린다. 아래는 `@DirtiesContext`를 적용한 뒤 달록에 있는 테스트를 실행한 것이다.\n\n![](dirties-context.png)\n\n이제 위에서 작성한 `DatabaseCleaner`를 활용해보자.\n\n![](database-cleaner.png)\n\n`33 sec`에서 `10 sec`의 성능 개선을 확인할 수 있었다.\n\n## 정리\n\n`@DirtiesContext`를 활용한 테스트 격리는 테스트 마다 context를 새롭게 로드하기 때문에 매우 오랜 시간이 걸린다.\n\n그렇기 때문에 `TRUNCATE`를 통해 테스트 격리를 진행하는 것이 더욱 빠른 테스트 피드백을 확인할 수 있다. `@Sql`을 통해 적용할 경우 간편하게 적용이 가능하다. 하지만 `Entity`가 추가될 경우 직접 스크립트 파일에 접근하여 수정해야 하는 단점이 있다. 또한 문자열로 작성되기 때문에 최악의 경우 오타로 인하여 테이블을 찾지 못할 수도 있다.\n\n`EntityManager`를 통해 `TRUNCATE`를 적용할 경우 현재 작성된 `Entity`를 기반으로 명시한 테이블명을 가져온다. 즉 우리가 의도한 테이블명을 그대로 활용할 수 있기 때문에 직접 수정하지 않아도 편리하게 활용할 수 있다.\n\n## References.\n\n[TRUNCATE (SQL)](https://ko.wikipedia.org/wiki/TRUNCATE_(SQL))<br>\n[Annotation Type Sql](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/test/context/jdbc/Sql.html)<br>\n[Guide to Java Reflection](https://www.baeldung.com/java-reflection)<br>\n[Separated Interface](https://www.martinfowler.com/eaaCatalog/separatedInterface.htmlhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. NGINX 설치 NGINX란? 달록 기술블로그 - 리버의 NGINX 란? 설치 SSL 인증서 발급 (with Certbot) Let’s Encrypt 90일짜리 단기 인증서를 무료로 발급해주는 곳입니다. 주기적으로 재발급 해줘야 합니다. Snapd 설치 snapcraft 공식문서 - Installing s…","fields":{"slug":"/deploy-full-course3/"},"frontmatter":{"date":"August 08, 2022","title":"달록 배포 총🔫정리 3 - SSL 인증서 발급과 NGINX 설정","tags":["deploy","DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## NGINX 설치\n\n### NGINX란?\n\n[달록 기술블로그 - 리버의 NGINX 란?](https://dallog.github.io/what_is_nginx/)\n\n### 설치\n\n```bash\n$ sudo apt update # 운영체제에서 사용 가능한 패키지들과 그 버전에 대한 정보(리스트) 업데이트 \n$ sudo apt install nginx -y # nginx 설치\n$ nginx -v # 설치한 nginx 버전 확인\n$ sudo service nginx status # nginx 상태 확인\n```\n\n## SSL 인증서 발급 (with Certbot)\n\n### Let’s Encrypt\n\n90일짜리 단기 인증서를 무료로 발급해주는 곳입니다. 주기적으로 재발급 해줘야 합니다.\n\n### Snapd 설치\n\n[snapcraft 공식문서 - Installing snap on Ubuntu](https://snapcraft.io/docs/installing-snap-on-ubuntu)\n\ncertbot은 snapd로 certbot을 설치하는 것을 권장하고 있습니다. 리눅스 통합 패키지 관리 툴인 snapd를 먼저 설치해봅시다.\n\n```bash\n$ sudo apt update\n$ sudo apt install snapd\n```\n\n### Certbot 설치\n\n[certbot 공식 문서 - certbot 설치](https://certbot.eff.org/instructions?ws=apache&os=centosrhel7)\n\n```bash\n$ sudo snap install core; \n$ sudo snap install --classic certbot # certbot 설치\n$ sudo ln -s /snap/bin/certbot /usr/bin/certbot # certbot 명령어를 사용하기 위해 심볼릭링크 걸기\n$ certbot --version # 버전 확인\n```\n\n### SSL 인증서 발급 방법\n\n[certbot 공식 문서 - 인증서 발급 방법](https://eff-certbot.readthedocs.io/en/stable/using.html#apache)\n\nCertbot을 통해 SSL 인증서를 발급받는 방법은 여러가지가 있습니다. 그 중 몇 가지를 간단하게 살펴볼게요. 달록은 Nginx 방법을 사용했습니다.\n\n- **`Nginx`** (우리가 사용한 방법)\n    - 인증 및 설치를 위해 nginx 플러그인을 사용한다. nginx에 대한 인증서 취득 및 설치가 자동적으로 이루어진다.\n- `Webroot`\n    - 기능이 동작하고 있는 로컬 웹 서버를 실행 중이라면 웹 서버를 중지하면 안 되는 경우가 있다. 그런 경우 webroot 방식을 사용할 수 있다.\n    - 웹의 디렉토리 내에 인증서의 유효성을 확인할 수 있는 임시 파일을 만들어 인증을 진행한다.\n    - 한 번에 하나의 도메인만 발급 가능하다.\n- `Standalone`\n    - 인증서 발급을 위해 기존의 웹 서버를 중지해야 한다.\n    - 인증을 위해 80 포트로 가상의 standalone 웹서버를 띄워 인증서를 발급한다.\n    - 한 번에 여러 도메인을 발급할 수 있다.\n- `Manual`\n    - 도메인의 유효성 검증을 직접 수행하는 경우 사용한다.\n    - 디렉토리에 특정 이름과 특정 내용을 가진 임시 파일을 만들어야 한다. webroot 방식과 비슷하지만 자동으로 진행되지 않는다.\n    - 도메인에 `_acme-challenge` 를 붙인 TXT DNS 레코드를 설정해주어야 한다.\n\n        ```bash\n        # 예시\n        _acme-challenge.example.com. 300 IN TXT \"gfj9Xq...Rg85nM\"\n        ```\n\n\n### 인증서 발급하기\n\n그럼 이제 Nginx 방법을 사용해서 인증서를 발급받아 보겠습니다.\n\n```bash\n$ sudo certbot certonly --nginx -d {도메인 이름}\n```\n\n- `certonly`\n    - 자동으로 nginx 설정을 수정하지 않고 인증서만 발급받는 옵셥입니다. 자동으로 어떻게 설정되는지 확실히 알지 못하는 경우엔 certonly로 받는 것이 나을 수 있습니다.\n- `—-nginx`\n    - nginx 방식을 활용하겠다는 의미입니다.\n\n### Nginx 설정\n\n이제 nginx 설정을 위해서 설정 파일을 생성합니다. `/etc/nginx/conf.d` 디렉토리로 이동하여 `default.conf` 파일을 생성합니다. 꼭 sudo로 명령어를 입력해야 합니다.\n\n```bash\n$ cd /etc/nginx/conf.d\n$ sudo vim default.conf\n```\n\n> ⚠️ 주의: `sites-available` 과 `sites-enabled` 는 더 이상 사용되지 않는 nginx 설정 방법입니다.\n>\n\n`sites-available` 과 `sites-enabled` 두 디렉토리는 sym link로 연결되어 있습니다. 기존 방식은 `sites-available`에 여러 설정파일들을 때려 박은 다음에 `sites-enabled`에 원하는 설정을 선택적으로 동기화하는 방식입니다. 이는 비효율적인 방법이므로 더 이상 사용되지 않는다고 합니다.\n\n**따라서 `sites-available` 에 기본 설정 파일이 있다면 제거해줍시다.**\n\n<br>\n\n`default.conf` 파일을 생성했다면 아래와 같이 내용을 적어줍니다.\n\n```bash\n# dallog-frontend /etc/nginx/conf.d/default.conf\n\n# 80 포트로 요청이 오는 경우 https가 적용된 url로 redirect 한다.\nserver {\n  listen 80;\n  server_name dallog.me;\n\n  return 301 https://dallog.me;\n}\n\n# 443 포트로 요청이 오는 경우\nserver {\n  listen 443 ssl http2;\n  server_name dallog.me;\n\n\t# SSL 인증을 위한 pem key 위치를 설정한다.\n  ssl_certificate /etc/letsencrypt/live/dallog.me/fullchain.pem;\n  ssl_certificate_key /etc/letsencrypt/live/dallog.me/privkey.pem;\n\n\t# front의 경우 정적 페이지를 보여주기 위해 index.html 위치를 설정한다.\n  location / {\n    root /usr/share/nginx/html;\n    index index.html;\n    try_files $uri $uri/ /index.html;\n  }\n}\n```\n\n```bash\n# dallog-ws /etc/nginx/conf.d/default.conf\n\nserver {\n  listen 80;\n  server_name api.dallog.me;\n\n  return 301 https://api.dallog.me;\n}\n\nserver {\n  listen 443 ssl http2;\n  server_name api.dallog.me;\n\n  ssl_certificate /etc/letsencrypt/live/api.dallog.me/fullchain.pem;\n  ssl_certificate_key /etc/letsencrypt/live/api.dallog.me/privkey.pem;\n\n\t# web server의 경우 리버스 프록시를 위한 proxy 설정이 들어간다.\n  location / {\n    proxy_pass http://{backend-prod server ip}; # backend-prod server ip\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n  }\n}\n```\n\n- `proxy_pass` : 프록시 주소, 백엔드 운영 서버 ip를 넣어준다.\n- `proxy_set_header Host $http_host` : HTTP Request의 Host 헤더 값, 클라이언트가 요청한 원래 호스트 주소\n- `X-Real-IP $remote_addr`: 실제 방문자의 원격 ip 주소\n- `X-Forwarded-For $proxy_add_x_forwared_for` : 클라이언트가 프록시 처리한 모든 서버의 IP 주소를 포함하는 목록\n- `X-forwarded-Proto $scheme` : HTTP의 구조로 http or https를 의미한다. HTTPS 서버 블록 내에서 사용할 경우 프록시 서버의 HTTP 응답이 HTTPS로 변환된다.\n\n### Nginx 설정 이후 재시작\n\n설정을 끝냈다면 적용을 위해 Nginx를 재시작 해주어야 합니다.\n\n```bash\n# nginx 재시작, 둘 중 하나 선택\n$ sudo service nginx reload\n$ sudo service nginx restart\n```\n\n## 인증서 발급 시 발생할 수 있는 에러\n\n인증서 발급 시 발생할 수 있는 에러들을 알아봅시다. 아래 두 가지 에러는 실제로 달록이 SSL 인증서를 발급받는 과정에서 겪은 에러입니다.\n\n### 1. too many failed authorizations recently\n\n[Let's encrypt 공식 문서 - failed validation limit](https://letsencrypt.org/docs/failed-validation-limit/)\n\n```bash\n$ sudo certbot certonly --nginx -d api.dallog.me\n\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\nRequesting a certificate for api.dallog.me\nAn unexpected error occurred:\nError creating new order :: too many failed authorizations recently: see https://letsencrypt.org/docs/failed-validation-limit/\nAsk for help or search for solutions at https://community.letsencrypt.org. See the logfile /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\n```\n\n이 오류는 계정, 호스트 네임, 시간당 5번의 유효성 검사 실패시 발생합니다. 몇 시간(3-4시간 정도) 기다리면 제한이 풀립니다. 그 때 다시 시도하면 됩니다.\n\n### 2. too many certificates\n\n[Let's encrypt 공식 문서 - duplicate certificate limit](https://letsencrypt.org/docs/duplicate-certificate-limit/)\n\n![](error.png)\n\n문제는 이 두 번째 에러입니다. 이 오류는 주당 5번이라는 인증서 발급 횟수 제한을 넘긴 경우 발생합니다. 이 경우 일주인간 인증서 발급이 제한됩니다(…🥲). 이 오류를 피하기 위해서는 인증서를 재발급 받는 대신 pem key를 복사해서 사용해야 합니다. **재발급은 신중히 받도록 합시다.**\n\n## 기타 인증서 관련 명령어\n\n### 인증서 자동 갱신\n\n```bash\n$ sudo certbot renew --dry-run\n```\n\n### 인증서 삭제\n\n```bash\n$ certbot delete --cert-name {인증서 이름}\n```\n\n<br>\n \n#### reference\n\n[후디 블로그 - Nginx와 Let's Encrypt로 HTTPS 웹 서비스 배포하기 (feat. Certbot)](https://hudi.blog/https-with-nginx-and-lets-encrypt/)\n[2. Nginx 설치 부터 spring boot 기반 앱 배포 - Cerbot 인증서 발급과 SSL 적용](https://velog.io/@jihyunhillpark/2.-spring-boot-기반-앱-배포-Cerbot-인증서-발급과-SSL-적용)\n[4. SSL 인증서(letsencrypt) 발급](https://blog.naver.com/PostView.naver?blogId=kangdydtjs&logNo=222546308110&from=search&redirect=Log&widgetTypeCall=true&directAccess=false)\n\n#### Special Thanks To `승팡`\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 실제 배포 설정 그럼 이제 실제로 어떤 과정을 통해 배포를 진행했는지 자세히 살펴보겠습니다. EC2 생성 EC2 → 인스턴스 → 인스턴스 시작 이름 및 태그 이름: ec2-dallog-XXXX 태그:  -  애플리케이션 및 OS 이미지  아키텍처:  인스턴스 유형  키 페어  네트워크 설정 VPC PROJ…","fields":{"slug":"/deploy-full-course2/"},"frontmatter":{"date":"August 07, 2022","title":"달록 배포 총🔫정리 2 - EC2와 DNS 설정하기","tags":["deploy","DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 실제 배포 설정\n\n그럼 이제 실제로 어떤 과정을 통해 배포를 진행했는지 자세히 살펴보겠습니다.\n\n## EC2 생성\n\nEC2 → 인스턴스 → 인스턴스 시작\n\n### 이름 및 태그\n\n- 이름: ec2-dallog-XXXX\n- 태그: `Role` - `student`\n\n### 애플리케이션 및 OS 이미지\n\n- `Ubuntu`\n- 아키텍처: `64비트(Arm)`\n\n### 인스턴스 유형\n\n- `t4g.micro`\n\n### 키 페어\n\n- `key-dallog`\n\n### 네트워크 설정\n\n- VPC\n  - PROJECT\n\n- 보안 그룹\n    - 기존 보안 그룹 선택: `SG-PROJECT-DEFAULT`\n\n## 탄력적 IP(Elastic IP) 설정\n\n[AWS 공식 문서](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html)\n\n### 탄력적 IP란?\n\n- 동적 클라우드 컴퓨팅을 위해 고안된 정적 IPv4 주소 (IPv6는 지원X)\n- 탄력적 IP 주소는 AWS 계정에 할당되며 릴리스할 때까지 할당된 상태로 유지된다.\n\n원래 EC2 인스턴스를 생성하면 동적 IP를 할당받습니다. 따라서 인스턴스를 중지했다가 다시 시작하면 IP가 바뀝니다. IP가 계속 바뀌면 배포 설정을 계속해서 바꿔줘야 하겠죠? 탄력적 IP는 고정 IP 주소이기 때문에 인스턴스를 중지했다 재시작해도 IP 주소가 바뀌지 않습니다.\n\n### 설정\n\n- EC2 → 네트워크 및 보안 → 탄력적 IP → 탄력적 IP 주소 할당\n- 태그: `Role` - `student` → 할당\n- 작업 → 탄력적 IP 주소 연결 → 원하는 인스턴스 선택 → 프라이빗 주소 선택 → 연결\n\n![](ec2.png)\n\n## 도메인(DNS) 설정\n\n![](dns.png)\n\n- `jenkins.dallog.me`\n    - 달록의 젠킨스 서버\n- `allog.dallog.me`\n    - 달록의 프론트 서버 (현재 ssl 인증서 발급 문제로 해당 서브 도메인 사용 중. 추후 dallog.me로 변경 예정)\n- `api.dallog.me`\n    - 달록의 백엔드 WS 운영 서버\n- `dev-api.dallog.me`\n    - 달록의 백엔드 개발 서버\n\n#### 여기서 잠깐, `TTL`이란?\n\nTime To Live. DNS 변경 사항이 적용되기까지 걸리는 시간. 단위는 초.\n\n#### 여기서 잠깐, `레코드 타입` 어떻게 다를까?\n\n- `A`\n    - 정규화된 도메인 이름/호스트명을 IPv4에 연결한다.\n    - `HOST`: 도메인에 접두사 부여   ex) api.dallog.me\n    - `ANSWER`: IP   ex) 123.456.123.456\n- `CNAME`\n    - 실제 호스프명(A 레코드)와 연결되는 별칭 도메인을 정의한다.\n    - `HOST`: 도메인에 접두사 부여   ex) api.dallog.me\n    - `ANSWER`: 연결할 별칭 도메인 정의\n- `TXT`\n    - 임의의 문자열로 구성된 값. 소유한 도메인에 TXT 레코드를 추가하여 유효한 도메인인지 판단할 수 있다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 전체적인 구조 배포 과정을 다루기 전에 전체적인 구조부터 살펴볼게요.  총 5개의 EC2를 사용하고 있습니다. 하나하나 살펴보겠습니다. 1. Jenkins 젠킨스를 위한 인스턴스입니다. 젠킨스는 설정이 필요한 부분이 많아 도커 위에 젠킨스를 띄워놓았습니다. 2. FrontEnd 프론트엔드 배포를 위한 인스…","fields":{"slug":"/deploy-full-course1/"},"frontmatter":{"date":"August 07, 2022","title":"달록 배포 총🔫정리 1 - 전체적인 구조 살펴보기","tags":["deploy","DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 전체적인 구조\n\n배포 과정을 다루기 전에 전체적인 구조부터 살펴볼게요.\n\n![](structure.png)\n\n총 5개의 EC2를 사용하고 있습니다. 하나하나 살펴보겠습니다.\n\n### 1. Jenkins\n\n젠킨스를 위한 인스턴스입니다. 젠킨스는 설정이 필요한 부분이 많아 도커 위에 젠킨스를 띄워놓았습니다.\n\n### 2. FrontEnd\n\n프론트엔드 배포를 위한 인스턴스입니다. 여기선 Nginx가 정적 파일을 처리하는 역할을 합니다.\n\n### 3. BackEnd Dev\n\n실제 배포 전 테스트를 위한 백엔드 개발 서버입니다. 한 인스턴스 안에 Nginx와 Spring 함께 넣어두었습니다.\n\n### 4. Web Server\n\n리버스 프록시 역할을 하는 Nginx가 있습니다. 프론트엔드에서 받은 요청을 백엔드 서버로 전달합니다.\n\n- 리버스 프록시란? [https://hudi.blog/forward-proxy-reverse-proxy/](https://hudi.blog/forward-proxy-reverse-proxy/)\n\n\n### 5. BackEnd Prod\n\n운영를 위한 백엔드 서버입니다. WS에서 받은 요청을 실제로 처리하여 응답을 전달합니다.\n\n## CD 흐름\n\n![](cd.png)\n\nCD 흐름은 위와 같습니다. github의 webhook이 merge를 감지하면 젠킨스에 알려줍니다. 흐름은 두 가지 케이스로 나뉩니다.\n\n1. dev 브랜치가 머지된 경우\n\n프론트엔드 서버와 백엔드 dev 서버로 빌드된 파일을 보냅니다.\n\n2. main 브랜치가 머지된 경우\n\n프론트엔드 서버와 백엔드 prod 서버로 빌드된 파일을 보냅니다.\n\n\n## 도커를 제거한 이유\n\n배포 구조를 개편하면서 젠킨스를 제외한 모든 EC2 인스턴스에서 도커를 제거하였습니다. 도커를 제거한 이유는 크게 두 가지입니다.\n\n1. depth가 깊어지면서 관리 포인트가 늘어나고 설정을 수정하기가 과하게 복잡하고 힘들어진다.\n2. 도커 안에 Nginx를 띄우니 EC2가 무거워진다.\n\n도커를 통해 얻을 수 있는 장점보다 단점이 훨씬 뚜렷하여 도커를 제거하였습니다. 필요성을 느껴서 기술을 도입한 것이 아니라 남들 다 하니까, 별 생각 없이 기술을 도입했을 때의 문제점을 절실히 깨달은 순간이었습니다.😅 그래도 이번 기회로 도커를 직접 다뤄보고 장점과 단점을 몸소 느낄 수 있어서 좋았습니다.\n\n젠킨스의 경우 설정이 복잡하여 도커를 사용하는 것이 오히려 장점이 더 크다고 느껴 도커를 빼지 않았습니다. 추후에 여력이 된다면 백엔드 배포 서버를 여러 개 띄우고 도커 이미지를 통해 동일한 설정으로 관리해보는 것도 좋을 것 같다는 생각이 들었습니다.\n\n## 백엔드 서버와 Web Server를 나눈 이유\n\n기존의 백엔드 서버를 개발 서버로 전환하고, 운영 서버를 새로 만들면서 백엔드 서버와 web server를 분리했습니다. 그 이유는 아래와 같습니다.\n\n![](ws.png)\n\n1. Nginx 서버를 따로 두면서 리버스 프록시의 장점을 누릴 수 있다.\n2. 추후 백엔드 서버 확장에 유리하다.\n3. 여러 개의 백엔드 서버를 두더라도 https를 위한 ssl 인증서 발급 절차를 web server에서 한 번만 진행해도 된다.\n\n현재는 두 개를 나눈 장점을 100% 느낄 수 없지만 앞으로 백엔드 서버를 추가할 경우에 매우 편할 것으로 예상하고 있습니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 배경 최근 달록팀의 젠킨스 EC2 인스턴스가 문제가 생겼습니다. 여러 문제가 얽혀서 문제의 원인을 추적하고 해결하는 것 보다 인스턴스를 제거하고 새로 띄워 젠킨스를 다시 설치하는 것이 빠를 것 같았습니다. 다행히 젠킨스 설치 및 초기 세팅에 대한 문서화는 잘 되어 있어서 초기에 젠킨스를 설치한 제가 아니더…","fields":{"slug":"/jenkins-pipeline-from-scm/"},"frontmatter":{"date":"August 07, 2022","title":"젠킨스 파이프라인 스크립트 형상 관리","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 배경\n\n최근 달록팀의 젠킨스 EC2 인스턴스가 문제가 생겼습니다. 여러 문제가 얽혀서 문제의 원인을 추적하고 해결하는 것 보다 인스턴스를 제거하고 새로 띄워 젠킨스를 다시 설치하는 것이 빠를 것 같았습니다. 다행히 젠킨스 설치 및 초기 세팅에 대한 문서화는 잘 되어 있어서 초기에 젠킨스를 설치한 제가 아니더라도 다른 크루가 쉽게 젠킨스를 셋업할 수 있었습니다.\n\n하지만, 파이프라인 스크립트에 대한 문서는 꾸준히 갱신되지 않아 파이프라인 스크립트를 다시 작성해야하는 문제가 발생했습니다. 이런 배경에서 젠킨스 파이프라인 스크립트의 형상 관리를 고민하게 되었습니다.\n\n## 젠킨스에서의 형상관리\n\n흔히 SCM이라고 불리는 소프트웨어 형상 관리(Software Configuration Management)란 무엇일까요? 형상 관리는 소프트웨어의 변경 사항을 체계적으로 추적하고 통제하는 것을 의미합니다. 현재 가장 많이 사용되는 형상 관리 도구는 Git 입니다. 달록팀의 백엔드, 프론트엔드 코드도 모두 Git으로 형상 관리되고 있습니다.\n\n그런데, 젠킨스 파이프라인을 형상관리하는 방법은 없을까요? 다행히도 젠킨스는 파이프라인 스크립트의 형상관리를 지원합니다. 형상관리를 함으로써 젠킨스 서버와 독립적으로 파이프라인 스크립트를 관리할 수 있습니다. 또한 젠킨스 파이프라인 스크립트도 버전관리 대상이 됨으로서 문제 발생 시 쉬운 롤백 등의 여러 이점을 가져갈 수 있습니다.\n\n달록팀이 젠킨스 파이프라인 스크립트를 형상 관리한 방법에 대해 소개드리겠습니다.\n\n### 레포지토리에 파이프라인 스크립트 추가\n\n파이프라인 스크립트를 별도의 레포지토리에서 관리할 수도 있겠지만, 달록팀은 프로젝트와 같은 레포지토리에서 관리하도록 결정 하였습니다. 레포지토리의 제일 상위에 `jenkins` 라는 디렉토리를 만들고, 그 하위에 젠킨스 잡(job)별로 파일을 생성하고 파이프라인 스크립트를 작성합니다.\n\n파일명은 `OOO.jenkinsfile` 으로 통일하였습니다. 예를 들어 백엔드 개발 서버에 대한 파이프라인 스크립트는 `backend-dev.jenkinsfile` 로 명명하였습니다. 파일에는 원래 젠킨스에서 돌아가고 있던 파이프라인 스크립트를 그대로 가져오면 됩니다.\n\n### 환경변수 설정\n\n그런데, 원래라면 젠킨스 파이프라인 스크립트가 외부에 노출되지 않았기 때문에 민감한 정보(서버의 IP정보 등)가 그대로 파이프라인 스크립트에 작성되어 있었습니다. 이런 민감한 정보가 외부로 노출되지 않도록 젠킨스의 환경변수로 분리하고, 파이프라인 스크립트에서는 민감한 정보를 젠킨스의 환경 변수에서 가져오도록 해야합니다.\n\n**Jenkins 관리 > 시스템 설정 > Global properties > Enviroment variables** 에 민감한 정보를 환경변수로 등록합니다. 그리고 파이프라인 스크립트에서는 아래와 같이 환경변수의 이름을 명시하여 사용합니다.\n\n```\nstage('Deploy') {\n    steps {\n        dir('backend/build/libs') {\n            sshagent(credentials: ['key-dallog']) {\n                sh \"scp -o StrictHostKeyChecking=no backend-0.0.1-SNAPSHOT.jar ubuntu@${env.BACKEND_DEV_IP}:/home/ubuntu\"\n                sh \"ssh -o StrictHostKeyChecking=no ubuntu@${env.BACKEND_DEV_IP} 'sh run.sh' &\"\n            }\n        }\n    }\n}\n```\n\n### Pipeline script from SCM\n\n형상 관리를 진행할 파이프라인의 **구성** 메뉴에 진입하고, **Advanced Project Options > Pipeline > Definition** 에서 **Pipeline script from SCM**을 선택합니다.\n\n**SCM**에서 **Git**을 선택하고, **Repositories** 메뉴에 레포지토리 URL과 자격 증명(Credential), 브랜치 관련 설정을 해줍니다. 달록팀은 레포지토리를 퍼블릭으로 열어두었으므로 별도의 자격 증명 설정은 하지 않았습니다. **Repository browser**는 자동으로 설정합니다.\n\n**Script Path** 에는 파이프라인 스크립트가 위치한 레포지토리의 경로를 명시하면 됩니다. 백엔드 개발 서버라면 `jenkins/backend-dev.jenkinsfile` 로 입력해두면 됩니다.\n\n모든 세팅이 끝났습니다. 지금 빌드를 클릭해보면 정상적으로 Git에서 파이프라인 스크립트를 가져와 실행되는 모습을 확인할 수 있을 것 입니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. UI가 비슷하면 재사용 초반에 개발을 진행할 때 은 대부분 UI였습니다. \"UI가 비슷하면 분명 재사용될 것이니 이 부분들을 묶어서 컴포넌트화하자!\" 다음은 달록의 카테고리 목록을 조회하는 페이지입니다.  BE 공식일정 카테고리 알록달록 팀 회의 카테고리 두 카테고리가 매우 흡사하게 생겼죠? 비슷하게 생겼…","fields":{"slug":"/seperate-components/"},"frontmatter":{"date":"August 01, 2022","title":"컴포넌트 분리 기준","tags":["react"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\n## UI가 비슷하면 재사용\n\n> 초반에 개발을 진행할 때 `재사용의 기준, 컴포넌트의 분리 기준`은 대부분 UI였습니다.\n\n\"UI가 비슷하면 분명 재사용될 것이니 이 부분들을 묶어서 컴포넌트화하자!\"\n\n다음은 달록의 카테고리 목록을 조회하는 페이지입니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/efeb82b3-fc46-4acc-b9d3-2df795c0fe51/image.png)\n\n- BE 공식일정 카테고리\n- 알록달록 팀 회의 카테고리\n\n두 카테고리가 매우 흡사하게 생겼죠? 비슷하게 생겼으니 하나의 컴포넌트로 취급하여 사용했습니다. (CategoryItem)\n\n```tsx\nfunction CategoryItem({ category, subscriptionId }: CategoryItemProps) {\n  // ... 생략\n  \n  // ⚠️구독을 위한 react query\n  const { mutate: postSubscription } = useMutation<\n    AxiosResponse<Pick<SubscriptionType, 'color'>>,\n    AxiosError,\n    Pick<SubscriptionType, 'color'>,\n    unknown\n  >(() => subscriptionApi.post(accessToken, category.id, body), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️구독 해제를 위한 react query\n  const { mutate: deleteSubscription } = useMutation(\n    () => subscriptionApi.delete(accessToken, subscriptionId),\n    {\n      onSuccess: () => {\n        queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n      },\n    }\n  );\n\n  // ⚠️구독 해제 로직 \n  const unsubscribe = () => {\n    if (window.confirm(CONFIRM_MESSAGE.UNSUBSCRIBE)) {\n      deleteSubscription();\n    }\n  };\n\n  // ⚠️구독 버튼을 눌렀을 때 구독 여부에 따라 수행해야할 로직 변경\n  const handleClickSubscribeButton = () => {\n    subscriptionId > 0 ? unsubscribe() : postSubscription(body);\n  };\n\n  return (\n    <div css={categoryItem(theme)}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        // ⚠️구독 여부에 따른 버튼 스타일링 변경\n        <SubscribeButton\n          isSubscribing={subscriptionId > 0}\n          handleClickSubscribeButton={handleClickSubscribeButton}\n        ></SubscribeButton>\n      </div>\n    </div>\n  );\n}\n\nexport default CategoryItem;\n\n```\n구독 여부에 따라 버튼 스타일링만 다르게 하였고 실제로 큰 문제없이 의도한대로 렌더링되었습니다.\n\n---\n\n## 하지만.. 문제 발생\n\n> 문제는 API를 연동했을 때 발생했습니다. (부끄럽게도 컴포넌트에 많은 고민과 설계 없이 구현한 대가를 치룬 것이죠.)\n\n![](https://velog.velcdn.com/images/jhy979/post/ebd07538-e6d0-48cf-a43c-e04562ba3b65/image.png)\n\nBE 공식일정은 이미 구독중인 상태여서 버튼을 누르면 `구독해제` api가 실행되어야합니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/236b2064-2dba-4e75-8130-f4074fe20732/image.png)\n알록달록 팀 회의는 버튼을 누르면 `구독` api가 실행되어야합니다.\n\n😢 문제는 바로 두 카테고리의 `데이터 스키마가 다르다는 점`이였습니다.\n\n\n- `구독 api`는 구독을 위한 `카테고리 id`가 필요합니다. (애초에 구독을 안했기 때문에 구독 id가 존재할 수 없습니다.)\n\n- `구독 해제 api`에서는 카테고리 id가 아닌 구독으로 새롭게 발급된 `구독 id`가 필요합니다.\n\n구독 여부와 관계없이 하나의 컴포넌트로(CategoryItem) 묶었을 때에는 위 2가지 케이스로 인해 `구독 id`에서 문제가 발생합니다. \n\n하나의 컴포넌트로 묶었기 때문에 구독id가 존재하든 존재하지 않든 subscriptionId라는 필드가 반드시 존재해야합니다.\n\n😱 요리조리 머리를 굴려 `구독하지 않았을 때에는 구독 id를 -1로 할당했습니다.`\n(아마 위의 코드를 읽으시면서 subscriptionId > 0인 경우 구독 중이라고 판단하는 로직을 어색하게 느끼셨을 겁니다.)\n\n물론 실제 DB에서 구독 id에 -1이 할당되는 경우는 없기 때문에 큰 문제는 아닐 수 있습니다.\n\n---\n## 새로운 컴포넌트의 분리기준: 데이터 스키마, 모델\n\n하지만 태초에 \"이런 컴포넌트 설계가 맞을까?\" 라는 생각이 들었고 팀 회의를 통해 컴포넌트의 분리 기준을 `✨데이터 스키마와 모델✨`에 초점을 맞추는 방향으로 바꾸었습니다.\n\n> 즉, 구독 중인 카테고리와 구독하지 않은 카테고리는 구독id의 존재 여부가 다르기 때문에 데이터 스키마가 다르다고 말할 수 있겠습니다.\n\n따라서, 구독 중인 카테고리 컴포넌트와 구독하지 않은 카테고리 컴포넌트 2가지 컴포넌트로 분리하였습니다.\n\n---\n\n실제 코드를 볼까요?\n\n### 👇 구독하지 않은 컴포넌트 (UnsubscribedCategoryItem)\n\n![](https://velog.velcdn.com/images/jhy979/post/236b2064-2dba-4e75-8130-f4074fe20732/image.png)\n\n- 구독 id가 없습니다.\n- 구독 react query가 있습니다.\n\n```tsx\n// ⚠️subscriptionId를 아예 받지 않음\nfunction UnsubscribedCategoryItem({ category }: UnsubscribedCategoryItemProps) {\n  // ... 생략\n\n  // ⚠️구독을 위한 react query\n  const { mutate } = useMutation<\n    AxiosResponse<Pick<SubscriptionType, 'color'>>,\n    AxiosError,\n    Pick<SubscriptionType, 'color'>,\n    unknown\n  >(() => subscriptionApi.post(accessToken, category.id, body), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️오직 한 가지 일만 담당하는 핸들러 함수\n  const handleClickSubscribeButton = () => {\n    mutate(body);\n  };\n\n  return (\n    <div css={categoryItem}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        <Button cssProp={subscribeButton(theme)} onClick={handleClickSubscribeButton}>\n          구독\n        </Button>\n      </div>\n    </div>\n  );\n}\n```\n\n---\n### 👇 구독중인 컴포넌트 (SubscribedCategoryItem)\n\n![](https://velog.velcdn.com/images/jhy979/post/ebd07538-e6d0-48cf-a43c-e04562ba3b65/image.png)\n\n- 구독 id가 있습니다.\n- 구독 해제 react query가 있습니다.\n\n```tsx\nfunction SubscribedCategoryItem({ category, subscriptionId }: SubscribedCategoryItemProps) {\n  // ... 생략\n  \n  // ⚠️구독 해제를 위한 react query\n  const { mutate } = useMutation(() => subscriptionApi.delete(accessToken, subscriptionId), {\n    onSuccess: () => {\n      queryClient.invalidateQueries(CACHE_KEY.SUBSCRIPTIONS);\n    },\n  });\n\n  // ⚠️오직 한 가지 일만하는 핸들러 함수\n  const handleClickUnsubscribeButton = () => {\n    if (window.confirm(CONFIRM_MESSAGE.UNSUBSCRIBE)) {\n      mutate();\n    }\n  };\n\n  return (\n    <div css={categoryItem}>\n      <span css={item}>{category.createdAt.split('T')[0]}</span>\n      <span css={item}>{category.name}</span>\n      <div css={item}>\n        <Button cssProp={unsubscribeButton(theme)} onClick={handleClickUnsubscribeButton}>\n          구독중\n        </Button>\n      </div>\n    </div>\n  );\n}\n```\n\n---\n### 마무리\n\n> 어떤가요?\n\n💪 subscriptionId가 필요하지 않은 경우에는 애초에 props로 들어가지 않기 때문에 `subscriotionId가 -1이 되는 경우가 없습니다.`\n\n또한 react query와 관련된 로직들은 어떠한가요? \n\n💪 관심사 분리가 적절히 이루어져 각 컴포넌트에서 알맞게 호출되고 있습니다.\n\n\n> UI를 기준으로 컴포넌트를 나누는 것도 좋지만 먼저 데이터 스키마에 따라 컴포넌트를 분리하는 것을 먼저 고려해보면 더 좋을 것이라고 이번 리팩토링을 통해 느끼게 되었습니다.\n\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. React-Query의 캐싱개념은 stale과 cacheTime을 통해 이루어집니다. Stale 사전적 의미로 '신선하지 않은' 입니다. react query는 기본적으로 캐싱된 데이터를 stale하다고 생각합니다. react query에서는 stale time의 default이 0입니다. (즉, 캐싱이 …","fields":{"slug":"/query-invalidation/"},"frontmatter":{"date":"August 01, 2022","title":"React-Query에서의 데이터 최신화 (Query Invalidation)","tags":["react","react-query"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\nReact-Query의 캐싱개념은 stale과 cacheTime을 통해 이루어집니다.\n\n## Stale\n> 사전적 의미로 '신선하지 않은' 입니다. react query는 기본적으로 캐싱된 데이터를 stale하다고 생각합니다.\n\nreact query에서는 stale time의 default이 0입니다. (즉, 캐싱이 되지 않는다고 볼 수 있겠죠?)\n\n### staleTime\n> 데이터가 fresh한 상태에서 stale한 상태로 변하는 시간입니다.\n\n- fresh 상태일때는 쿼리 인스턴스가 새롭게 mount 되어도 fetch가 일어나지 않습니다. \n\n- 데이터가 fetch 되고 나서 staleTime이 지나지 않았다면 unmount 후 mount 되어도 fetch가 일어나지 않습니다.\n\n### cacheTime\n> 데이터가 inactive 상태일 때 캐싱된 상태로 남아있는 시간입니다.\n\n- 쿼리 인스턴스가 unmount 되면 데이터는 inactive 상태로 변경되며, 캐시는 cacheTime만큼 유지됩니다.\n\n- cacheTime은 staleTime과 관계없이, 무조건 `inactive된 시점`을 기준으로 캐싱을 결정합니다.\n\n---\n## 달록에서의 데이터 최신화\n\n> 달록은 일정, 카테고리, 구독 등의 작업으로 인해 데이터의 변화가 잦은 어플리케이션입니다. 따라서 저희 팀은 staleTime을 지정해주지 않았습니다.\n\n그렇다면 stale한 데이터는 늘 최신화가 필요할 것입니다.\n\n그래서 저는 매번 새로운 데이터가 필요할 때마다 `useQuery의 refetch`를 강제적으로 실행시켜주는 방식을 생각했었습니다.\n\n예를 들면 일정을 추가(post)한 후에 일정을 다시 조회(get)하는 경우가 있을겁니다.\n\n![](https://velog.velcdn.com/images/jhy979/post/287f66a2-8b7e-49c8-b623-c05a491db600/image.png)\n\n```tsx\n// refetch 함수를 부모로 부터 주입 받아야 합니다.\nfunction ScheduleAddModal({ closeModal, refetch }: ScheduleAddModalProps) {\n  const onSuccessPostSchedule = () => {\n    refetch();\n  };\n\n```\n\n😢 하지만 이 방식이 우아하지는 않더라구요. \n\n만약 부모와 자식 간의 props전달이 아니라 조부모와 자식 간의 props 전달이라면 어떨까요? 벌써 머리가 아파옵니다. (props hell)\n\n\n## Query Invalidation 도입\n🤔 다른 방법이 있을텐데.. 어떤 방법이 있을까? 고민하며 공식문서를 읽던 도중 `Query Invalidation`을 발견하게 되었습니다.\n\n```tsx\n// Invalidate every query in the cache\nqueryClient.invalidateQueries()\n// Invalidate every query with a key that starts with `todos`\nqueryClient.invalidateQueries(['todos'])\n```\n> The QueryClient has an invalidateQueries method that lets you intelligently mark queries as stale and potentially refetch them too!\n\n❗ 캐싱키로 관련된 stale 쿼리들을 체크하고 refetch할 수 있는 메서드가 존재했습니다.\n\n```tsx\nfunction ScheduleAddModal({ closeModal }: ScheduleAddModalProps) {\n  const queryClient = useQueryClient();\n\n  const onSuccessPostSchedule = () => {\n    // 일정 post 성공 시 데이터 최신화를 invalidateQueries메서드를 통해 수행합니다.\n    queryClient.invalidateQueries(CACHE_KEY.SCHEDULES);\n  };\n```\n💪 이로써 useQuery의 refetch함수를 넘겨줄 필요없이 어디서든 캐싱키로 fresh한 데이터를 보장할 수 있게 되었습니다.\n\n\n\n#### 참고자료\nhttps://tanstack.com/query/v4/docs/guides/query-invalidation"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. @SpringBootTest  애노테이션을 사용하면 우리 애플리케이션에서 사용하고 있는 모든 빈을 등록한 뒤 간편하게 테스트를 진행한다. 하지만 모든 빈을 등록하기 때문에 아래와 같은 단점을 가질 수 있다. 모든 빈들을 등록하기 때문에 비교적 오랜 시간이 걸린다. 모든 빈들을 등록하기 때문에 의존성을 …","fields":{"slug":"/integration-test-slice-test/"},"frontmatter":{"date":"August 01, 2022","title":"통합 테스트와 슬라이스 테스트","tags":["매트","BE","통합 테스트","슬라이스 테스트"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## @SpringBootTest\n\n`@SpringBootTest` 애노테이션을 사용하면 우리 애플리케이션에서 사용하고 있는 모든 빈을 등록한 뒤 간편하게 테스트를 진행한다. 하지만 모든 빈을 등록하기 때문에 아래와 같은 단점을 가질 수 있다.\n\n- 모든 빈들을 등록하기 때문에 비교적 오랜 시간이 걸린다.\n- 모든 빈들을 등록하기 때문에 의존성을 고려하지 않고 테스트를 진행할 수 있다. 즉 테스트 하고자 하는 객체의 의존성을 무시한채 테스트하게 된다.\n- 도메인 혹은 영속 계층의 경우 application 계층, presentation 계층에 대해 의존하지 않기 때문에 필요 없는 리소스에 대한 소모가 늘어난다.\n\n이러한 `@SpringBootTest`는 모든 빈을 등록한 채 테스트를 진행하는 `통합 테스트`에 적합한 애노테이션이다.\n\n아래는 실제 프로젝트에 작성한 테스트 중 일부를 가져온 것이다. \n\n```java\n@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Import(TestConfig.class)\nclass AcceptanceTest {\n\n    @LocalServerPort\n    private int port;\n\n    @Autowired\n    private DatabaseCleaner databaseCleaner;\n\n    @BeforeEach\n    void setUp() {\n        RestAssured.port = port;\n        databaseCleaner.execute();\n    }\n}\n```\n\n```java\n@DisplayName(\"구독 관련 기능\")\n@Import(TestConfig.class)\npublic class SubscriptionAcceptanceTest extends AcceptanceTest {\n\n    @DisplayName(\"인증된 회원이 카테고리를 구독하면 201을 반환한다.\")\n    @Test\n    void 인증된_회원이_카테고리를_구독하면_201을_반환한다() {\n        // given\n        String accessToken = 자체_토큰을_생성하고_토큰을_반환한다(GOOGLE_PROVIDER, 인증_코드);\n        CategoryResponse 공통_일정 = 새로운_카테고리를_등록한다(accessToken, 공통_일정_생성_요청);\n\n        // when\n        ExtractableResponse<Response> response = RestAssured.given().log().all()\n                .auth().oauth2(accessToken)\n                .contentType(MediaType.APPLICATION_JSON_VALUE)\n                .body(빨간색_구독_생성_요청)\n                .when().post(\"/api/members/me/categories/{categoryId}/subscriptions\", 공통_일정.getId())\n                .then().log().all()\n                .statusCode(HttpStatus.CREATED.value())\n                .extract();\n\n        // then\n        상태코드_201이_반환된다(response);\n    }\n    ...\n}\n```\n\n인수 테스트의 경우 사용자의 시나리오에 맞춰 수행하는 테스트이기 때문에 실제 운영 환경과 유사하게 테스트를 진행해야 한다. 그렇기 때문에 슬라이스 테스트를 진행하는 것 보다 모든 빈들을 등록하여 시나리오를 적절히 수행하는지 집중해야 하기 때문에 `@SpringBootTest`를 활용한 통합 테스트를 진행해야 한다.\n\n## 슬라이스 테스트\n\n앞서 언급한 것 처럼 특정 계층은 다른 계층에 의존하지 않기 때문에 필요한 빈들만 주입받아 독립적으로 테스트할 수 있다. 이렇게 계층별로 필요한 빈들만 주입받기 위해서 Spring은 슬라이스 테스트관련 애노테이션을 제공한다.\n\n슬라이스 테스트란? 레이어를 독립적으로 테스트하기 위해 필요한 빈들만 주입 받아 테스트를 진행하는 것을 의미한다.. 슬라이스 테스트를 적절히 활용하면 모든 빈들을 ApplicationContext에 등록하지 않기 때문에 보다 더 호율적으로 테스트가 가능하다.\n\n### @WebMvcTest\n\n`@WebMvcTest`는 웹 계층 테스트를 위해 필요한 빈들이 주입된다. 주입 되는 빈의 항목은 아래와 같다.\n\n- `@Controller`\n- `@ControllerAdvice`\n- `@JsonComponent`\n- `Converter`\n- `Filter`\n- `WebMvcConfigurer`\n- `HandlerMethodArgumentResolver`\n- `MockMvc`\n- 등\n\n아래는 실제 프로젝트에 작성한 테스트 중 일부를 가져온 것이다.\n\n```java\n@AutoConfigureRestDocs\n@WebMvcTest(SubscriptionController.class)\nclass SubscriptionControllerTest {\n\n    private static final String AUTHORIZATION_HEADER_NAME = \"Authorization\";\n    private static final String AUTHORIZATION_HEADER_VALUE = \"Bearer aaaaa.bbbbb.ccccc\";\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Autowired\n    private ObjectMapper objectMapper;\n\n    @MockBean\n    private AuthService authService;\n\n    @MockBean\n    private SubscriptionService subscriptionService;\n\n    @DisplayName(\"회원과 카테고리 정보를 기반으로 구독한다.\")\n    @Test\n    void 회원과_카테고리_정보를_기반으로_구독한다() throws Exception {\n        // given\n        CategoryResponse 공통_일정_응답 = 공통_일정_응답(관리자_응답);\n        SubscriptionResponse 빨간색_구독_응답 = 빨간색_구독_응답(공통_일정_응답);\n\n        given(authService.extractMemberId(any())).willReturn(매트_응답.getId());\n        given(subscriptionService.save(any(), any(), any())).willReturn(빨간색_구독_응답);\n\n        // when & then\n        mockMvc.perform(post(\"/api/members/me/categories/{categoryId}/subscriptions\", 빨간색_구독_응답.getId())\n                        .header(AUTHORIZATION_HEADER_NAME, AUTHORIZATION_HEADER_VALUE)\n                        .accept(MediaType.APPLICATION_JSON).contentType(MediaType.APPLICATION_JSON)\n                        .content(objectMapper.writeValueAsString(빨간색_구독_생성_요청)))\n                .andDo(print())\n                .andDo(document(\"subscription/save\",\n                        preprocessRequest(prettyPrint()),\n                        preprocessResponse(prettyPrint()),\n                        pathParameters(\n                                parameterWithName(\"categoryId\").description(\"카테고리 id\")\n                        ),\n                        requestHeaders(\n                                headerWithName(\"Authorization\").description(\"JWT 토큰\")\n                        ),\n                        requestFields(\n                                fieldWithPath(\"color\").type(JsonFieldType.STRING).description(\"구독 색 정보\")\n                        )))\n                .andExpect(status().isCreated());\n    }\n    ...\n]\n```\n\n컨트롤러 테스트의 경우 실제 동작하는 로직을 활용하는 것 보다 API의 문서화에 집중했기 때문에 웹 계층에 대한 의존성만 추가한 뒤 의존하는 객체는 Mocking을 통해 진행하였다.\n\n### @DataJpaTest\n\nSpring Data JPA를 사용하고 있다면 테스트하기 위해 간단히 `@DataJpaTest`를 활용할 수 있다. `@Entity` 객체, `JpaRepository` 등 JPA 사용에 필요한 빈들을 등록하여 테스트할 때 사용한다. 아래는 실제 `@DataJpaTest`의 코드를 가져온 것이다.\n\n```java\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@BootstrapWith(DataJpaTestContextBootstrapper.class)\n@ExtendWith(SpringExtension.class)\n@OverrideAutoConfiguration(enabled = false)\n@TypeExcludeFilters(DataJpaTypeExcludeFilter.class)\n@Transactional\n@AutoConfigureCache\n@AutoConfigureDataJpa\n@AutoConfigureTestDatabase\n@AutoConfigureTestEntityManager\n@ImportAutoConfiguration\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n주의 깊게 봐야할 애노테이션들이 많다. 몇 가지 예시로 `@AutoConfigureTestDatabase`, `@Transactional`에 대해 간단히 살펴보자.\n\n#### @AutoConfigureTestDatabase\n\n```java\n...\n@AutoConfigureTestDatabase\n...\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n애플리케이션에 정의되어 있거나 자동으로 설정된 DataSoruce를 대신하여 테스트용 DB를 정의할 때 사용된다. 아래 실제 코드를 살펴보자.\n\n```java\n@Target({ ElementType.TYPE, ElementType.METHOD })\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@ImportAutoConfiguration\n@PropertyMapping(\"spring.test.database\")\npublic @interface AutoConfigureTestDatabase {\n\n\t@PropertyMapping(skip = SkipPropertyMapping.ON_DEFAULT_VALUE)\n\tReplace replace() default Replace.ANY;\n\n\tEmbeddedDatabaseConnection connection() default EmbeddedDatabaseConnection.NONE;\n\n\tenum Replace {\n\t\tANY,\n\t\tAUTO_CONFIGURED,\n\t\tNONE\n\t}\n}\n```\n\n- `replace()`:  대체할 수 있는 기존 DataSource 빈의 유형을 결정한다.\n    - `Replace.ANY`: 자동 구성 또는 수동 정의의 여부에 상관 없이 DataSource를 교체한다. default 설정 이기 때문에 `@DataJpaTest`를 사용하면 기본적으로 `in-memory embedded database`를 활용한다.\n    - `Replace.AUTO_CONFIGURED`: 자동 설정된 경우에만 DataSource를 교체한다.\n    - `Replace.NONE`: 기본 DataSource를 교체하지 않는다. 즉 우리가 직접 빈으로 등록하거나 명시한 DataSource를 사용한다. 만약 `in-memory embedded database`가 아닌 외부 DB나 테스트 용 DB를 사용하고 싶다면 `@AutoConfigureTestDatabase(replace = Replace.NONE)`으로 설정을 덮어야 한다.\n\n#### @Transactional\n\n```java\n...\n@Transactional\n...\npublic @interface DataJpaTest {\n\t...\n}\n```\n\n앞서 언급한 것 처럼 `@DataJpaTest`는 기본적으로 `@Transactional` 애노테이션을 들고 있기 때문에 테스트가 완료되면 자동으로 롤백된다.\n\n아래는 실제 프로젝트를 진행하며 작성한 테스트 코드의 일부를 가져온 것이다.\n\n```java\n@DataJpaTest\n@Import(JpaConfig.class)\nclass ScheduleRepositoryTest {\n\n    @Autowired\n    private ScheduleRepository scheduleRepository;\n\n    @DisplayName(\"시작일시와 종료일시를 전달하면 그 사이에 해당하는 일정을 조회한다.\")\n    @Test\n    void 시작일시와_종료일시를_전달하면_그_사이에_해당하는_일정을_조회한다() {\n        // given\n        Schedule schedule1 = new Schedule(TITLE, LocalDateTime.of(2022, 7, 14, 14, 20),\n                LocalDateTime.of(2022, 7, 15, 16, 20), MEMO);\n\n        Schedule schedule2 = new Schedule(TITLE, LocalDateTime.of(2022, 8, 15, 14, 20),\n                LocalDateTime.of(2022, 8, 15, 16, 20), MEMO);\n\n        scheduleRepository.save(schedule1);\n        scheduleRepository.save(schedule2);\n\n        // when\n        List<Schedule> schedules = scheduleRepository.findByBetween(START_DAY_OF_MONTH, END_DAY_OF_MONTH);\n\n        // then\n        assertThat(schedules).hasSize(1);\n    }\n    ...\n}\n```\n\n## 정리\n\n이 밖에도 `@JdbcTest`, `@DataMongoTest`, `@RestClientTest` 등 다양한 슬라이스 테스트를 위한 애노테이션이 제공된다. 어떠한 애노테이션을 사용하는 것에 집중하기 보다 `테스트의 목적`에 대해 고민해야 한다. 테스트하고자 하는 것에 집중하여 의존하거나 필요한 빈들에 대해 고민한 뒤 적절한 애노테이션을 적용하면 불필요한 리소스를 줄일 수 있으며 보다 더 빠른 테스트 피드백을 확인할 수 있을 것이라 판단한다.\n\n## References.\n[Spring Boot Test](https://meetup.toast.com/posts/124)<br>\n[8. Testing](https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing)<br>\n[Spring Boot 슬라이스 테스트](https://tecoble.techcourse.co.kr/post/2021-05-18-slice-test/)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 글을 쓰게 된 계기 우아한테크코스 레벨3 프로젝트를 진행하면서 클라이언트와 서버의 통신에 SSL 인증서를 적용하여 HTTPS 통신을 하도록 하였다.\nHTTPS의 통신과정에 대한 이해를 공유하기 위해 글을 작성하고자 한다. HTTPS 는 SSL(Secure Sokect Layer) 위에 를 얹어서 보안이…","fields":{"slug":"/ssl_protocol/"},"frontmatter":{"date":"July 31, 2022","title":"SSL을 통한 HTTPS통신 과정","tags":["HTTPS"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## 글을 쓰게 된 계기\n우아한테크코스 레벨3 프로젝트를 진행하면서 클라이언트와 서버의 통신에 SSL 인증서를 적용하여 HTTPS 통신을 하도록 하였다.\nHTTPS의 통신과정에 대한 이해를 공유하기 위해 글을 작성하고자 한다.\n\n\n## HTTPS\n`HTTPS`는 SSL(Secure Sokect Layer) 위에 `HTTP`를 얹어서 보안이 보장된 통신을 하고자하는 프로토콜이다.\n\n## SSL 인증서\n클라이언트와 웹 서버간의 통신을 제3자가 보증해주는 전자문서이다.\n클라이언트가 서버에 접속한 직후에 서버는 클라이언트에게 `SLL 인증서`를 전달한다.\n클라이언트는 신뢰 할 수 있는 인증서인지 검증한 후 서버와 통신을 진행한다.\n(밑에서 자세히 설명)\n\n## SSL 인증서 사용이유\n왜 HTTP 통신에 굳이 SSL 인증 과정을 붙혓을까? 이유는 다음과 같다.\n- 통신 내용이 악의적인 목적을 가진 사람에게 노출되는 것 을 막을 수 있다.\n- 클라이언트가 접속하는 서버가 신뢰할 수 있는 서버인지 판단 할 수 있다.\n- 통신 내용의 악의적인 변경을 방지 할 수 있다.\n\n\nSSL 통신과정은 꽤나 복잡하다. \n그래서 SSL 프로토콜을 이해하려면 우선 `암호화/복호화`와 `키(대칭키, 공개키)`에 대해 이해할 필요가 있다.\n\n## 암호화, 복호화, 키\n어떤 정보를 외부에 노출시키지 않기 위해 변형하는 것을 `암호화` 라고한다.\n반대로 암호화된 데이터를 원본으로 복원하는것을 `복호화`라고 한다.\n암호화와 복호화에서 데이터 변형을 위해 사용하는것을 `키`라고 한다.\n\n이렇게 키를 사용하여 `암호화/복호화` 하는 방식에는 `대칭키 방식`과 `공개키 방식`이 있다. \n두 방식 모두 `SSL 통신`에 사용되기 때문에 이해할 필요가 있다.\n\n\n## 대칭키 방식\n대칭키는 동일한 키로 `암호화`와 `복호화`를 같이 할 수 잇는 방식의 암호화 기법을 의미한다.\n`암호화`와 `복호화`를 위해 양쪽이 같은 키를 가져야 한다는 점에서 이 키를 `대칭키`라고 한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/01b1f9b4-66ed-46d6-87d4-b18948611b80/image.JPG)\n\n전쟁상황을 가정해서 살펴보자. `A진영`과 `B진영`이 전쟁 중일 때, 적군에게 정보를 노출하지 않으려면 `서로 같은 키(대칭키)`를 가지고 정보를 암호화 해서 전달 할 수 있을 것이다.\n\n그런데, `대칭키`는 하나의 큰 단점을 가진다. 바로 `대칭키`를 `A진영`에서 `B진영` 또는 `B진영`에서 `A진영`으로 전달 할 때, \n`대칭키`가 탈취 된다면 적군도 정보를 복호화 할 수 있다는 것이다.\n\n이런 배경에서 나온 암호화 방식이 `공개키 방식`이다.\n\n## 공개키 방식\n`공개키`는 키가 두개가 있다.\nA키로 암호화 하면 B키로 복호화 할 수 있고 B키로 암호화 하면 A키로 복호화 하는 방식이다.\n\n이 방식에 착안해서 두개의 키 중 하나를 공개키, 하나를 비공개 키로 지정한다. \n\n그림을 통해 자세히 살펴보자.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/58d5be37-7ee5-4d8f-aacb-8ea66c23ecda/image.JPG)\n\n`A진영`은 `공개키`를 `B진영`에게 전달한다.\n그리고 `B진영`은 `A진영`의 `공개키`를 통해 정보를 암호화 한 후 `A진영`에 전달한다.\n`A진영`은 `비공개키`를 통해 전달 받은 정보를 복호화한다.\n\n`A진영`의 공개키가 탈취되거나 `B진영`이 전달한 정보가 탈취되어도 `비공개키`가 없으면 복호화가 불가능 하기 때문에 `대칭키 방식`의 단점을 극복 할 수 있다.\n\n`공개키 방식`은 이렇게 응용 할 수 있다.\n`A진영`이 `비공개키`를 이용해서 정보를 암호화 한 후에 공개키와 함께 암호화된 정보를 `B진영`에 전달한다.\n\n그런데, 이런 응용방식은 중간에 `공개키`와 `암호화된 정보`를 탈취 당하면 적군에게 정보를 노출 당할 수 있는 문제를 가져온다.\n\n그런데 `SSL 통신`에서는 이런 응용방식을 사용한다. 그 이유가 무엇일까?\n\n## SSL 통신이 공개키 응용방식을 사용하는 이유\n그 이유는 `공개키 응용방식`이 데이터 보호 목적이 아니기 때문이다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/bdfe5ab8-c202-4d3a-bd6a-34b1e46b37c3/image.JPG)\n\n`A진영`이 전달한 정보를 `A진영의 공개키`로 복호화 할 수 있다면, 정보가 `A진영`이 전달한 정보임을 신뢰 할 수 있다.\n\n즉, 공개키가 정보를 전달한 사람의 신원을 보장해주는 것이다. 이것을 전자 서명이라고 하고 `SSL 통신`에서 서버의 신원 확인을 위해 사용한다.\n\n\n## SSL 용어 정리\n\n이제 SSL 통신에 사용되는 `대칭키 방식`, `공개키 방식`, `공개키 응용방식`을 살펴보았으니 SSL을 통한 통신에 대해 자세히 살펴보자. \n우선, SSL과 관련된 용어를 정리해보자.\n\n### CA\nSSL 인증서의 역할은 클라이언트가 접속한 서버가 클라이언트가 의도한 서버가 맞는지를 보장하는 역할을 한다. \n이 역할을 하는 민간기업들이 있는데 이런 기업들을 `CA(Certificate authority)` 라고한다.\n\n\n### SSL 인증서 내용\n`SSL 인증서`에는 다음과 같은 정보가 포함되어 있다.\n\n- 서비스의 정보 (인증서를 발급한 CA, 서비스의 도메인 등등)\n- 서버 측 공개키 (공개키의 내용, 공개키의 암호화 방법)\n\n## SSL 통신 과정\n이제 `키를 사용한 3가지 통신 방법`을 살펴보았으니 SSL 통신과정을 이해해보자.\n\n`SSL 통신과정`에서 가장 이상적인 방법은 `공개키 방식`이다.(공개키 응용방식 아님)\n![](https://velog.velcdn.com/images/gudonghee2000/post/b528a5c7-9515-4a8b-a4d8-f1341abf57d2/image.JPG)\n\n\n`클라이언트`와 `서버`는 각각의 `비공개키, 공개키`를 가지고\n자신의 `공개키`를 서로에게 전달한다. 이때, `공개키`는 탈취되어도 `비공개키`가 없으면 복호화가 불가능하기 때문에 보안에 취약점이 없다.\n\n그리고, `클라이언트`는 `서버`의 `공개키`로 정보를 암호화 해서 전달하고 `서버`는 전달 받은 정보를 자신의 `비공개키(서버 비공개키)`로 복호화한 뒤 정보를 처리한다.\n\n마지막으로 `서버`는 `클라이언트의 공개키`로 응답해줄 정보를 암호화해서 다시 클라이언트에게 전달한다.\n\n그런데, `공개키 방식`은 컴퓨팅 파워를 많이 쓴다고한다.(컴퓨팅 파워를 많이쓰는 정확한 이유는 모름) 그래서 성능측면에서 비효율적이다.\n\n결론적으로, `SSL 통신`은 `공개키 방식 + 대칭키 방식`을 사용하여 보안과성능 두가지 측면을 보장한다.\n\n이제 SSL 통신과정을 천천히 자세하게 살펴보자.\n\n> HandShake -> 통신 -> 통신종료\n\n간단하게, SSL 통신은 위 세가지 과정으로 이루어진다.\n순서대로 하나씩 살펴보자.\n\n### HandShake(악수)\nSSL 통신은 데이터를 주고 받기전에 `어떻게 데이터를 암호화 할지`, `믿을 만한 서버인지` 등에 대해 이 과정에서 확인한다.\n\n1. 클라이언트가 서버에 접속한다. 이 단계를 `Client Hello`라고 한다. 이 단계에서 주고 받는 정보는 아래와 같다.\n  \n    - 클라이언트 측에서 생성한 `랜덤 데이터` (밑에서 설명)\n  \n    - 클라이언트가 지원하는 암호화 방식들 => 클라이언트가 가능한 암호화 방식을 서버에 알려주기 위함\n  \n2. 서버는 `Client Hello`에 대한 응답으로 `Server Hello`를 하게 된다. 이 단계에서 주고 받는 정보는 아래와 같다.\n  \n    - 서버 측에서 생성한 랜덤 데이터 (밑에서 설명)\n  \n    - 서버가 선택한 클라이언트의 암호화 방식 => 선택한 암호화 방식을 클라이언트에게 알려주기 위함\n  \n    - 인증서\n  \n  \n3. `클라이언트`는 `서버의 인증서`가 `CA`에 의해 발급된 것인지 확인한다.\n  이때, `클라이언트`에 내장된 `CA리스트와 CA의 공개키`를 사용해서 인증서를 복호화 한다. \n>   💡 참고\n  위에서 말한 공개키 응용방식의 활용\n  그리고, 클라이언트(브라우저)는 CA 종류와 CA의 공개키가 내장되있음\n  \n  성공적으로 인증서가 복호화 됬다면, `서버`가 전달한 인증서가 `CA의 개인키`로 암호화된 문서임이 보증된 것이다.\n**  즉, 올바른 서버임을 신뢰 할수 있게된다.**\n  \n  서버를 신뢰할수 있으므로 클라이언트는 `서버가 생성한 랜덤 데이터`와 `클라이언트가 생성한 랜덤 데이터`를 조합하여 `pre master secret`이라는 키를 생성한다. \n  \n 이때, `pre master secret`키는 대칭키 방식으로 사용 할 것이다.\n 클라이언트와 서버가 동일하게 가지고 데이터를 암호화/복호화 하는데 사용한다는 것이다.\n \n 그런데 대칭키 방식의 단점에서 살펴봤듯이 `pre master secret`를 그대로 서버에 전달하면 중간에 탈취당해 악용될수 있다.\n \n 이때, 사용하는 방법이 `공개키 방식`이다.\n `서버의 공개키(서버가 전달해준 인증서 내부에 들어있었음)`로 `pre master secret`를 암호화해서 `서버`로 전송한다.\n\n\n \n 4. `서버`는 자신의 비공개키를 통해 `pre master secret`를 복호화한다.\n \n 이를 통해, 클라이언트와 서버는 안전하게 같은 `pre master secret`를 가진다.\n 서버와 클라이언트는 일련의 과정을 통해 `pre master secret`을 `master secret`라는 `session key`를 생성한다.\n`master secret`은 실제로 클라이언트와 서버가 주고받는 데이터를 `암호화/복호화` 하는데 사용한다.\n\n5. 클라이언트와 서버는 `HandShake`가 종료됬음을 서로에게 알린다.\n\n\n### 통신\n이제 `master secret`을 통해 `클라이언트`와 `서버`는 데이터를 `암호화/복호화`하면서 주고 받는다. \n\n### 통신종료\n데이터의 전송이 끝나면 SSL 통신이 끝났음을 서로에게 알려준다.\n그리고 사용한 대칭키인 `master secret`은 폐기한다.\n \n \n## 마치면서\n이상 SSL통신에 대해서 자세하게 살펴보았다.\nSSL 통신은 `공개키 방식`, `대칭키 방식`을 합쳐서 사용하는 만큼 꽤나 복잡하다. \n작성한 글이 많은 크루들에게 HTTPS와 SSL 통신에 대한 이해에 도움이 되었으면 좋겠다.\n\n## 참조\nhttps://opentutorials.org/course/228/4894"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 글을 쓰게 된 계기 우테코 레벨3 프로젝트를 진행하면서, HTTPS 통신을 적용하기 위해 NGINX에 대한 이해가 필요했다.\n그래서 NGINX에 대한 이해를 돕기위해 글을 작성하고자 한다. NGINX란? Nginx는 WS(Web Server)의 일종이다. 주로 정적 컨텐츠를 제공하거나 ReversePr…","fields":{"slug":"/what_is_nginx/"},"frontmatter":{"date":"July 30, 2022","title":"NGINX 란?","tags":["WS","NGINX"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## 글을 쓰게 된 계기\n우테코 레벨3 프로젝트를 진행하면서, HTTPS 통신을 적용하기 위해 NGINX에 대한 이해가 필요했다.\n그래서 NGINX에 대한 이해를 돕기위해 글을 작성하고자 한다.\n\n<br>\n\n## NGINX란?\n> _Nginx는 WS(Web Server)의 일종이다. 주로 정적 컨텐츠를 제공하거나 ReverseProxy, LoadBalancer의 역할을 한다._\n\n<br>\n\n## NGINX의 등장 배경\nNGINX를 살펴보기전에, 무슨 이유로 NGINX가 등장했는지를 먼저 살펴볼 필요가 있다. 우선 NGINX의 등장배경을 살펴보자.\n\n<br>\n\nNGINX 등장이전 Apache가 웹서버로써의 높은 인기를 가졌다.\n잘 사용되던 Apache 웹서버는 어느순간 어떠한 문제를 가져왔다. Apache의 요청처리 메커니즘과 함께 어떠한 문제를 가져왔는지 살펴보자. \n![](https://velog.velcdn.com/images/gudonghee2000/post/4c31d7af-155b-4998-9ef3-93559914949e/image.JPG)\n\n\n초기의 Apache는 그림과 같이 요청(Request)이 들어올 떄 마다 새로운 Process를 생성하여 네트워크 연결을 하고 요청을 처리했다.(이러한 처리방식을 `prefork`라고 함)\n\n그런데, 2000년에 접어 들고 인터넷 트래픽이 증가하면서 아파치의 요청 처리방식은 `C10K` 문제를 가져왔다.\n> \nC10K 문제란 Connection 10000 Problem의 줄임말로 웹서버가 1만개의 동시 Connection을 처리하기 어렵다는 의미이다.\n\n\n\n일단 동시 Connection을 자세히 살펴보자.\n![](https://velog.velcdn.com/images/gudonghee2000/post/954a297f-e564-43b1-acfa-0f2452854647/image.JPG)\n\n\n`웹서버`는 `클라이언트`로 부터 요청이 들어오면 `Connection`을 생성하고 유지한다.\n그리고 그림과 같이 `클라이언트`는 생성된 `Connection`을 통해 또다른 요청을 `서버`에게 전달한다. \n\n이렇게 `Connection` 하나로 여러 요청을 처리하는 이유는 다음과 같다. 클라이언트와 서버는 `Connection`을 생성하는데 여러가지 절차가 필요하다. \n그래서 매 요청마다 `Connection`을 생성하는것은 비효율적이고 느렸다. \n\n비효율성을 해결하기 위해 사람들은 이미 만들어진 `Connection`이 있다면 이를 활용하여 요청을 보내고자 하였다.\n\n이렇게 유지되는 `Connection`들을 `동시 Connection`이라고 한다.\n\n> 💡 여담으로 HTTP 프로토콜 `Header`부분의 `Keep-Alive`가 바로 `Connection`을 얼마나 유지할 것인지에 대한 통신 규약이다.\n\n이때, Apache 서버는 C10K문제를 가져왔다.\n![](https://velog.velcdn.com/images/gudonghee2000/post/fa69475a-0326-4652-b1da-616e60164dea/image.JPG)\n\n그림과 같이 `Apache`서버는 요청이 들어올 때 마다, `Process`를 생성했는데 요청이 만단위를 넘어가면서 어느순간 부터 요청에 대한 `Connection`을 생성하지 못한것이다. \n\n이러한 문제를 가져온 원인은 다음과 같았다.\n-  메모리 부족\nApache서버는 `Connection`이 생성될 때마다 `Process`를 생성해야했는데 동시에 유지해야할 `Connection`이 많아지면서 유지해야할 `Process`가 증가했고 메모리 부족을 발생시켰다.\n\n- CPU 과부하\n또한, 실행중인 `Process`가 많아지면서 CPU는 `Process`를 처리 할 때, `컨텍스트 스위칭`을 굉장히 많이해야했다. 그래서 CPU의 부하가 증가했다.\n\n결국, 수많은 동시 커넥션을 처리하기엔 Apache의 요청 처리구조는 부적합했다.\n\n그래서 Apache의 단점을 보완하기 위해 2004년 NGINX가 등장했다.\n\n<br>\n\n## NGINX 자세히 알아보기\n\n우선, NGINX의 요청 처리 방식을 살펴보자.![](https://velog.velcdn.com/images/gudonghee2000/post/93b0890b-c015-46d0-9165-5cf6a7b4d9f6/image.JPG)\n\nNGINX는 `MasterProcess`를 통해 설정 파일을 읽고 `WorkerProcess`와 같은 자식 `Process` 3종류를 생성한다. (`cache loader`, `cache manager`가 있음)\n\n그리고 생성된 `WorkerProcess`는 요청이 들어오면 `Connection`을 형성하고 요청을 처리한다.\n요청에 따라서 매번 `Process`를 생성하던 Apache와 달리, NGINX는 `MasterProcess`에 따라 `WorkerProcess`를 생성하고 고정된 개수의 `WorkerProcess` 들이 요청을 처리한다.\n\n이렇게 고정된 `Process`의 개수로 요청을 처리하기 위해 NGINX는 `Event-Driven` 방식으로 요청을 처리한다.\n\n> 💡 Event-Driven이란?\nNGINX는 형성된 Connection에 아무런 요청이 없으면 새로운 요청에 대한 Connection을 형성하여 요청을 처리한다. \n또는 이미 만들어진 다른 Connection으로부터 요청을 처리한다. \nNginx에서의 Conneciton 형성, Connection 제거, 새로운 요청 처리를 Event라고 부른다.\n또한, Event를 비동기 방식으로 처리하는 것을 Event-Driven이라고 한다.\n\n<br>\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/f1b98399-de8f-4f10-bbba-050ca377c4be/image.JPG)\n\nNGINX는 그림과 같이 큐형태의 저장소에 Event들을 담아 `WorkerProcess`가 순차적으로 작업을 처리한다.\n\n<br>\n\n## NGINX의 Apache의 차이점\nApache는 동기방식으로 하나의 `Connection`이 끝날 때 까지 `Process`를 유지했다. \n\n반면, NGINX는 고정된 `WokerProcess`를 생성하고 `Event`가 발생 할 때마다 요청을 처리하는 비동기 `Event-Driven` 방식을 사용한다는 것이 차이점이다.\n\n보통 `WorkerProcess`는 CPU의 코어 개수만큼 생성하는것이 일반적이라고 한다. **(이유: 컨텍스트 스위칭 회수를 최소하 하기 위함.)**\n\n<br>\n\n## NGINX의 장점\nNGINX는 Apache에 비해 성능 측면에서 두가지 장점이있다.\n\n1. 처리할수 있는 동시 커넥션 개수가 훨씬 많다.\n\n2. 동일한 개수의 커넥션 처리 속도가 더 빠르다.\n\n이러한 장점을 가지고 오는 이유를 마지막으로 정리해보자.\n- 고정된 `Process` 개수 만을 사용하기에 `Process` 생성 비용이 없다.\n- 또한, `Process` 개수가 제한되어 CPU의 부담이 줄어든다. (컨텍스트 스위칭이 적어지기 때문)\n- 비동기 방식이기에 `Process`가 쉬지않고 일을 할 수 있다.\n\n<br>\n\n## NGINX 활용 방법\n도입부에서 이야기햇듯이 NGINX는 정적파일처리, 리버스 프록시, 로드밸런서의 역할로써 WAS의 부담을 줄여주는 역할로 다양하게 활용 할 수 있다.\n\n<br>\n\n## 마치면서\n이상 NGINX의 등장 배경과 작동 방식을 살펴보았다.\n많은 크루들이 우아한테크코스 프로젝트를 진행하면서 사용하는 NGINX에 대한 이해에 도움이 되었으면 좋겠다.\n\n<br>\n\n### 참조\nhttps://livlikwav.github.io/study/NGINX-inside/\nhttps://jizard.tistory.com/306\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. 문제 에서 카테고리 수정 기능에 대한 테스트가 터졌다.  시 다른 회원의 id를 넣으면 이 발생해야 하는데 아무런 예외도 발생하지 않았다. 더 이상한 점은 단독으로 돌렸을 때는 잘 돌아가지만 전체 테스트를 돌리면 터진다는 것이다. 테스트 격리에 문제가 있어 보였다. 디버깅을 해보았다.  MEMBER까지 저…","fields":{"slug":"/test-fixture-constant/"},"frontmatter":{"date":"July 27, 2022","title":"테스트에서 Entity 객체를 상수로 두면 안 되는 이유","tags":["Spring","test fixture","상수"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n## 문제\n\n```java\n// MemberFixtures\n\npublic static final Member MEMBER \n\t\t= new Member(\"example@email.com\", \"/image.png\", \"example\", SocialType.GOOGLE);\npublic static final Member CREATOR \n\t\t= new Member(\"creator@email.com\", \"/image.png\", \"creator\", SocialType.GOOGLE);\n```\n\n```java\n// CategoryServiceTest\n\n@DisplayName(\"자신이 만들지 않은 카테고리를 수정할 경우 예외를 던진다.\")\n@Test\nvoid 자신이_만들지_않은_카테고리를_수정할_경우_예외를_던진다() {\n    // given\n    Member member = memberRepository.save(MEMBER);\n    Member creator = memberRepository.save(CREATOR);\n    CategoryResponse savedCategory = categoryService.save(creator.getId(),\n            new CategoryCreateRequest(CATEGORY_NAME));\n\n    CategoryUpdateRequest categoryUpdateRequest = new CategoryUpdateRequest(MODIFIED_CATEGORY_NAME);\n\n    // when & then\n    assertThatThrownBy(\n            () -> categoryService.update(member.getId(), savedCategory.getId(), categoryUpdateRequest))\n            .isInstanceOf(NoPermissionException.class);\n}\n```\n\n`CategoryServiceTest`에서 카테고리 수정 기능에 대한 테스트가 터졌다. `update` 시 다른 회원의 id를 넣으면 `NoPermissionException`이 발생해야 하는데 아무런 예외도 발생하지 않았다. 더 이상한 점은 단독으로 돌렸을 때는 잘 돌아가지만 전체 테스트를 돌리면 터진다는 것이다. 테스트 격리에 문제가 있어 보였다. 디버깅을 해보았다.\n\n![member 객체](debug1.png)\n\nMEMBER까지 저장했을 때는 분명 값들이 잘 들어가있는 걸 볼 수 있다.\n\n![필드가 변경된 member 객체](debug2.png)\n\n근데 CREATOR를 저장하는 순간!? member의 값이 갑자기 CREATOR의 값으로 전부 변경되었다. 왜 이런 걸까.\n\n## 원인\n\n> 가변객체인 회원의 Entity를 상수로 등록하여 사용하고 있는 것이 문제였다.\n\n아래 그림을 통해 알아보자.\n\n![](img1.png)\n\n테스트 실행 전 MEMBER와 CREATOR는 id를 가지지 않은 상태이다. 영속성 컨텍스트도 비어있다.\n\n전체 테스트를 실행하면 MEMBER와 CREATOR는 상수이므로 온갖 테스트에서 두 개의 상수를 가져다 쓰게 된다. 그 과정에서 MEMBER가 저장되는 상황이 발생한다.\n\n![](img2.png)\n\n그럼 상수 MEMBER의 id 값이 1로 바뀌게 된다. 이후 다른 테스트에서는 CREATOR가 저장된다. 이 때 각각의 테스트는 격리되어 있으므로 영속성 컨텍스트는 빈 상태이다. 따라서 CREATOR의 id도 1로 저장된다.\n\n![](img3.png)\n\n이렇게 상수 MEMBER와 CREATOR의 id가 모두 1로 바뀌었다. 디버깅을 해보니 실제로 상수의 id 값이 모두 1로 바뀐 것을 볼 수 있다.\n\n![](debug3.png)\n\n이 상태에서 처음 봤던 테스트 코드로 돌아가보자.\n\n```java\n// CategoryServiceTest\n\n@DisplayName(\"자신이 만들지 않은 카테고리를 수정할 경우 예외를 던진다.\")\n@Test\nvoid 자신이_만들지_않은_카테고리를_수정할_경우_예외를_던진다() {\n    // given\n    Member member = memberRepository.save(MEMBER);\n    Member creator = memberRepository.save(CREATOR); // 문제 발생 !!!\n    CategoryResponse savedCategory = categoryService.save(creator.getId(),\n            new CategoryCreateRequest(CATEGORY_NAME));\n\n    CategoryUpdateRequest categoryUpdateRequest = new CategoryUpdateRequest(MODIFIED_CATEGORY_NAME);\n\n    // when & then\n    assertThatThrownBy(\n            () -> categoryService.update(member.getId(), savedCategory.getId(), categoryUpdateRequest))\n            .isInstanceOf(NoPermissionException.class);\n}\n```\n\n테스트를 보면 처음으로 MEMBER를 저장한다. 이까지는 아무 문제가 없다. 문제는 두 번째로 CREATOR를 저장할 때 발생한다.\n\n![](img4.png)\n\nCREATOR를 저장하려고 하는데 저장된 MEMBER의 id도 1이고, CREATOR의 id도 1이다. CREATOR를 저장할 때 영속성 컨텍스트에 같은 id를 가진 entity가 있으므로 CREATOR를 새롭게 저장하지 않고, 기존의 id가 1번인 entity를 수정하게 된다.\n\n![](img5.png)\n\n기존의 값이 수정되니 id가 1번인, MEMBER를 저장한 member instance의 값까지 변경되는 것이다.\n\n## 해결\n\nEntity 객체에 대한 fixture가 필요한 경우 객체를 생성하는 메서드를 통해 호출할 때마다 새로운 객체를 생성해서 반환하도록 했다.\n\n```java\npublic static Member 파랑() {\n    return new Member(파랑_이메일, 파랑_프로필, 파랑_이름, SocialType.GOOGLE);\n}\n```\n\n## 결론\n\n상수란 변하지 않고 고정된 값을 담는 변수를 의미한다. 하지만 우리는 필드가 변하는 객체인 Member Entity를 상수로 선언했다. 테스트 격리를 아무리 잘 해도 상수까지 계속 리프레쉬 해주진 않기 때문에 이런 문제가 발생한 것이다.\n\n**변하지 않는 불변 객체만 상수로 지정하자!!**\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. properties 객체로 다루기 Spring에서 이나 에 존재하는 값을 불러오는 방법에는 대표적으로  애노테이션을 사용한 방법과 를 사용한 방법이 존재한다. 두 방식을 직접 적용해 본 뒤 차이와 이점에 대해 알아보려 한다. @Value 사용하기 는 기본적으로 설정 정보를 단일값으로 주입 받기 위해 사…","fields":{"slug":"/properties-to-object/"},"frontmatter":{"date":"July 27, 2022","title":"properties 객체로 다루기","tags":["properties","Value","ConfigurationProperties"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## properties 객체로 다루기\n\nSpring에서 `application.yml`이나 `application.properties`에 존재하는 값을 불러오는 방법에는 대표적으로 `@Value` 애노테이션을 사용한 방법과 `@ConfigurationProperties`를 사용한 방법이 존재한다. 두 방식을 직접 적용해 본 뒤 차이와 이점에 대해 알아보려 한다.\n\n## @Value 사용하기\n\n`@Value`는 기본적으로 설정 정보를 단일값으로 주입 받기 위해 사용된다. 아래는 실제 달록 프로젝트에서 적용한 예시이다.\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final String clientId;\n    private final String clientSecret;\n    private final String grantType;\n    private final String redirectUri;\n    private final String tokenUri;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(@Value(\"${oauth.google.client-id}\") final String clientId,\n                             @Value(\"${oauth.google.client-secret}\") final String clientSecret,\n                             @Value(\"${oauth.google.grant-type}\") final String grantType,\n                             @Value(\"${oauth.google.redirect-uri}\") final String redirectUri,\n                             @Value(\"${oauth.google.token-uri}\") final String tokenUri,\n                             final RestTemplateBuilder restTemplateBuilder, final ObjectMapper objectMapper) {\n        this.clientId = clientId;\n        this.clientSecret = clientSecret;\n        this.grantType = grantType;\n        this.redirectUri = redirectUri;\n        this.tokenUri = tokenUri;\n        this.restTemplate = restTemplateBuilder.build();\n        this.objectMapper = objectMapper;\n    }\n\t\t...\n}\n```\n\n간단하게 적용이 가능하지만 공통으로 묶인 프로퍼티가 많아질 경우 코드가 지저분해진다. 이러한 프로퍼티 값들을 객체로 매핑하여 사용하기 위한 애노테이션으로 `@ConfigurationProperties`가 존재한다.\n\n## @ConfigurationProperties\n\n우리는 때때로 DB 설정을 작성하기 위해 `application.yml`을 통해 관련 정보를 작성하곤 한다. 아래는 간단한 h2 DB를 연결하기 위한 설정을 적은 예시이다.\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:h2:~/dallog;MODE=MYSQL;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE\n    username: sa\n```\n\n이러한 설정들은 어디서 어떻게 활용되고 있을까? 실제 바인딩 되고 있는 객체를 따라가보자.\n\n```java\n@ConfigurationProperties(prefix = \"spring.datasource\")\npublic class DataSourceProperties implements BeanClassLoaderAware, InitializingBean {\n\t\n    private ClassLoader classLoader;\n    private boolean generateUniqueName = true;\n\tprivate String name;\n\tprivate Class<? extends DataSource> type;\n\tprivate String driverClassName;\n\tprivate String url;\n    ...\n}\n```\n\n위 `DataSourceProperties`는 우리가 `application.yml`에 작성한 설정 정보를 기반으로 객체로 추출하고 있다. 이것은 Spring Boot의 자동설정으로 `DataSource`가 빈으로 주입되는 시점에 설정 정보를 활용하여 생성된다. \n\n간단히 디버깅을 진행해보면 Bean이 주입되는 시점에 아래와 같이 `application.yml`에 명시한 값들을 추출한 `DataSourceProperties`를 기반으로 생성하고 있다.\n\n![](debug-1.png)\n\n![](debug-2.png)\n\n정리하면 우리는 Spring Boot를 사용하며 자연스럽게 `@ConfigurationProperties`를 활용하여 만든 객체를 사용하고 있는 것이다.\n\n이제 우리가 작성한 설정 값을 기반으로 객체를 생성해서 활용해보자. 아래는 실제 프로젝트에서 사용하고 있는 `application.yml`의 일부를 가져온 것이다.\n\n```yaml\n...\noauth:\n  google:\n    client-id: ${GOOGLE_CLIENT_ID}\n    client-secret: ${GOOGLE_CLIENT_SECRET}\n    redirect-uri: ${GOOGLE_REDIRECT_URI}\n    oauth-end-point: https://accounts.google.com/o/oauth2/v2/auth\n    response-type: code\n    scopes:\n        - https://www.googleapis.com/auth/userinfo.profile\n        - https://www.googleapis.com/auth/userinfo.email\n    token-uri: ${GOOGLE_TOKEN_URI}\n    grant-type: authorization_code\n...\n```\n\n이것을 객체로 추출하기 위해서는 아래와 같이 작성해야 한다.\n\n```java\n@ConfigurationProperties(\"oauth.google\")\n@ConstructorBinding\npublic class GoogleProperties {\n\n    private final String clientId;\n    private final String clientSecret;\n    private final String redirectUri;\n    private final String oAuthEndPoint;\n    private final String responseType;\n    private final List<String> scopes;\n    private final String tokenUri;\n    private final String grantType;\n\n    public GoogleProperties(final String clientId, final String clientSecret, final String redirectUri,\n                            final String oAuthEndPoint, final String responseType, final List<String> scopes,\n                            final String tokenUri, final String grantType) {\n        this.clientId = clientId;\n        this.clientSecret = clientSecret;\n        this.redirectUri = redirectUri;\n        this.oAuthEndPoint = oAuthEndPoint;\n        this.responseType = responseType;\n        this.scopes = scopes;\n        this.tokenUri = tokenUri;\n        this.grantType = grantType;\n    }\n\n    public String getClientId() {\n        return clientId;\n    }\n\n    public String getClientSecret() {\n        return clientSecret;\n    }\n\n    public String getRedirectUri() {\n        return redirectUri;\n    }\n\n    public String getoAuthEndPoint() {\n        return oAuthEndPoint;\n    }\n\n    public String getResponseType() {\n        return responseType;\n    }\n\n    public List<String> getScopes() {\n        return scopes;\n    }\n\n    public String getTokenUri() {\n        return tokenUri;\n    }\n\n    public String getGrantType() {\n        return grantType;\n    }\n}\n```\n\n- `@ConfigurationProperties`: 프로퍼티에 있는 값을 클래스로 바인딩하기 위해 사용하는 애노테이션이다. `@ConfigurationProperties`는 값을 바인딩하기 위해 기본적으로 `Setter`가 필요하다. 하지만 `Setter`를 열어둘 경우 불변성을 보장할 수 없다. 이때 생성자를 통해 바인딩 하기 위해서는 `@ConstructorBinding`을 활용할 수 있다.\n- `@ConstructorBinding`: 앞서 언급한 것 처럼 생성자를 통해 바인딩하기 위한 목적의 애노테이션이다.\n\n```java\n@Configuration\n@EnableConfigurationProperties(GoogleProperties.class)\npublic class PropertiesConfig {\n}\n```\n\n- `@EnableConfigurationProperties`: 클래스를 지정하여 스캐닝 대상에 포함시킨다.\n\n### 개선하기\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final GoogleProperties googleProperties;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(final GoogleProperties googleProperties, final RestTemplateBuilder restTemplateBuilder,\n                             final ObjectMapper objectMapper) {\n        this.googleProperties = googleProperties;\n        this.restTemplate = restTemplateBuilder.build();\n        this.objectMapper = objectMapper;\n    }\n    ...\n}\n```\n\n이전 보다 적은 수의 필드를 활용하여 설정 정보를 다룰 수 있도록 개선되었다.\n\n### 정리\n\n 우리는 `application.yml` 혹은 `application.properties`에 작성하여 메타 정보를 관리할 수 있다. 클래스 내부에서 관리할 경우 수정하기 위해서는 해당 클래스에 직접 접근해야 한다. 하지만 설정 파일로 분리할 경우 우리는 환경에 따라 유연하게 값을 설정할 수 있다. 또한 `@ConfigurationProperties` 애노테이션을 사용할 경우 클래스로 값을 바인딩하기 때문에 연관된 값을 한 번에 바인딩할 수 있다.\n\n## References.\n\n[달록 repository](https://github.com/woowacourse-teams/2022-dallog)<br>\n[[Spring] @Value와 @ConfigurationProperties의 사용법 및 차이 - (2/2)](https://mangkyu.tistory.com/207)<br>\n[appendix.configuration-metadata.annotation-processor](https://docs.spring.io/spring-boot/docs/2.7.1/reference/html/configuration-metadata.html#appendix.configuration-metadata.annotation-processor)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. Rest Docs Spring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다. Rest Docs와 Swagger 자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n…","fields":{"slug":"/apply-rest-docs/"},"frontmatter":{"date":"July 26, 2022","title":"MockMvc를 사용한 Spring RestDocs","tags":["Spring","Rest API"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n## Rest Docs\nSpring Rest Docs는 테스트 코드 기반으로 자동으로 Rest API 문서를 작성 할 수 있도록 도와주는 프레임 워크이다.\n\n## Rest Docs와 Swagger\n자바 문서 자동화에는 주로 Rest Docs와 Swagger가 사용된다.\n각 자동화 프레임 워크의 장단점을 살펴보자.\n![](./compare.png)\n\nSwagger는 API 문서의 작성을 위해 프로덕션 코드에 추가적인 코드를 작성해야한다. \n그래서 Swagger의 사용은 프로덕션 코드의 가독성을 떨어트린다고 생각한다.\n\n반대로, Spring Rest Docs는 테스트 코드에 의존적이기 때문에 Spring Rest Docs를 사용하는것이 좋다고 생각한다.\n\n## MockMvc vs Rest Assured\nSpring Rest Docs를 사용하여 문서를 작성 하려면 테스트 코드가 필요하다.\n테스트 코드를 작성 할 때, 대표적으로 MockMvc와 Rest Assured를 사용한다.\n\nMockMvc를 사용하면  `@WebMvcTest`로 테스트 할 수 있다.\n그래서 Controller Layer만으로 테스트 하기 때문에 테스트 속도가 빠르다.\n\n반면, RestAssured는 `@SpringBootTest`로 수행해야한다. 그러면 전체 어플리케이션 컨텍스트를 로드하여 빈을 주입하기에 테스트 속도가 느리다.\n하지만, 실제 객체를 통한 테스트가 가능하기 때문에 테스트의 신뢰성이 높다.\n\n통합 테스트, 인수 테스트의 경우 RestAssuerd가 좋을 수 있지만, 문서를 작성하기 위한 테스트에는 MockMvc가 더 적절하다고 생각한다.\n\n**_💡 @WebMvcTest와 @SpringBootTest_**\n@WebMvcTest는 Application Context를 완전하게 Start하지 않고 Present Layer 관련 컴포넌트만 스캔하여 빈 등록한다.\n반면, @SpringBootTest의 경우 모든 빈을 로드하여 등록한다.\n\n\n## AsciiDoc\n\nSpring Boot Rest Docs는 Asciidoc를 문서 번역을 위한 텍스트 프로세서로 사용한다.\n\n## Rest Docs API 문서 생성 매커니즘\n우선, Rest Docs의 문서 생성 매커니즘을 살펴보자.\n\n1. MockMvc로 작성한 테스트 코드를 실행한다.\n\n2. 테스트가 통과하면 아래와 같이 `build/generated-snippets` 하위에 스니펫(문서조각)들이 생성된다. \n![](./first.png)\n\n   _❗❗ gradle은 build/generated-snippets에 스니펫이 생성된다._\n\n3. `build/generated-snippets` 하위에 생성된 스니펫들을 묶어서 HTML 문서를 만들기 위해서는, gradle의 경우 아래와 같이`src/docs/asciidoc` 하위에 스니펫들을 묶은 adoc문서를 만든다.![](./second.png)\n\n4. 스니펫을 이용해서 `src/docs/asciidoc` 하위에 adoc 파일을 생성했다면, `./gradlew build` 명령어를 통해 빌드를 해준다.\n![](./third.png)\n빌드가 완료되면 위와 같이 `resources - static - docs` 하위에 HTML 문서가 생성된다.\n\n5. 어플리케이션을 실행 한 후, `http://localhost:8080/docs/{HTML 파일명}` 을 웹브라우저에 검색하면 생성한 REST API 문서를 확인 할 수 있다. \n\n\t**❗❗ API문서 url은 코드를 통해 변경 가능하다.**\n    \n### ❗유의할 점\nresources - static - docs 하위의 HTML 파일은 실제로는 build.gradle의 설정파일에 따라서 위와같이 build - docs - asciidoc 하위의 HTML 파일을 복사해온 파일이다.\n![](./four.png)\n\n### 아이디어\nREST API 문서를 확인할 때, `http://localhost:8080/docs/{HTML 파일명}` 을 통해서 웹브라우저에 접근하지 않아도 확인하는 방법이 있다.\n![](./five.png)\nAsciiDoc 플러그인을 설치하면 위와같이, 인텔리제이 상에서도 REST API 문서를 실시간으로 확인할수 있다.  (✔설치 추천)\n\n## Rest Docs 사용을 위한 빌드파일 설정\n``` java\nplugins {\n    id 'org.asciidoctor.jvm.convert' version '3.3.2' // 1\n}\n\next {\n    snippetsDir = file('build/generated-snippets') // 2\n}\n\ntest { \n    outputs.dir snippetsDir // 3\n    useJUnitPlatform()\n}\n\nconfigurations {\n    asciidoctorExtensions\n}\n\nasciidoctor { // 4\n    configurations 'asciidoctorExtensions' \n    inputs.dir snippetsDir \n    dependsOn test\n}\n\ndependencies {\n    testImplementation 'org.springframework.restdocs:spring-restdocs-mockmvc' // 5\n    asciidoctorExtensions 'org.springframework.restdocs:spring-restdocs-asciidoctor' // 6\n}\n\ntask copyDocument(type: Copy) { // 7\n    dependsOn asciidoctor\n    \n    from file(\"build/docs/asciidoc\")\n    into file(\"src/main/resources/static/docs\")\n}\n\t\nbootJar { \n    dependsOn copyDocument // 8\n}\n  ```\n\n\n1. gradle7부터 사용하는 플러그인으로 asciidoc 파일 변환, build 디렉토리에 복사하는 플러그인이다.\n\n2. 생성된 스니펫을 저장할 위치를 정의한다. gradle은 `build/generated-snippets`에 스니펫이 생성된다.\n\n3. 테스트 Task의 결과 아웃풋 디렉토리를 `build/generated-snippets`로 지정한다.\n\n4. asciidoctor Task가 사용할 인풋 디렉토리를 `build/generated-snippets`로 지정한다.\n\tdependsOn test로 문서가 작성되기 전에 테스트가 실행되도록 한다.\n    \n5. MockMvc를 테스트에 사용하기 위한 의존성을 추가 해준다.\n\n6. 일반 텍스트를 처리하고 HTML 파일을 생성하는 의존성을 추가 해준다.\n\n7. asciidoctor Task로 생성한 `build/docs/asciidoc`파일을 `src/main/resources/static/docs`로 복사한다.\n\n8. bootJar 실행시 copyDocument를 먼저 실행하도록 한다.\n\n\n--- \n\n✅MockMvc를 사용한 Rest Docs 테스트 작성을 알아보기 전에 우선 MockMvc에 대해 알아보자.\n\n##MockMvc 기본 메서드\n어떠한 것들이 있는지 알아보고 밑에서 자세히 알아보자.\n\n### perform()\n가상의 request를 처리한다.\n\n```java\nmockMvc.perform(get(\"/api/schedules/?year=2022&month=7\"))\n```\n\n### andExpert()\nandExpert()\n\n예상값을 검증한다. \n\n```java\n.andExpect(status().isOk())\n// status 값이 정상인 경우를 기대하고 만든 체이닝 메소드의 일부\n\n.andExpect(content().contentType(\"application/json;charset=utf-8\"))\n//contentType을 검증\n```\n\n### andDo()\n요청에 대한 처리를 맡는다. print() 메소드가 일반적이다.\n\n```java\n.andDo(print())\n```\n\n### andReturn()\n테스트한 결과 객체를 받을 때 사용한다.\n\n```java\nMvcResult result = mockMvc.perform(get(\"/\"))\n.andDo(print())\n.andExpect(status().isOk())\n.andReturn();\n```\n\n## MockMvc 요청 만들기\n요청을 만들 때는 static 메서드인 get, post, put, delete, fileUpload 등을 이용해서 MockHttpServletRequestBuilder 객체를 생성하는 것에서 시작한다.\n\nMockHttpServletRequestBuilder는 ServletRequest를 구성하기에 필요한 다양한 메서드를 제공한다.\n![](./six.png)\n위 메서드들은 메서드 체이닝을 지원하기 때문에, 아래와 같이 요청 데이터를 연결해서 작성하면된다.\n\n\n```java \n@Test\n    void test() throws Exception {\n        MockHttpServletRequestBuilder builder = get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\");\n\n        mockMvc.perform(builder)\n                .andExpect(status().isOk());\n    }\n\n```\n_**❗❗ 유의 할 점**_\nMockMvc.perform() 의 파라미터 값이 MockHttpServletRequestBuilder의 상위 객체이다. \n\n그래서 perform() 파라미터로 아래와 같이 넣어주어도 작동된다.\n```java\n@Test\n    void test() throws Exception {\n        mockMvc.perform(get(\"/api/schedules\")\n                .param(\"year\", \"2022\")\n                .param(\"month\", \"7\")\n                .accept(MediaType.APPLICATION_JSON)\n                .header(\"sessionId\", \"세션아이디입니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 검증\nperform()은 반환 값으로 ResultActions가 반환된다.\nResultActions의 andExpect는 요청 실행 결과를 검증 하려면 ResultMatcher를 넘겨줘서 검증해야한다.\nResultMatcher는 다음의 MockMvcResultMatchers가 가지는 static 메서드를 통해서 얻는다.\n\nMockMvcResultMatchers는 다음의 static 메서드를 통해 다양한 ResultMatcher를 제공한다.\n\n![](./seven.png)\n\n아래의 예시를 살펴보자.\n```java\n\t@Test\n    void test() throws Exception {\n        mockMvc.perform(builder)\n                .andExpect(handler().handlerType(ScheduleController.class))\n                .andExpect(handler().methodName(\"save\"))\n                .andExpect(forwardedUrl(\"index\"))\n                .andExpect(header().stringValues(\"Content-Language\", \"en\"))\n                .andExpect(model().attribute(\"message\", \"저장이 잘되었습니다.\"))\n                .andExpect(status().isOk());\n    }\n```\n\n## MockMvc 실행 결과 처리\n실행 결과를 출력할 떄는 andDo 메서드를 사용한다.\nandDo 메서드 의 인수에는 실행 결과를 처리 할 수 있는 ResultHandler를 지정한다.\nMockMvcResultHandlers는 다양한 ResultHandler를 제공하지만 print()를 주로 사용한다.\n\n\n## MockMvc를 사용한 Rest Docs 생성\n테스트 코드와 함께 MockMvc를 사용한 Rest Docs 생성을 알아보자.\n\n```java\n@WebMvcTest(ScheduleController.class)\n@AutoConfigureRestDocs // 1\nclass ScheduleControllerTest {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Autowired\n    private ObjectMapper objectMapper;\n\n    @MockBean // 2\n    private ScheduleService scheduleService;\n\n    @Test\n    void save() throws Exception {\n        // given\n        ScheduleCreateRequest request = new ScheduleCreateRequest(\"제목\", LocalDateTime.now(), LocalDateTime.now(), \"메모\");\n\n        given(scheduleService.save(request))\n                .willReturn(1L); // 3\n\n        // when & then\n        mockMvc.perform(post(\"/api/schedules\")\n                        .content(objectMapper.writeValueAsString(request))\n                        .contentType(MediaType.APPLICATION_JSON)\n                        .accept(MediaType.APPLICATION_JSON))\n                .andExpect(status().isOk())\n                .andDo(document(\"schedule-save\", // 4\n                        requestFields(\n                                fieldWithPath(\"title\").type(JsonFieldType.STRING).description(\"제목\"),\n                                fieldWithPath(\"startDateTime\").type(JsonFieldType.STRING)\n                                        .description(\"2022-07-04T13:00\"),\n                                fieldWithPath(\"endDateTime\").type(JsonFieldType.STRING).description(\"2022-07-05T07:00\"),\n                                fieldWithPath(\"memo\").type(JsonFieldType.STRING).description(\"메모입니다.\")\n                        )\n                ));\n    }\n}\n```\n\n1. target/generated-snippets dir 생성하고 테스트 코드를 통해 snippets를 추가해주는 애노테이션이다.\n\n2. `ScheduleService`를 mocking을 하기위해서 `@MockBean` 을 선언한다.\n\n3. mocking을 통해 `ScheduleService` 를 통해 받을 응답값을 설정한다.\n\n4. test 수행시 `andDo(document(\"xxx\"))`를 통해서 `./build/generated-snippets` 하위에 문서가 작성된다.\n\n\n---\n## 끝내면서 \n이상 Rest Docs의 매커니즘, 설정 그리고 MockMvc를 활용한 Rest Docs 생성 방법을 살펴보았다.\n프로젝트에 RestAssuered를 사용한 Rest Docs를 적용하면서 테스트 격리에 문제를 경험하였는데,\n테스트 격리에 대해서 추후에 포스팅 해봐야겠다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '리버'가 작성했습니다. 들어가기에 앞서 이전 포스팅에서 SQL 중심적인 개발의 문제점을 살펴보고 객체 중심적인 개발을 하기위한 JAVA의 JPA를 가볍게 언급해보았다. 그렇다면, JPA는 어떠한 매커니즘으로 JAVA에서 작동할까?\n이번 포스팅에서 JPA의 구동방식을 자세히 알아보자.   JPA 구동방식 우선, 아래의 그림을 …","fields":{"slug":"/mechanism-of-jpa/"},"frontmatter":{"date":"July 26, 2022","title":"JPA 작동 메커니즘","tags":["JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[리버](https://github.com/gudonghee2000)'가 작성했습니다.\n\n### 들어가기에 앞서\n이전 포스팅에서 SQL 중심적인 개발의 문제점을 살펴보고 객체 중심적인 개발을 하기위한 JAVA의 JPA를 가볍게 언급해보았다.\n\n그렇다면, JPA는 어떠한 매커니즘으로 JAVA에서 작동할까?\n이번 포스팅에서 JPA의 구동방식을 자세히 알아보자.  \n\n\n### JPA 구동방식\n우선, 아래의 그림을 통해 JPA의 구동방식을 가볍게 살펴보자.\n\n![](./jpa-mechanism.png)\n\n\n먼저, JPA는 Build 파일을 통해서 JPA 인터페이스를 구현할 **구현체 클래스인** `Persistence` 생성한다. \n**(JPA는 인터페이스이며 구현체로는 Hibernate, EclipseLink 등이 있음)**\n\n그리고 생성한 `Persistence`로 `META-INF/Persistence.xml`라는 설정파일의 정보를 읽어서 `EntityManagerFactory`라는 클래스를 생성한다.\n\n`EntityManagerFactory`는 필요할때 마다 `EntityManager` 라는 클래스를 생성한다. \n\n개발자는 `EntityManager`를 통해서 DB에 접근하고 CRUD 작업을 수행한다.\n\n위와 같은 과정을 통해 JPA를 사용 할 수 있다.\n\n그렇다면, `Persistence.xml`, `EntityManagerFactory`, `EntityManager`란 무엇일까?\n아래에서 자세히 살펴보자.\n\n### Persistence.xml란?\n`Persistence.xml`은 JPA가 엑세스하려는 데이터베이스들에 대해 필요한 설정정보 들을 기술해둔 파일이다.\nJPA는 이 파일의 설정정보를 바탕으로 접근할 DB의 정보를 가져 올 수 있다.\n\n`Persistence.xml` 파일에 대해 아래 그림과 함께 자세히 살펴보자.\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<persistence version=\"2.2\"\n \txmlns=\"http://xmlns.jcp.org/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_2.xsd\">\n\t<persistence-unit name=\"hello\">\n \t\t<properties>\n \t\t\t<!-- 필수 속성 -->\n \t\t\t<property name=\"javax.persistence.jdbc.driver\" value=\"org.h2.Driver\"/>\n \t\t\t<property name=\"javax.persistence.jdbc.user\" value=\"sa\"/>\n \t\t\t<property name=\"javax.persistence.jdbc.password\" value=\"\"/>\n\t\t\t<property name=\"javax.persistence.jdbc.url\" value=\"jdbc:h2:tcp://localhost/~/test\"/>\n \t\t\t<property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.H2Dialect\"/>\n\n \t\t\t<!-- 옵션 -->\n \t\t\t<property name=\"hibernate.show_sql\" value=\"true\"/>\n \t\t\t<property name=\"hibernate.format_sql\" value=\"true\"/>\n \t\t\t<property name=\"hibernate.use_sql_comments\" value=\"true\"/>\n \t\t\t<!--<property name=\"hibernate.hbm2ddl.auto\" value=\"create\" />-->\n \t\t</properties>\n\t</persistence-unit>\n</persistence> \n```\n\n첫번째로 `<persistence version=\"2.2\" ...>`태그는 사용할 JPA의 버전과 버전에 대한 Persistence.XML 문서형식 링크를 담고 있다.\n\n`<persistence-unit name=\"hello\">`태그는 하나의 지속성 단위를 의미한다. 서술적으로 표현하면, `hello`라는 이름을 가지는 `EntityManagerFactory`를 하위 설정에 따라 만들것이라는 의미이다. \n\n`<properties>`는 `<persistence-unit>`이 엑세스할 DB의 정보와 추가적인 옵션 정보들을 담는다.\n\n`<property>`는 옵션 정보를 하나씩 설정하는 부분이다.\n위 그림과 같이 `필수 속성` 부분은 엑세스할 DB에 대한 정보를 담는다.\n\n추가로 `옵션` 부분을 하나씩 살펴보자.\n`hibernate.show_sql`는 DB에 날리는 쿼리문을 확인 할것인지에 대한 설정이다.\n`hibernate.format_sql`는 보여지는 쿼리문을 포맷팅하는 설정이다.\n`hibernate.use_sql_comments`는 추가적인 `/* */` 주석을 쿼리문에 보여주는 설정이다.\n\n`hibernate.hbm2ddl.auto` 는 데이터베이스 스키마 자동생성에 대한 설정이다.\n`hibernate.hbm2ddl.auto`가 가지는 옵션은 아래와 같다.\n\n```\ncreate: 기존 테이블 삭제 후 다시 생성 (DROP + CREATE)\ncreate-drop: create와 같으나 종료시점에 테이블 DROP\nupdate: 변경된 부분만 반영 (운영 DB에 사용하면 안됌)\nvalidate: entity와 table이 정상 매핑되었는지만 확인\nnone: 사용하지 않음\n```\n\n#### **주의점❗**\n**create, create-drop, update는 운영 DB에 사용하면 안된다.  \ncreate, create-drop은 운영 DB의 데이터를 전체 삭제 시키기 때문이다.\nupdate는 아직 알아볼필요가있음**\n\n이제 설정파일을 통해 생성하는 `EntityManagerFactory`를 살펴보자.\n\n\n### EntityManagerFactory란?\n`EntityManagerFactory`는 `EntityManager`(Entity를 관리하고 DB 관련작업을 수행 함)을 생성하는 일을한다.\n\n**💡 `Entity` 는 DB 테이블에 대응하는 하나의 객체를 의미한다.**\n\n우리는 `Persistence.xml` 설정정보를 통해서 `EntityManagerFactory`를 생성할수 있다.\n\n아래의 예시코드로 살펴보자.\n``` java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\");\n        // 파라미터로 Persistence.xml에 설정한 persistence-unit의 name 속성값을 넣는다.\n        ...\n    }\n}\n```\n\n전 장에서 설정했던 `Persistence.xml` 파일을 통해 \n우리는 `hello`라는 이름을 가진 `<persistence-unit>`이 어떤 DB에 엑세스 할 것인지에 대한 정보를 설정했었다.\n\n위 코드가 바로 `Persistence.xml`에 작성한 Persistence Unit(지속성 단위)을 바탕으로 `EntityManagerFactory`를 생성하는 코드이다.\n\n`EntityManagerFactory`를 통해서 우리는 DB 관련 작업을 실제로 수행할 `EntityManager`를 생성 할 수 있다.\n\n#### 🔎`EntityManagerFactory`가 가지는 특징\n**1. 엔티티 매니저 팩토리는 하나의 데이터베이스에 하나만 생성한다. (생성 비용이 비싸기 때문)**\n\n**2. 엔티티 매니저 팩토리는 여러 스레드가 동시에 접근해도 안전하다.**\n\n  \n### EntityManager란?\n\n엔티티 매니저는 특정 작업을 위해 데이터베이스에 액세스 하는 역할을 한다.\n또한 엔티티를 데이터베이스에 CRUD 할 수 있다.\n즉, 엔티티와 관련된 모든 일을 처리하기에 이름 그대로 엔티티를 관리하는 관리자다.\n\n``` java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\");\n        // 파라미터로 Persistence.xml에 설정한 persistence-unit의 name 속성값을 넣는다.\n        \n        EntityManager em = emf.createEntityManager();\n        ...\n    }\n}\n```\n\n위와 같이 `EntityManagerFactory`를 통해서 `EntityManager`를 생성 할 수 있다.\n\n#### 🔎`EntityManager`가 가지는 특징\n**1. 엔티티 매니저는 DB connection과 밀접한 관계가 있기 때문에, 스레드 간에 공유하거나 재사용하면 안된다.**\n\n**2. 엔티티 매니저의 CRUD 작업은 트랜잭션 단위로 처리 해야한다.**\n\n#### **주의점❗**\n**EntityManager의 작업은 트랜잭션 단위로 진행되어야한다.\n왜냐하면, CRUD 작업을 수행하다가 중간에 문제가 발생하는 경우 트랜잭션 이전의 상태로 돌아가야 하기 때문이다.**\n\n\n마지막으로 코드를 통해 `EntityManagerFactory` 생성부터 DB에 엔티티의 데이터를 저장하는 과정을 살펴보자.\n\n### JPA의 구동과정\nJAVA 코드상에서 `JPA`를 통해 데이터를 DB에 저장하는 과정은 아래와 같다.\n\n```java\npublic class JpaMain {\n\n    public static void main(String[] args) {\n        EntityManagerFactory emf = \n        \t\t\tPersistence.createEntityManagerFactory(\"hello\"); \n                    // 엔티티 매니저 팩토리 생성\n        EntityManager em = emf.createEntityManager(); // 엔티티 매니저 생성\n        \n        EntityTransaction tx = em.getTransaction(); // 트랜잭션 생성\n        tx.begin(); // 트랜잭션 시작\n\n        try {\n        \tMember member = new Member() // 멤버 엔티티 생성\n            member.setId(100L)\n            member.setName(\"JPA\")\n            \n            em.persist(member) // 멤버 엔티티 데이터를 저장\n        \n            tx.commit(); // 트랜잭션 커밋\n        } catch (Exception e) {\n            tx.rollback(); // 트랜잭션 롤백\n        } finally {\n            em.close(); // 엔티티 매니저 연결 종료\n        }\n        emf.close();\n    }\n}\n```\n1. Persistence.xml 설정파일의 Persistence Unit을 통해 엔티티 매니저 팩토리를 생성한다.\n\n2. 엔티티 매니저 팩토리로 엔티티 매니저를 생성한다. \n\n3. 엔티티 매니저에 대한 트랜잭션을 생성한다.\n\n4. 트랜잭션을 시작한다.\n\n5. DB에 저장할 멤버 엔티티를 생성한다.\n\n6. em.persist(member)를 통해 엔티티 데이터를 저장한다.\n\n7. 트랜잭션을 커밋한다.\n\n8. 엔티티 매니저를 닫는다.\n\n9. 엔티티 매니저 팩토리를 닫는다.\n\n\n위와 같은 과정을 통해 DB에 데이터를 저장 할 수 있다.\n\n`em.persist()` 메서드는 파라미터로 들어온 엔티티에 대한 insert 쿼리문을 JPA가 작성하여 DB에 엔티티의 데이터를 저장하는 메서드이다.\n\n얼핏 보면, `em.persist(member)`를 실행할때 데이터가 바로 DB에 저장 될 것 같다.\n\n하지만 실제로는 `Member` 엔티티가 `EntityManager`의 영속 컨텍스트라는 공간에 저장되고 트랜잭션이 커밋되는 시점에 DB에 `Member` 엔티티에 대한 insert 쿼리문이 날라간다.\n\n왜 바로, DB에 데이터를 저장하지 않고 영속 컨텍스트에 데이터를 저장할까? 다음 포스팅에서 알아보자.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 바로 직전 포스팅으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요? 프론트엔드 CD 다이어그램  프론트엔드의 지속적 배포 과정…","fields":{"slug":"/continuous-deploy-with-jenkins-2-frontend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (2) - 프론트엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 바로 [직전 포스팅](https://dallog.github.io/continuous-deploy-with-jenkins-1-backend/)으로 달록팀 백엔드의 배포 환경과 지속적 배포 환경을 구축한 방법을 소개해드렸었죠. 이번 포스팅에서는 프론트엔드의 배포 환경과 지속적 배포 환경 구성을 소개해드리려고 합니다. 바로 시작할까요?\n\n## 프론트엔드 CD 다이어그램\n\n![프론트엔드 지속적 배포 환경](./fe.png)\n\n프론트엔드의 지속적 배포 과정은 사실 백엔드과 크게 다른점이 없습니다. PR이 생성되고, 병합되고, 이 이벤트가 Webhook을 통해 젠킨스 서버에 전달됩니다.\n\n젠킨스 서버는 Webpack을 이용하여 리액트 프로젝트를 빌드하고, `index.html`과 `bundle.js`를 생성합니다. 달록의 프론트엔드 EC2 인스턴스에는 **NGINX가 도커 컨테이너**로 띄워져 있는데요, 생성된 정적파일은 이 NGINX 디렉토리로 전송됩니다.\n\n## 파이프라인 셋업\n\n### 사전 작업\n\n달록팀 프론트엔드 팀은 리액트를 사용합니다. 또한 달록팀은 모듈 번들러로 **웹팩(Webpack)**을 채택하여 사용하고 있습니다. 또한 웹팩을 사용하여 프로젝트를 빌드하기 위해서는 빌드 서버에 **node.js**가 설치되어있어야 합니다.\n\n이를 위해서 달록팀은 젠킨스에 `NodeJS` 플러그인을 설치했습니다. 플러그인을 설치한 이후 사용할 node.js의 버전을 선택해야합니다. **Jenkins 관리 > Global Tool Configuration > NodeJS > NodeJS Installation** 에 들어가 사용할 node.js 버전을 선택하고 이름을 지정해줍니다.\n\n달록팀은 개발 환경에서 node.js 16.14.0을 사용하므로 16.14.0 버전을 선택하고 이름을 `NodeJS 16.14.0`로 지정하였습니다.\n\n### 파이프라인 스크립트\n\n```groovy\npipeline {\n   agent any\n   stages {\n       stage('Github') {\n           steps {\n               git branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n           }\n       }\n       stage('Build') {\n           steps {\n               dir('frontend') {\n                   nodejs(nodeJSInstallationName: 'NodeJS 16.14.0') {\n                        sh 'npm install && npm run build'\n                    }\n               }\n           }\n       }\n       stage('Deploy') {\n           steps {\n               dir('frontend/dist') {\n                   sshagent(credentials: ['key-dallog']) {\n                        sh 'ls'\n                        sh 'scp ./index.html ubuntu@192.168.XXX.XXX:/home/ubuntu/'\n                        sh 'scp ./bundle.js ubuntu@192.168.XXX.XXX:/home/ubuntu/'\n                        sh 'ssh ubuntu@192.168.XXX.XXX \"sudo mv ./index.html ./html && sudo mv ./bundle.js ./html\"'\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\n#### Build stage\n\n`nodejs` Directive를 사용하여 사용할 NodeJS Installation의 이름을 입력하고, 실행할 명령을 입력합니다. `npm install` 로 필요한 모듈을 설치하고, `npm run build` 명령으로 빌드합니다.\n\n#### Deploy stage\n\n백엔드와 마찬가지로 `sshagent` 를 사용하여 프론트엔드 배포 서버에 SSH로 접속합니다. 빌드된 두개의 파일 `index.html`과 `bundle.js` 을 전송합니다.\n\n이후 전송된 파일을 원격지의 sudo 권한으로 `html` 디렉토리로 이동합니다. 이 `html` 디렉토리는 이후 설명하겠지만, NGINX 컨테이너 내부의 디렉토리와 마운팅된 디렉토리입니다. 배포 서버의 Docker가 sudo 권한으로 실행되고 있으므로, 파일을 해당 디렉토리로 이동시킬 때에도 sudo 권한이 필요합니다.\n\n## 도커 및 NGINX 설정\n\n### docker-compose\n\n맨 위의 다이어그램과 같이 프론트엔드 배포 서버는 NGINX가 도커로 띄워져있습니다. 이를 위한 **docker-compose.yml** 내용은 아래와 같습니다.\n\n```yaml\nversion: \"3\"\nservices:\n  dallog-front:\n    image: nginx\n    volumes:\n      - ./html:/usr/share/nginx/html\n    ports:\n      - 80:80\n```\n\n`/usr/share/nginx/html` 디렉토리는 NGINX가 서빙할 정적 파일이 위치될 디렉토리입니다. 이를 컨테이너 외부의 `./html` 디렉토리와 볼륨 마운팅 설정을 해두었습니다.\n\n### 클라이언트 사이드 라우팅 대응\n\n달록은 리액트 라우터를 사용하여 클라이언트 사이드 라우팅을 구현하였습니다. 따라서 이를 위해서 NGINX에 별도 설정이 필요합니다. `index.html` 로 접속하면 html 파일을 잘 찾을 수 있지만, 그 외의 라우트로 접속한다면 html 파일 자체를 찾지 못해 404 에러를 반환하기 때문입니다.\n\n도커 내부의 `/etc/nginx/conf.d/default.conf` 파일을 열어 아래와 같이 설정을 수정합니다.\n\n```\n...\n\nlocation / {\n\t    root   /usr/share/nginx/html;\n\t    index  index.html index.htm;\n\t    try_files $uri $uri/ /index.html;\n}\n\n...\n```\n\n알 수 없는 라우트로 접속했을 때 자동으로 `index.html` 로 Fallback 되도록 `try_files` 설정을 추가하였습니다. 파일을 수정하고 도커 컨테이너를 재시작해주면 설정이 완료됩니다.\n\n## 트러블 슈팅\n\n### 빌드 시 메모리 부족 문제\n\n리액트 프로젝트를 빌드할 때 아래와 같은 메모리 이슈를 마주하게 되었습니다.\n\n```\n<--- Last few GCs --->\n\n[21854:0x147b3960]    21931 ms: Scavenge 456.7 (469.5) -> 456.4 (470.7) MB, 3.2 / 0.0 ms  (average mu = 0.381, current mu = 0.228) allocation failure\n[21854:0x147b3960]    21939 ms: Scavenge 457.4 (470.7) -> 457.2 (475.2) MB, 4.5 / 0.0 ms  (average mu = 0.381, current mu = 0.228) allocation failure\n\n\n<--- JS stacktrace --->\n\nFATAL ERROR: Reached heap limit Allocation failed - JavaScript heap out of memory\n 1: 0xaf9c78 node::Abort() [webpack]\n 2: 0xa21a88 node::FatalError(char const*, char const*) [webpack]\n 3: 0xccdec8 v8::Utils::ReportOOMFailure(v8::internal::Isolate*, char const*, bool) [webpack]\n```\n\n**Jenkins 관리 > 시스템 설정 > Global Properties** 에서 `NODE_OPTIONS` 환경변수를 `-max-old-space-size=3072` 로 설정하여 메모리 이슈를 해결하였습니다.\n\n## 마치며\n\n현재 생성된 PR에 대한 코드를 검사하고 정상적으로 빌드되지 않은 PR의 병합을 막기 위해 Github Actions를 사용하고 있습니다. 가능하다면 조금 더 학습하여 이후 이런 작업까지 젠킨스에서 처리하도록 할 계획이 있습니다.\n\n또한 현재 백엔드와 프론트엔드 모두 `develop` 이라는 하나의 브랜치에서 작업을 하다보니 백엔드, 프론트엔드 어느 한쪽의 코드만 병합되어도 백엔드, 프론트엔드 두개의 빌드 프로세스가 실행되는 이슈가 존재합니다. 이후에는 빌드 트리거를 조금 더 세분화하여 Github의 라벨을 기반으로 트리거 되도록 개선할 계획을 가지고 있습니다.\n\n여기까지 달록팀의 젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기를 읽어주셔서 감사드립니다 🙂\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다. 달록팀 지속적 배포 환경 일단 현재 구성된 달록팀의 …","fields":{"slug":"/continuous-deploy-with-jenkins-1-backend/"},"frontmatter":{"date":"July 24, 2022","title":"젠킨스를 사용한 달록팀의 지속적 배포 환경 구축기 (1) - 백엔드편","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 달록팀 후디입니다. 지난번 포스팅에서 달록팀이 도커를 활용하여 EC2 인스턴스에 도커를 설치한 과정을 이야기 드렸었죠. 이번 포스팅에서는 젠킨스를 활용해서 백엔드와 프론트엔드에 지속적 배포 환경을 구성한 과정에 대해 이야기 드리려고 합니다.\n\n## 달록팀 지속적 배포 환경\n\n일단 현재 구성된 달록팀의 지속적 배포 환경을 간단한 다이어그램으로 살펴보겠습니다.\n\n![백엔드 지속적 배포 환경](./be.png)\n\n우선, 달록의 백엔드 개발자가 열심히 기능을 개발하여 Github에 PR을 생성합니다. 이때 PR 코드가 정상적으로 빌드되고, 모든 테스트를 통과하는지 **Github Actions**를 사용하여 우선적으로 검사합니다. 이때, PR 브랜치의 코드가 문제가 있다면 develop 브랜치로 병합이 불가능합니다.\n\n> 달록팀의 브랜치 전략에 대해서는 달록팀 매트가 작성한 '[달록팀의 git 브랜치 전략을 소개합니다.](https://dallog.github.io/git-branch-strategy/)' 포스팅을 참고해주세요!\n\n정상적으로 빌드가 되는 코드는 이후 달록 개발자들끼리 코드리뷰가 진행되고, develop 브랜치에 병합이 됩니다. 이때, Github는 달록팀이 구축한 젠킨스 서버에 **Webhook**을 통해 병합 사실을 알립니다.\n\nWebhook이란, 특정한 애플리케이션이 다른 애플리케이션으로 **이벤트 발생 정보를 실시간으로 제공하기 위한 방법**입니다. 젠킨스는 외부에 Webhook URL을 열어두고, Github으로부터 이 Webhook URL로 요청을 받아 이벤트가 발생한 즉시 그 사실을 알 수 있습니다.\n\nWebhook을 통해 신호를 받은 젠킨스는 미리 지정된 젠킨스 파이프라인 스크립트를 실행하여 스프링부트 어플리케이션을 빌드하여 `jar` 파일을 생성합니다.\n\n생성한 `jar` 파일은 스프링부트 어플리케이션이 실행되고 있는 EC2 인스턴스로 전송됩니다. 그리고 스프링부트 인스턴스에서 `jar` 파일이 실행되어 배포가 완료됩니다.\n\n## 프리스타일과 파이프라인\n\n젠킨스에서 잡(Job)을 생성하는 방식은 크게 **프리스타일과 파이프라인** 두가지로 나뉩니다.\n\n### 프리스타일\n\n프리스타일은 **GUI기반**으로 젠킨스 잡을 구성할 수 있습니다. 따라서 간단한 작업에 적합하고, **복잡한 작업에는 다소 적절치 않습니다.** 또한 GUI 기반이므로 설정 과정이 다소 난잡하게 느껴질 수 있습니다.\n\n### 파이프라인\n\n반면 젠킨스 파이프라인은 일련의 배포 과정을 **코드로 작성**할 수 있습니다. 이 코드는 **Jenkinsfile** 라고 불리는 파일로 관리할 수 있어, Git 등을 통한 **버전관리**도 가능합니다. 또한 파이프라인을 사용하면 스테이지(Stage)라는 단위로 각 작업에 소요되는 시간, 실패여부를 **시각화**하여 확인할 수 있습니다.\n\n달록은 프리스타일 대신 젠킨스 파이프라인을 이용하여 잡(Job)을 생성했습니다. 개발자 입장에서는 아무래도 GUI보다는 코드 기반으로 작업 과정을 작성하는 것이 편하게 느껴집니다. 또한 추후 배포 프로세스 자체를 Github 저장소에서 관리할 수 있다는 것이 큰 장점으로 다가왔습니다.\n\n#### Scripted vs Declarative\n\n파이프라인도 문법에 따라 크게 **Scripted Pipeline**과 **Declarative Pipeline** 두가지로 나뉩니다.\n\nScripted는 Groovy라는 언어로 작성되며, 변수 선언등이 지원되어 프로그래밍을 할 수 있다는 특징이 있다. 반면, Declarative는 Scripted에 비해 간단하며, Groovy를 알지 않아도 사용할 수 있다는 장점이 존재합니다. **Scripted 문법은 Declarative에 비해 유연성이나 확장성이 높지만, 복잡도와 유지보수 난이도가 더 높습니다**.\n\n최근 CI/CD 기조는 Declarative 스타일로 많이 이동되고 있다고 합니다 ([참고](https://www.theserverside.com/answer/Declarative-vs-scripted-pipelines-Whats-the-difference)). 당장 비교적 최근에 출시된 Github Actions 도 YAML 기반의 Declarative 스타일만 사용할 수 있습니다. 따라서 달록팀은 Declarative 문법을 사용하기로 결정했습니다.\n\n## 잡(Job) 생성 및 세팅\n\n### 파이프라인으로 생성\n\n![](./pipeline.png)\n\n젠킨스 메뉴에서 '새로운 Item'을 클릭하고, 'Pipeline'을 선택하여 새로운 잡을 생성합니다.\n\n### Github URL 설정\n\n**General > GitHub project > Project url** 에 저희 깃허브 저장소 주소인 **https://github.com/woowacourse-teams/2022-dallog** 를 입력합니다.\n\n### 오래된 빌드 삭제\n\n빌드 이력을 오래 가지고 있어봤자 큰 의미가 없을 것 같기도 하고, 아무래도 **t4g.micro** 에서 돌리다보니 용량이 넉넉치 않기도 합니다. 따라서 **General > 오래된 빌드 삭제** 옵션을 클릭하여 활성화하고, 보관할 최대 갯수를 3으로 지정했습니다.\n\n> 나중에 빌드 이력 보관 개수가 너무 적다고 판단되면 조금 더 늘릴 생각입니다 🙂\n\n### 빌드 트리거 설정\n\n**Build Triggers > GitHub hook trigger for GITScm polling** 을 체크해줍니다. Github의 Webhook을 통해 빌드가 트리거되는 옵션입니다.\n\n> 이 기능은 **GitHub plugin**에서 제공하는 기능입니다. 최초 젠킨스를 설치했을 때 Install suggested plugins 를 선택하지 않으면 이 플러그인이 설치되어있지 않을수도 있습니다.\n\n#### Github 저장소에 Webhook 등록하기\n\nWebhook을 사용하려면 Github 저장소의 Settings > Webhooks 에서 Webhook URL을 등록해주어야 합니다.\n\n```\nhttp://{서비스 IP 혹은 도메인 주소}/github-webhook/\n```\n\n위와 같이 URL을 등록해줍니다. 이때 URL의 마지막에 `/`가 들어가지 않으면 오류가 발생하니 꼭 추가합니다.\n\n## 파이프라인 작성\n\n이제 빌드가 트리거 되었을 때 실행될 파이프라인 스크립트를 작성할 차례입니다.\n\n```groovy\npipeline {\n   agent any\n   stages {\n       stage('Github') {\n           steps {\n               git branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n           }\n       }\n       stage('Build') {\n           steps {\n               dir('backend') {\n                   sh \"./gradlew bootJar\"\n               }\n           }\n       }\n       stage('Deploy') {\n           steps {\n               dir('backend/build/libs') {\n                   sshagent(credentials: ['key-dallog']) {\n                        sh 'scp backend-0.0.1-SNAPSHOT.jar ubuntu@192.168.XXX.XXX:/home/ubuntu'\n                        sh 'ssh ubuntu@192.168.XXX.XXX \"sh run.sh\" &'\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\n### Github stage\n\nGithub stage에서는 사용할 깃허브의 저장소 주소와 브랜치를 설정합니다. 젠킨스는 이 스테이지에 명시된 저장소와 브랜치를 기준으로 코드를 가져옵니다.\n\n### Build stage\n\n`gradlew` 파일을 사용하여 빌드합니다. 이때, `dir` 지시어(Directive)를 사용하여 명령을 수행할 디렉토리를 지정할 수 있습니다.\n\n### Deploy stage\n\n> 이 작업을 위해서 사전에 Jenkins 관리 > Manage Credentials 에서 'SSH Username with private key' 로 AWS에서 발급 받은 PEM 키를 먼저 등록해야합니다.\n\n달록팀은 SSH Agent 플러그인을 사용하여 배포 서버에 원격으로 접속합니다. SSH Agent 플러그인을 사용하여 파이프라인에서 젠킨스에 등록해둔 SSH 자격증명을 쉽게 사용할 수 있습니다. 플러그인을 설치하면 `sshagent` 라는 Directive를 사용할 수 있는데, 인자로 사전에 젠킨스 Credential로 등록한 PEM키의 이름을 넣어줍니다. 이렇게 만들어진 `sshagent` Directive Block 내부에서 `sh` 를 통해 `ssh` 관련 명령을 수행할 수 있습니다.\n\n#### jar 파일 전송\n\nSCP(Secure Copy)는 SSH 통신 기반으로 원격지에 파일이나 디렉토리를 전송할 수 있는 프로토콜입니다. 리눅스에서는 `scp` 명령을 통해 SCP 프로토콜을 사용할 수 있습니다. 달록팀은 이 `scp` 명령을 통해 스프링부트 애플리케이션이 실행되고 있는 EC2 인스턴스로 빌드된 `jar` 파일을 전송합니다.\n\n> 여기서 IP주소가 프라이빗 IP로 적혀있습니다. 우테코에서 제공되는 EC2 인스턴스의 보안그룹 설정상 잠실 및 선릉 캠퍼스의 IP로만 SSH(22번 포트) 인바운드가 허용되어 있습니다. 따라서 퍼블릭 IP 대신 프라이빗 IP로 지정해주었습니다. 프라이빗 IP를 사용해도 두 EC2 인스턴스가 같은 VPC에 존재하므로 정상적으로 접근이 가능합니다.\n\n#### 원격지의 쉘 스크립트 실행\n\n스프링부트 인스턴스에는 사전에 `run.sh` 라는 이름의 쉘 스크립트가 작성되어 있습니다. 이 쉘 스크립트는 현재 실행중인 스프링부트 애플리케이션의 프로세스를 제거하고, 환경변수를 설정하고, 같은 디렉토리에 있는 `jar` 파일을 실행합니다. 스크립트는 아래와 같습니다.\n\n```shell\n#! /bin/bash\n\nPROJECT_NAME=backend\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n    echo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n    echo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n    kill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXXXXXXXXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXXXXXXXXXXX\"\nexport JWT_EXPIRE_LENGTH=86400000\nexport JWT_SECRET_KEY=\"XXXXXXXXXXXXXX\"\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_NAME=$(ls | grep .jar | head -n 1)\nsudo -E nohup java -jar /home/ubuntu/$JAR_NAME &\n```\n\n지난 포스팅인 **[쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화](/deploy-automation-with-shell-script)** 에서 소개드린 쉘 스크립트에서 소스코드를 Pull 해오고 빌드하는 과정만 사라진 스크립트입니다.\n\n이 원격지에 있는 스크립트를 `ssh` 명령을 통해 실행하게되면 배포과정이 완료됩니다.\n\n## 트러블슈팅\n\n### Webhook 트리거 이슈\n\n처음 젠킨스를 설정할 때 아무리 Webhook 설정을 건드려봐도 빌드가 트리거되지 않는 이슈가 발생했었습니다.\n\n먼저 위 파이프라인 스크립트의 Github 스테이지에서 사용된 `git` Directive를 명시적으로 사용해줘야합니다.\n\n```\ngit branch: 'develop', url: 'https://github.com/woowacourse-teams/2022-dallog.git'\n```\n\n또한 최초로 '지금 빌드' 버튼을 클릭해야지 그 이후 Webhook 요청을 수신할 수 있게됩니다.\n\n### SSH 자격 증명 이슈\n\n```\nHost key verification failed.\nlost connection\n```\n\n젠킨스 서버에서 다른 인스턴스에 최초로 SSH 연결을 시도할 경우 젠킨스에서 위와 같은 에러가 발생할 수 있습니다. 다들 SSH로 최초 원격접속 시 아래와 같은 메시지를 본 적이 있을 것 입니다.\n\n```\nroot@XXXXXXXX:/home# ssh -i key.pem ubuntu@192.168.XXX.XXX\nThe authenticity of host '192.168.XXX.XXX (192.168.XXX.XXX)' can't be established.\nECDSA key fingerprint is SHA256:XXXXXXXXXXXXXXXX\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n```\n\n터미널에서 직접 접속할 때에는 yes 를 입력하면, 곧바로 접속될텐데요. 젠킨스 파이프라인에서는 불가능합니다. 따라서 미리 젠킨스 측의 `known_hosts` 에 원격지의 공개키를 등록해야합니다.\n\n`keyscan` 명령을 통해서 호스트의 공개키를 수집할 수 있습니다. 이 명령을 통해서 `~/.ssh/known_hosts` 에 접속할 호스트의 공개키를 추가해야합니다.\n\n## 마치며\n\n이번 포스팅으로 달록팀이 젠킨스를 활용하여 스프링부트 애플리케이션의 지속적 배포 환경을 구축한 방법을 소개드렸습니다. 이어지는 다음 포스팅에서는 달록팀의 프론트엔드 배포 환경과 지속적 배포 환경 구축 방법을 소개드리려고 합니다. 읽어주셔서 감사드립니다 😄\n"},{"excerpt":"이 글은 우테코 달록팀 크루 '매트'가 작성했습니다. 외부와 의존성 분리하기 도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다.  인터페이스 활용하기 우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수…","fields":{"slug":"/separated-interface/"},"frontmatter":{"date":"July 24, 2022","title":"외부와 의존성 분리하기","tags":["매트","BE","분리된 인터페이스","의존성 분리"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 '[매트](https://github.com/hyeonic)'가 작성했습니다.\n\n## 외부와 의존성 분리하기\n\n도메인 로직은 우리가 지켜야할 매우 소중한 비즈니스 로직들이 담겨있다. 이러한 도메인 로직들은 변경이 최소화되어야 한다. 그렇기 때문에 외부와의 의존성을 최소화 해야 한다. \n\n### 인터페이스 활용하기\n\n우선 우리가 지금까지 학습한 것 중 객체 간의 의존성을 약하게 만들어 줄 수 있는 수단으로 인터페이스를 활용할 수 있다. 간단한 예시로 `JpaRepository`를 살펴보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n\n    Optional<Member> findByEmail(final String email);\n\n    boolean existsByEmail(final String email);\n}\n```\n\n이러한 인터페이스 덕분에 우리는 실제 DB에 접근하는 내부 구현에 의존하지 않고 데이터를 조작할 수 있다. 핵심은 `실제 DB에 접근하는 행위`이다.\n\n아래는 `Spring Data`가 만든 `JpaRepository의 구현체` `SimpleJpaRepository`의 일부를 가져온 것이다.\n\n```java\n@Repository\n@Transactional(readOnly = true)\npublic class SimpleJpaRepository<T, ID> implements JpaRepositoryImplementation<T, ID> {\n\n\tprivate static final String ID_MUST_NOT_BE_NULL = \"The given id must not be null!\";\n\n\tprivate final JpaEntityInformation<T, ?> entityInformation;\n\tprivate final EntityManager em;\n\tprivate final PersistenceProvider provider;\n\n\tprivate @Nullable CrudMethodMetadata metadata;\n\tprivate EscapeCharacter escapeCharacter = EscapeCharacter.DEFAULT;\n\n\tpublic SimpleJpaRepository(JpaEntityInformation<T, ?> entityInformation, EntityManager entityManager) {\n\n\t\tAssert.notNull(entityInformation, \"JpaEntityInformation must not be null!\");\n\t\tAssert.notNull(entityManager, \"EntityManager must not be null!\");\n\n\t\tthis.entityInformation = entityInformation;\n\t\tthis.em = entityManager;\n\t\tthis.provider = PersistenceProvider.fromEntityManager(entityManager);\n\t}\n  ...\n}\n```\n\n해당 구현체는 `entityManger`를 통해 객체를 영속 시키는 행위를 진행하고 있기 때문에 `영속 계층`에 가깝다고 판단했다. 즉 도메인의 입장에서 `MemberRepository`를 바라볼 때 단순히 `JpaRepository`를 상속한 인터페이스를 가지고 있기 때문에 `영속 계층`에 대한 직접적인 의존성은 없다고 봐도 무방하다. 정리하면 우리는 인터페이스를 통해 실제 구현체에 의존하지 않고 로직을 수행할 수 있게 된다. \n\n### 관점 변경하기\n\n이러한 사례를 외부 서버와 통신을 담당하는 우리가 직접 만든 인터페이스인 `OAuthClient`에 대입해본다. `OAuthClient`의 가장 큰 역할은 n의 소셜에서 `OAuth 2.0`을 활용한 인증의 행위를 정의한 인터페이스이다. google, github 등 각자에 맞는 요청을 처리하기 위해 `OAuthClient`를 구현한 뒤 로직을 처리할 수 있다. 아래는 실제 google의 인가 코드를 기반으로 토큰 정보에서 회원 정보를 조회하는 로직을 담고 있다.\n\n```java\npublic interface OAuthClient {\n\n    OAuthMember getOAuthMember(final String code);\n}\n```\n\n```java\n@Component\npublic class GoogleOAuthClient implements OAuthClient {\n\n    private static final String JWT_DELIMITER = \"\\\\.\";\n\n    private final String googleRedirectUri;\n    private final String googleClientId;\n    private final String googleClientSecret;\n    private final String googleTokenUri;\n    private final RestTemplate restTemplate;\n    private final ObjectMapper objectMapper;\n\n    public GoogleOAuthClient(@Value(\"${oauth.google.redirect_uri}\") final String googleRedirectUri,\n                             @Value(\"${oauth.google.client_id}\") final String googleClientId,\n                             @Value(\"${oauth.google.client_secret}\") final String googleClientSecret,\n                             @Value(\"${oauth.google.token_uri}\") final String googleTokenUri,\n                             final RestTemplate restTemplate, final ObjectMapper objectMapper) {\n        this.googleRedirectUri = googleRedirectUri;\n        this.googleClientId = googleClientId;\n        this.googleClientSecret = googleClientSecret;\n        this.googleTokenUri = googleTokenUri;\n        this.restTemplate = restTemplate;\n        this.objectMapper = objectMapper;\n    }\n\n    @Override\n    public OAuthMember getOAuthMember(final String code) {\n        GoogleTokenResponse googleTokenResponse = requestGoogleToken(code);\n        String payload = getPayloadFrom(googleTokenResponse.getIdToken());\n        String decodedPayload = decodeJwtPayload(payload);\n\n        try {\n            return generateOAuthMemberBy(decodedPayload);\n        } catch (JsonProcessingException e) {\n            throw new IllegalArgumentException();\n        }\n    }\n\n    private GoogleTokenResponse requestGoogleToken(final String code) {\n        HttpHeaders headers = new HttpHeaders();\n        headers.setContentType(MediaType.APPLICATION_FORM_URLENCODED);\n        MultiValueMap<String, String> params = generateRequestParams(code);\n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(params, headers);\n        return restTemplate.postForEntity(googleTokenUri, request, GoogleTokenResponse.class).getBody();\n    }\n\n    private MultiValueMap<String, String> generateRequestParams(final String code) {\n        MultiValueMap<String, String> params = new LinkedMultiValueMap<>();\n        params.add(\"client_id\", googleClientId);\n        params.add(\"client_secret\", googleClientSecret);\n        params.add(\"code\", code);\n        params.add(\"grant_type\", \"authorization_code\");\n        params.add(\"redirect_uri\", googleRedirectUri);\n        return params;\n    }\n\n    private String getPayloadFrom(final String jwt) {\n        return jwt.split(JWT_DELIMITER)[1];\n    }\n\n    private String decodeJwtPayload(final String payload) {\n        return new String(Base64.getUrlDecoder().decode(payload), StandardCharsets.UTF_8);\n    }\n\n    private OAuthMember generateOAuthMemberBy(final String decodedIdToken) throws JsonProcessingException {\n        Map<String, String> userInfo = objectMapper.readValue(decodedIdToken, HashMap.class);\n        String email = userInfo.get(\"email\");\n        String displayName = userInfo.get(\"name\");\n        String profileImageUrl = userInfo.get(\"picture\");\n\n        return new OAuthMember(email, displayName, profileImageUrl);\n    }\n}\n```\n\n보통의 생각은 인터페이스인 `OAuthClient`와 구현체인 `GoogleOAuthClient`를 같은 패키지에 두려고 할 것이다. `GoogleOAuthClient`는 외부 의존성을 강하게 가지고 있기 때문에 `domain` 패키지와 별도로 관리하기 위한 `infrastructure` 패키지가 적합할 것이다. 결국 인터페이스인 `OAuthClient` 또한 `infrastructure`에 위치하게 될 것이다. 우리는 이러한 생각에서 벗어나 새로운 관점에서 살펴봐야 한다.\n\n앞서 언급한 의존성에 대해 생각해보자. 위 `OAuthClient`를 사용하는 주체는 누구일까? 우리는 이러한 주체를 `domain` 내에 인증을 담당하는 `auth` 패키지 내부의 `Authservice`로 결정 했다. 아래는 실제 `OAuthClient`를 사용하고 있는 주체인 `AuthService`이다.\n\n```java\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\n    private final OAuthEndpoint oAuthEndpoint;\n    private final OAuthClient oAuthClient;\n    private final MemberService memberService;\n    private final JwtTokenProvider jwtTokenProvider;\n\n    public AuthService(final OAuthEndpoint oAuthEndpoint, final OAuthClient oAuthClient,\n                       final MemberService memberService, final JwtTokenProvider jwtTokenProvider) {\n        this.oAuthEndpoint = oAuthEndpoint;\n        this.oAuthClient = oAuthClient;\n        this.memberService = memberService;\n        this.jwtTokenProvider = jwtTokenProvider;\n    }\n\n    public String generateGoogleLink() {\n        return oAuthEndpoint.generate();\n    }\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        String email = oAuthMember.getEmail();\n\n        if (!memberService.existsByEmail(email)) {\n            memberService.save(generateMemberBy(oAuthMember));\n        }\n\n        Member foundMember = memberService.findByEmail(email);\n        String accessToken = jwtTokenProvider.createToken(String.valueOf(foundMember.getId()));\n\n        return new TokenResponse(accessToken);\n    }\n\n    private Member generateMemberBy(final OAuthMember oAuthMember) {\n        return new Member(oAuthMember.getEmail(), oAuthMember.getProfileImageUrl(), oAuthMember.getDisplayName(), SocialType.GOOGLE);\n    }\n}\n```\n\n지금 까지 설명한 구조의 패키지 구조는 아래와 같다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   └── application\n    │   │               │       └── AuthService.java\n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │   │   └── client\n    │   │               │   │       ├── OAuthClient.java\n    │   │               │   │       └── GoogleOAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java     \n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n결국 이러한 구조는 아래와 같이 `domain` 패키지에서 `infrastructure`에 의존하게 된다.\n  \n```java\n...\nimport com.allog.dallog.infrastructure.dto.OAuthMember; // 의존성 발생!\nimport com.allog.dallog.infrastructure.oauth.client.OAuthClient; // 의존성 발생!\n...\n\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n### Separated Interface Pattern\n\n`분리된 인터페이스`를 활용하자. 즉 `인터페이스`와 `구현체`를 각각의 패키지로 분리한다. 분리된 인터페이스를 사용하여 `domain` 패키지에서 인터페이스를 정의하고 `infrastructure` 패키지에 구현체를 둔다. 이렇게 구성하면 인터페이스에 대한 종속성을 가진 주체가 구현체에 대해 인식하지 못하게 만들 수 있다.\n\n아래와 같은 구조로 인터페이스와 구현체를 분리했다고 가정한다.\n\n```\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    │   │               │   │   ├── AuthService.java\n    │   │               │   │   └── OAuthClient.java\n    │   │               │   └── dto\n    │   │               │       └── OAuthMember.java         \n    │   │               ...\n    │   │               ├── infrastructure\n    │   │               │   ├── oauth\n    │   │               │       └── client\n    │   │               │           └── GoogleOAuthClient.java\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n자연스럽게 `domain` 내에 있던 `infrastructure` 패키지에 대한 의존성도 제거된다. 즉 외부 서버와의 통신을 위한 의존성이 완전히 분리된 것을 확인할 수 있다.\n\n```java\n...\nimport com.allog.dallog.auth.dto.OAuthMember; // auth 패키지 내부를 의존\n...\n@Transactional(readOnly = true)\n@Service\npublic class AuthService {\n\t...\n    private final OAuthClient oAuthClient;\n    ...\n\n    @Transactional\n    public TokenResponse generateTokenWithCode(final String code) {\n        OAuthMember oAuthMember = oAuthClient.getOAuthMember(code);\n        ...\n    }\n    ...\n}\n```\n\n## References.\n\n[Separated Interface](https://www.martinfowler.com/eaaCatalog/separatedInterface.htmlhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다. 지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를…","fields":{"slug":"/install-jenkins-with-docker-on-ec2/"},"frontmatter":{"date":"July 21, 2022","title":"EC2 환경에서 도커를 활용한 젠킨스 설치하기","tags":["DevOps","Jenkins"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n안녕하세요, 우테코 달록팀 후디입니다. 이번 스프린트에서는 저는 배포와 CI/CD와 같이 인프라와 관련된 태스크에 집중하고 있습니다.\n\n지난번 포스팅으로 달록팀이 쉘 스크립트를 통해 배포 자동화를 구축한 이야기를 했었죠. 하지만 새로운 기능이 병합될 때 마다 SSH로 EC2 인스턴스에 접속하여 쉘 스크립트를 **매번 실행해야한다는 단점**이 존재했습니다. 따라서 저희 팀은 메인 브랜치에 기능이 새로 병합 될 때마다 자동으로 감지하고, 스프링 어플리케이션을 `jar` 파일로 빌드하여 배포하는 환경이 필요하다고 느꼈습니다. 따라서 **CI/CD 도구를 도입**하기로 결정했습니다.\n\n이번 포스팅에서는 달록이 EC2 환경에서 도커를 사용하여 젠킨스를 설치한 방법에 대해서 정리합니다.\n\n## 젠킨스 도입 배경\n\n![유명한 CI/CD 도구들의 구글 트렌드 결과](./google-trends.png)\n\n달록은 CI/CD 도구로 **Jenkins**를 선정하였습니다. 위 사진은 시중에 배포되어있는 여러 CI/CD 도구의 구글 트렌드 분석 결과입니다. 파란색이 Jenkins 인데요, **압도적으로 높은 관심도**를 유지하고 있습니다.\n\n아무래도 달록팀 모두가 CI/CD에 익숙하지 않아 가장 사람들이 많이 사용하고, 그에 따라 **생태계가 넓고 레퍼런스가 많은 도구**를 선정하는 것이 좋다고 판단하였습니다. **레퍼런스가 많아** 초기 학습 비용이 적게 들고, 이슈가 발생했을때에도 **트러블슈팅이 비교적 쉽다**고 생각했습니다.\n\n## 도커\n\n달록팀은 EC2 인스턴스에 **도커를 사용하여** 젠킨스 컨테이너를 띄웠습니다. 도커를 사용하지 않고 젠킨스를 우분투에 직접 설치한다면 해주어야할 환경 설정이 가득합니다. 젠킨스를 돌리기 위한 JDK 설치, 젠킨스 설치, 젠킨스 포트 설정, 방화벽 설정 등등...\n\n하지만 도커를 사용하면 이런 **환경 설정 없이 간단한 명령어 몇가지로 젠킨스를 설치하고 서버에 띄울 수 있습니다.**\n\n도커는 서비스를 운용하는데 필요한 실행환경, 라이브러리, 소프트웨어, 코드 등을 컨테이너라는 단위로 가상화하는 컨테이너 기반 가상화 플랫폼입니다. 도커를 사용하면 EC2 인스턴스에는 미리 **도커라이징(Dockerizing)**된 이미지를 다운로드 받고 도커를 통해 실행하기만 하면되며, 해당 컨테이너가 어떤 환경을 필요로 하는지 전혀 알 필요가 없습니다.\n\n> 더 자세한 내용은 제가 작성한 [이론과 실습을 통해 이해하는 Docker 기초](https://hudi.blog/about-docker/)를 읽어보시면 좋을 것 같습니다 🤭\n\n### 우분투에 도커 설치\n\n> 달록팀은 EC2 t4g.micro 인스턴스에 우분투 22.04 (ARM 64) 환경을 사용하고 있습니다.\n\n> 아래 설치 방법은 [도커 공식 도큐먼트](https://docs.docker.com/engine/install/ubuntu/)에서 제공되는 내용입니다.\n\n#### 레포지토리 셋업\n\n아래 명령을 통해서 우분투의 `apt`의 패키지 인덱스를 최신화하고, `apt`가 HTTPS를 통해 패키지를 설치할 수 있도록 설정합니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n```\n\n#### 도커의 공식 GPG 키 추가\n\n```shell\n$ sudo mkdir -p /etc/apt/keyrings\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n```\n\n#### 레포지토리 셋업\n\n```shell\n$ echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n```\n\n#### 도커 엔진 설치\n\n아래 명령을 실행하면 가장 최신버전의 도커 엔진이 설치됩니다.\n\n```shell\n$ sudo apt-get update\n$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n```\n\n#### 도커 설치 확인\n\n```shell\n$ sudo docker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n...\n```\n\n위 명령을 실행하여 위와 같이 `Hello from Docker!` 메시지가 출력되면 성공적으로 도커 설치가 완료된 것 입니다. 다음 단계로 넘어가볼까요? 🤗\n\n## 젠킨스 컨테이너 실행\n\n### 젠킨스 이미지 다운로드\n\n```shell\n$ docker pull jenkins/jenkins:lts\n```\n\n위 명령을 통해 Jenkins LTS(Long Term Support) 버전의 이미지를 다운로드 받습니다.\n\n### 젠킨스 컨테이너 띄우기\n\n```shell\n$ sudo docker run -d -p 8080:8080 -v /jenkins:/var/jenkins_home --name jenkins -u root jenkins/jenkins:lts\n```\n\n위 명령을 통해 다운로드 받은 젠킨스 이미지를 컨테이너로 띄울 수 있습니다. 사용된 각 옵션을 간단히 알아볼까요?\n\n- **-d** : 컨테이너를 **데몬**으로 띄웁니다.\n- **-p 8080:8080** : 컨테이너 외부와 내부 포트를 **포워딩**합니다. 좌측이 호스트 포트, 우측이 컨테이너 포트입니다.\n- **-v /jenkins:/var/jenkins_home** : 도커 컨테이너의 데이터는 **컨테이너가 종료되면 휘발**됩니다. 도커 컨테이너의 데이터를 보존하기 위한 여러 방법이 존재하는데, 그 중 한 방법이 **볼륨 마운트**입니다. 이 옵션을 사용하여 젠킨스 컨테이너의 `/var/jenkins_home` 이라는 디렉토리를 호스트의 `/jenkins` 와 마운트하고 데이터를 보존할 수 있습니다.\n- **--name jenkins** : 도커 컨테이너의 이름을 설정합니다.\n- **-u root** : 컨테이너가 실행될 리눅스의 사용자 계정을 root 로 명시합니다.\n\n### docker-compose 사용하기\n\n하지만 위와 같은 명령어를 모두 외우고 있다가, 도커 컨테이너를 실행할 때 마다 입력하게 된다면 굉장히 번거롭겠죠. 따라서 도커는 docker-compose 라는 것을 지원합니다. 도커 컴포즈는 여러 컨테이너의 실행을 한번에 관리할 수 있게 도와주는 도커의 도구입니다. 하지만 저희와 같이 하나의 컨테이너만 필요한 상황에서도 유용하게 사용할 수 있죠.\n\n```shell\n$ sudo apt install docker-compose\n```\n\n위 명령을 이용하여 `docker-compose` 를 설치합니다.\n\n그리고 도커를 실행할 경로에 `docker-compose.yml` 이란 파일을 만들고, 아래의 내용을 작성해줍니다.\n\n```yaml\nversion: \"3\"\nservices:\n  jenkins:\n    image: jenkins/jenkins:lts\n    user: root\n    volumes:\n      - ./jenkins:/var/jenkins_home\n    ports:\n      - 8080:8080\n```\n\n생성한 `docker-compose.yml` 이 존재하는 경로에서 아래의 명령을 실행하면 복잡한 명령 없이도 도커 컨테이너를 실행할 수 있습니다.\n\n```shell\n$ sudo docker-compose up -d\n```\n\n> -d 옵션은 컨테이너가 데몬으로 실행됨을 의미합니다.\n\n## 젠킨스 설정\n\n도커를 사용하여 젠킨스 컨테이너가 EC2 인스턴스에 성공적으로 띄워졌다면, EC2의 퍼블릭 IP를 통해 외부에서 접속할 수 있을 것 입니다. localhost:8080으로 접속하면 아래와 같은 화면이 보일 것 입니다.\n\n![](./unlock-jenkins.png)\n\n```shell\n$ sudo docker logs jenkins\n```\n\n위 명령을 사용하면, `jenkins` 컨테이너에 출력된 로그를 확인할 수 있습니다. 젠킨스를 최초로 설치하고 실행하면 사진에서 요구하는 initial admin password를 출력해주는데요, 로그를 확인해봅시다.\n\n```\n*************************************************************\n*************************************************************\n*************************************************************\n\nJenkins initial setup is required. An admin user has been created and a password generated.\n\nPlease use the following password to proceed to installation:\n\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n\nThis may also be found at: /var/jenkins_home/secrets/initialAdminPassword\n\n*************************************************************\n*************************************************************\n*************************************************************\n```\n\n위에서 표시된 `XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` 를 웹사이트에 넣어주시면 됩니다.\n\n혹은 아래의 명령으로 `jenkins` 컨테이너 내부에 직접 접속하여, `/var/jenkins_home/secrets/initialAdminPassword` 파일의 내용을 조회하는 방법도 있습니다.\n\n```shell\n$ sudo docker exec -it jenkins /bin/bash\n$ cat /var/jenkins_home/secrets/initialAdminPassword\n```\n\n![](./customize-jenkins.png)\n\n그 다음 위 화면에서 Install suggested plugins를 클릭하여 추천되는 플러그인을 설치합니다. 그 이후 요구되는 여러 정보의 입력을 끝 마쳐주세요.\n\n```\nSystem.setProperty('org.apache.commons.jelly.tags.fmt.timeZone\n```\n\n이후 대시보드에서 **Jenkins 관리 > Script Console** 에서 위의 스크립트를 입력하여 타임존을 서울로 설정하기만 하면, 젠킨스 기본 설정이 완료됩니다! 🎉\n\n## 마치며\n\n다음 포스팅에서는 달록이 젠킨스를 이용하여 어떻게 배포 자동화 프로세스를 구축하였는지 작성해보도록 하겠습니다. 많은 기대 부탁드립니다 🙏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정... 배…","fields":{"slug":"/deploy-automation-with-shell-script/"},"frontmatter":{"date":"July 19, 2022","title":"쉘 스크립트와 함께하는 달록의 스프링부트 어플리케이션 배포 자동화","tags":["DevOps"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n웹서비스 개발팀은 새롭게 개발한 서비스의 기능을 어떻게 사용자에게 전달할까요? 새로운 기능이 메인 브랜치에 병합될 때 마다 EC2 인스턴스에 접속하여 브랜치를 Pull 하고, 프로젝트를 빌드하고, 현재 동작중인 어플리케이션의 프로세스를 종료하고, 새롭게 빌드된 어플리케이션의 프로세스를 띄우는 과정...\n\n배포가 필요할때마다 이런 명령을 수동으로 일일히 입력한다면, 그건 너무 지루한 작업아닐까요? 😫 실수라도 하면 어쩌죠? 😰\n\n우테코 달록팀 백엔드는 이런 한계점을 극복하고자 쉘 스크립트를 활용하여 배포 프로세스를 자동화하였습니다. 달록팀은 어떻게 스프링부트 어플리케이션의 배포를 자동화했을까요?\n\n## 쉘 스크립트\n\n쉘 스크립트는 유닉스/리눅스 기반 운영체제에서의 일련의 명령으로 구성된 실행가능한 텍스트 파일입니다. 원래라면 일일히 키보드로 입력해야하는 리눅스 명령을 하나의 파일에 모아두고, 한번에 실행할 수 있죠. 작성된 명령은 셸이라고 불리는 명령줄 인터프리터에서 실행되며, 위에서부터 아래로 차례로 실행됩니다. 이 쉘 스크립트를 이용해 리눅스 환경에서 여러 프로세스를 자동화할 수 있습니다.\n\n달록이 스프링부트 어플리케이션을 배포하는 환경은 Ubuntu 22 버전이므로 쉘 스크립트를 활용할 수 있습니다.\n\n## 수동으로 배포하기\n\n배포 프로세스를 자동화하려면 우선 수동으로 어떤 명령을 사용해서 배포를 하는지 알아야합니다. 어떤과정을 거쳐 배포되는지부터 알아볼까요?\n\n> 레포지토리는 이미 Clone 되어있다고 가정합니다.\n\n### 1. Git Pull\n\n```shell\n$ cd 2022-dallog/backend\n$ git pull\n```\n\n우선 Github에서 가장 최신 버전을 pull 해와야겠죠?\n\n### 2. 빌드\n\n```shell\n./gradlew bootJar\n```\n\n`gradlew` 를 사용하여 자바 프로젝트를 빌드해서 `.jar` 파일을 생성합니다.\n\n### 3. 프로세스 종료\n\n```shell\n$ ps -ef | grep jar\n$ kill -15 XXXXX\n```\n\n`ps` 명령을 사용해서 실행중인 스프링부트 어플리케이션의 PID를 알아내고, `kill` 명령을 통해 프로세스를 종료합니다.\n\n### 4. 환경변수 설정\n\n달록의 스프링부트 어플리케이션은 여러 민감한 정보를 환경변수를 사용하여 외부에 노출되지 않도록 하였습니다. 따라서 어플리케이션이 실행될 때 환경변수도 함께 설정을 해주어야합니다.\n\n```shell\n$ export GOOGLE_CLIENT_ID=\"XXXXX\"\n$ export GOOGLE_CLIENT_SECRET=\"XXXXX\"\n$ export GOOGLE_REDIRECT_URI=\"XXXXX\"\n$ export GOOGLE_TOKEN_URI=\"XXXXX\"\n$ export JWT_SECRET_KEY=\"XXXXX\"\n$ export JWT_EXPIRE_LENGTH=3600\n\n...\n```\n\n### 5. 드디어 실행\n\n```shell\n$ sudo -E nohup java -jar ./build/libs/backend-0.0.1-SNAPSHOT.jar\n```\n\n드디어 스프링부트 어플리케이션을 실행합니다.\n\n이 귀찮은 과정을 배포 할때마다 해야한다니 벌써 머리가 어질어질 하네요. 😵‍💫 그렇다면 앞서 소개드린 쉘 스크립트를 통해서 이 과정을 자동화해볼까요?\n\n## 달록의 배포 쉘 스크립트\n\n```shell\n#! /bin/bash\n\nPROJECT_PATH=/home/ubuntu/2022-dallog\nPROJECT_NAME=backend\nPROJECT_BUILD_PATH=backend/build/libs\n\ncd $PROJECT_PATH/$PROJECT_NAME\n\nclear\n\necho \"🌈 Github에서 프로젝트를 Pull 합니다.\\n\"\n\ngit pull\n\necho \"\\n🌈 SpringBoot 프로젝트 빌드를 시작합니다.\\n\"\n\n./gradlew bootJar\n\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n\necho \"\\n🌈 SpringBoot 환경변수 설정\"\n\nexport GOOGLE_CLIENT_ID=\"XXXXX\"\nexport GOOGLE_CLIENT_SECRET=\"XXXXX\"\nexport GOOGLE_REDIRECT_URI=\"XXXXX\"\nexport GOOGLE_TOKEN_URI=\"XXXXX\"\nexport JWT_SECRET_KEY=\"XXXXX\"\nexport JWT_EXPIRE_LENGTH=3600\n\necho \"\\n🌈 SpringBoot 애플리케이션을 실행합니다.\\n\"\n\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n달록이 작성한 배포 자동화 쉘 스크립트는 아래와 같습니다. 차근차근 알아볼까요?\n\n### #! /bin/bash\n\n`#! /bin/bash` 은 해당 쉘 스크립트가 많은 쉘 중 **Bash Shell 로 실행됨**을 알립니다.\n\n### 변수 사용\n\n`PROJECT_PATH`, `PROJECT_NAME` 과 같이 자주 사용되는 데이터는 쉘 스크립트에서 제공하는 변수 기능으로 분리하였습니다. 이때 주의할 점은 쉘 스크립트에서 변수를 선언할 때 `=` **앞뒤에 공백이 와서는 안된다는 점** 입니다.\n\n### echo\n\n`echo` 명령을 통해 배포 프로세스가 어디까지 진행됐는지 사용자에게 알려줍니다.\n\n### 실행중인 어플리케이션의 PID 가져오기\n\n```shell\nCURRENT_PID=$(pgrep -f ${PROJECT_NAME}-.*.jar | head -n 1)\n```\n\n#### pgrep\n\n쉘 스크립트 중 위와 같은 코드가 있었습니다. 위 코드는 우선 `pgrep` 이라는 명령을 통해서 실행중인 프로세스의 이름으로 PID 목록을 가져옵니다.\n\n#### pipe와 head\n\n그리고 파이프(`|`)명령으로 다른 프로세스로 PID 목록을 보냅니다. PID 목록은 `head` 명령으로 전달되며, `head` 명령은 PID 목록의 첫번째만을 가져옵니다.\n\n#### 명령의 실행결과를 변수에 담기\n\n이렇게 가져온 PID는 `$()` 문법을 통해 `CURRENT_PID` 변수에 저장됩니다. `$()` 는 `$(command)` 형태로 사용되며, 괄호 내부의 실행 결과를 변수로 저장하기 위해 사용됩니다.\n\n### 조건문\n\n쉘 스크립트에도 `if` 문을 사용하여 조건문을 작성할 수 있습니다. 다만, 우리에게 익숙한 프로그래밍 언어에서의 if문과는 조금 괴리가 존재해서 별도로 학습이 필요할수도 있습니다.\n\n```shell\nif [ -z \"$CURRENT_PID\" ]; then\n\techo \"🌈 구동중인 애플리케이션이 없으므로 종료하지 않습니다.\"\nelse\n\techo \"🌈 구동중인 애플리케이션을 종료했습니다. (pid : $CURRENT_PID)\"\n\tkill -15 $CURRENT_PID\nfi\n```\n\n위 코드는 아까 PID를 담은 `CURRENT_PID` 가 비어있는지 확인한 후 존재하지 않다면 메시지만 출력하고, 존재한다면 해당 PID를 `kill` 명령으로 종료합니다.\n\n쉘 스크립트의 `if` 문에서 `-z` 는 조건식의 종류 중 하나이며, 주어진 문자열의 길이가 0이라면 True를 나타냅니다. 확실히 조금 낯설죠? 😅\n\n### JAR파일 경로 가져오기\n\n```shell\nJAR_PATH=$(ls $PROJECT_PATH/$PROJECT_BUILD_PATH/ | grep .jar | head -n 1)\n```\n\n`ls` 명령을 통해 빌드 디렉토리의 파일 목록을 가져오고, `grep` 명령을 통해 `.jar` 파일만을 가져옵니다. 그다음 `head` 명령을 통해 단 하나의 파일만을 가져온 다음, `JAR_PATH` 변수에 저장합니다.\n\n### 어플리케이션 실행하기\n\n```shell\nsudo -E nohup java -jar $PROJECT_PATH/$PROJECT_BUILD_PATH/$JAR_PATH &\n```\n\n어플리케이션을 실행합니다.\n\n#### sudo -E\n\n`sudo` 명령 뒤에 붙은 `-E` 옵션은 유저가 설정한 환경변수를 `sudo` 명령에서도 공유하여 사용할 수 있도록 만드는 옵션입니다.\n\n#### nohup\n\n`nohup` 명령은 현재 **터미널 세션이 끊어져도 프로세스가 계속 살아있도록** 만들기 위해 사용되는 명령입니다.\n\n#### Background 프로세스\n\n그리고 명령 맨 뒤에 `&` 가 붙어있는데, 프로세스를 Foreground가 아닌 **Background에서 실행**하기 위해 붙여줍니다.\n\n## 한계점\n\n하지만, 이런 방식도 결국 한계점이 존재합니다. 특히나 달록과 같이 애자일한 조직에서는 최대한 작은 기능단위로 개발이 병렬적으로 진행되어, 메인 브랜치에 머지됩니다. 하루에 몇번이고 배포를 해야하는 상황이 발생할수도 있죠.\n그렇지 않아도 모든 개발자가 바쁘게 새로운 기능을 개발하기 바쁜데, 메인 브랜치에 병합된 시점마다 EC2 인스턴스에 접속해서 쉘 스크립트를 실행해야할까요?\n\n이런 한계점을 극복하고자, 달록팀은 앞으로 CI/CD 도구를 도입할 예정입니다. 세상에는 참 다양한 CI/CD 도구가 존재합니다. Jenkins, Github Actions, Travis CI, Circle CI, Gitlab CI/CD 등등...\n\n달록의 이번 스프린트의 배포 태스크에서는 이런 다양한 CI/CD 도구들의 장단을 분석하고 도입할 예정입니다. 많은 기대 부탁드립니다. 👏👏\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다. 모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면…","fields":{"slug":"/data-jpa-auditing/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Auditing","tags":["Spring","Data JPA","Auditing"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\n> auditing이란 엔티티와 관련된 이벤트(insert, update, delete)를 추적하고 기록하는 것을 의미한다.\n\n모든 엔티티에 생성일시, 수정일시, 생성한 사람을 추가하고 싶은 경우를 생각해보자. 모든 엔티티에 생성일시, 수정일시, 생성한 사람에 대한 필드를 일일이 구현해주어야 한다. 이렇게 되면 모든 엔티티에 중복이 생기고 유지보수가 어려워진다. Spring Data JPA가 제공하는 `Auditing` 기능을 사용하면 이런 기능을 쉽고 빠르게 구현할 수 있다.\n\n## Spring Data JPA Auditing 적용하기\n\n`Auditing`을 적용하기 위해서는 우선 어노테이션을 적용해야 한다. `@Configuration` 어노테이션이 적용된 Config 클래스에 아래와 같이 `@EnableJpaAuditing`을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing\npublic class JpaConfig {\n}\n```\n\n⚠️ 주의 : 원활한 Slice 테스트를 위해 @SpringBootApplication 과 @EnableJpaAuditing 어노테이션 분리하기\n\n[https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing](https://docs.spring.io/spring-boot/docs/current/reference/html/features.html#features.testing.spring-boot-applications.user-configuration-and-slicing)\n\n### Spring Entity Callback Listener 적용하기\n\nAuditing entity listener class로 지정하기 위해 `@EntityListeners` 어노테이션을 Entity 클래스에 추가한다. 인자로는 `AuditingEntityListener.class`를 넘긴다. 이 설정을 통해 엔티티에 이벤트가 발생했을 때 정보를 캡처할 수 있다.\n\n```java\n@Entity\n@EntityListeners(AuditingEntityListener.class)\npublic class BaseEntity {\n    // ...\n}\n```\n\n### 생성일시, 수정일시 추적하기\n\n생성일시는 `@CreatedDate`, 수정일시는 `@LastModifiedDate` 어노테이션을 통해 추적할 수 있다. 생성일시의 경우 한 번 생성되면 변경되어선 안 되며, 항상 존재해야 하므로 `nullable = false`, `updatable = false`로 지정한다.\n\n추가적으로 BaseEntity를 다른 Entity들이 상속받아 사용할 수 있도록 `@MappedSuperclass` 어노테이션을 통해 해당 클래스를 Entity가 아닌 SuperClass로 지정했다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedDate\n    @Column(name = \"created_at\", nullable = false, updatable = false)\n    private LocalDateTime createdAt;\n\n    @LastModifiedDate\n    @Column(name = \"updated_at\")\n    private LocalDateTime updatedAt;\n\n    // ...\n}\n```\n\n### 생성한 사람, 수정한 사람 추적하기\n\n생성한 사람은 `@CreatedBy`, 수정한 사람은 `@LastModifiedBy` 어노테이션을 통해 추적할 수 있다. 해당 필드는 생성자, 수정자의 이름으로 채워진다.\n\n```java\n@MappedSuperclass\n@EntityListeners(AuditingEntityListener.class)\npublic abstract class BaseEntity {\n\n    @CreatedBy\n    @Column(name = \"created_by\")\n    private String createdBy;\n\n    @LastModifiedBy\n    @Column(name = \"modified_by\")\n    private String modifiedBy;\n\n    // ...\n}\n```\n\n유저에 대한 정보는 SecurityContext's Authentication 인스턴스로부터 가져온다. 이 값을 커스텀하고 싶다면 `AuditorAware<T>` 인터페이스를 구현해야 한다.\n\n```java\npublic class AuditorAwareImpl implements AuditorAware<String> {\n\n    @Override\n    public String getCurrentAuditor() {\n        // your custom logic\n    }\n}\n```\n\n이렇게 만든 `AuditorAwareImpl`를 사용하려면 Config 클래스에 `AuditorAwareImpl` 인스턴스로 초기화되는 `AuditorAware` 타입의 빈을 설정해주어야 한다. 그리고 `@EnableJpaAuditing` 어노테이션에 `auditorAwareRef=\"auditorProvider\"` 설정을 추가한다.\n\n```java\n@Configuration\n@EnableJpaAuditing(auditorAwareRef=\"auditorProvider\")\npublic class JpaConfig {\n    //...\n\n    @Bean\n    AuditorAware<String> auditorProvider() {\n        return new AuditorAwareImpl();\n    }\n\n    //...\n}\n```\n\n---\n\n### References\n\n[https://www.baeldung.com/database-auditing-jpa](https://www.baeldung.com/database-auditing-jpa)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 파랑이 작성했습니다. Spring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자. 메서드를 보면 파라미터로  객체를 받는다.  객체는 Pagination을 위한 정보를 저장하는 객체다.  인터페이스의 구현체인 의 인스턴스를 생성하여…","fields":{"slug":"/data-jpa-slice-page/"},"frontmatter":{"date":"July 18, 2022","title":"Spring Data JPA의 Slice & Page","tags":["Spring","Data JPA","Slice","Page"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [파랑](https://github.com/summerlunaa)이 작성했습니다.\n\nSpring Data JPA에서는 Pagination을 위한 두 가지 객체를 제공한다. 바로 Slice와 Page다. Repository 코드를 먼저 보자.\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n\n// Service\nPageRequest pageRequest = PageRequest.of(page, size); // pageRequest 생성\nSlice<Member> slices = memberRepository.findSliceBy(pageRequest); // repository에서 페이지 가져오기\nPage<Member> pages = memberRepository.findPageBy(pageRequest);\n```\n\n메서드를 보면 파라미터로 `Pageable` 객체를 받는다. **`Pageable` 객체는 Pagination을 위한 정보를 저장하는 객체**다. `Pageable` 인터페이스의 구현체인 `PageRequest`의 인스턴스를 생성하여 메서드 인자로 넘겨줄 수 있다.\n\n![PageRequest static constructor](pagerequest.png)\n\n`PageRequest`는 정적 팩토리 메서드 `of`를 사용하여 인스턴스를 생성할 수 있다. `PageRequest`는 인자로 page, size, 필요하다면 sort까지 받을 수 있다.\n\n- `page` : 0부터 시작하는 페이지 인덱스 번호\n- `size` : 한 페이지에 반환할 데이터의 개수\n- `sort` : 정렬 방식\n\npage와 size를 쿼리 파라미터로 받아 `PageRequest`를 생성하여 Repository 메서드에 넘겨주는 것으로 간단하게 Pagination을 구현할 수 있는 것이다.\n\n## Slice VS Page\n\n```java\npublic interface MemberRepository extends JpaRepository<Member, Long> {\n    Slice<Member> findSliceBy(final Pageable pageable);\n        Page<Member> findPageBy(final Pageable pageable);\n}\n```\n\nRepository의 메서드를 보면 반환 값으로 `Slice` 혹은 `Page`를 받을 수 있다. 둘은 어떤 차이가 있을까?\n\n### Slice\n\n![slice](slice.png)\n\n`Slice`는 `Streamable`을 상속받는 인터페이스로 Pagination과 관련된 여러 메서드를 갖고 있다. 대표적인 메서드 몇 가지만 살펴보자.\n\n![slice methods](slice_methods.png)\n\n현재 페이지의 내용을 확인하거나 다음 페이지, 이전 페이지에 대한 정보를 가져올 수 있다. 그렇다면 `Page`는 무엇일까?\n\n### Page\n\n![page](page.png)\n\n`Page`는 `Slice`를 상속한다. 따라서 `Slice`가 가진 모든 메서드를 `Page`도 사용할 수 있다. 다만 `Page`가 다른 점은 **조회 쿼리 이후 전체 데이터 개수를 조회하는 쿼리가 한 번 더 실행된다는 것**이다.\n\nPage가 추가적으로 구현하고 있는 메서드 두 가지만 살펴보자.\n\n![page methods](page_methods.png)\n\nPage의 경우 전체 데이터 개수를 조회하는 쿼리가 추가적으로 실행되므로 Slice와 다르게 전체 데이터 개수나 전체 페이지 수까지 확인할 수 있다.\n\n> 추가적으로..\n\n```java\npublic interface PagingAndSortingRepository<T, ID> extends CrudRepository<T, ID> {\n\n    Iterable<T> findAll(Sort sort);\n\n    Page<T> findAll(Pageable pageable);\n}\n```\n\n`Page`를 반환받을 경우 아래처럼 **`Page`를 반환하는 `findAll` 메서드가 이미 존재**한다. 따로 Repository Interface에 메서드를 추가해주지 않아도 `findAll` 메서드에 `Pageable` 객체를 넘겨주면 Pagination을 사용할 수 있다. (Slice를 반환하려면 메서드를 정의해주어야 한다.)\n\n### Slice VS Page 어떤 걸 사용해야 할까?\n\n`Slice`는 전체 데이터 개수를 조회하지 않고 이전 or 다음 `Slice`가 존재하는지만 확인할 수 있다. 따라서 **`Slice`는 무한 스크롤 등을 구현하는 경우 유용**하다. `Page`에 비해 쿼리가 하나 덜 날아가므로 **데이터 양이 많을수록 `Slice`를 사용하는 것이 성능상 유리**하다.\n\n`Page`는 전체 데이터 개수를 조회하는 쿼리를 한 번 더 실행한다. 따라서 **전체 페이지 개수나 데이터 개수가 필요한 경우 유용**하다.\n\n> 알록에서는 카테고리 조회를 무한 스크롤로 구현하므로 Slice를 사용했다.\n\n## 컨트롤러에서 queryParameter를 Pageable 객체로 받는 방법\n\n앞에선 queryParameter를 통해 page, size를 받아 PageRequest를 만들어 넘겨주는 방법을 설명했다.\n\n```java\n// Controller\n// queryParameter를 통해 page, size를 받는다\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(@RequestParam int page, @RequestParam int size) {\n    // 생략\n}\n```\n\n하지만 이렇게 구현하니까 *“ModelAttribute를 통해 queryParameter를 DTO로 받는 것처럼, queryParameter를 Pageable 객체로 받을 수 없을까?”*하는 의문이 생겼다. 직접 ArgumentResolver를 구현할 뻔했지만 찾아보니 역시 똑똑한 JPA.. Pageable 객체를 인수로 설정하면 어노테이션 없이도 자동으로 객체를 만들어준다. `PageableHandlerMethodArgumentResolver` 가 이미 구현되어 있기 때문이다.\n\n```java\n// Controller\n// Pageable 객체를 바로 받을 수 있다.\n@GetMapping(\"/api/members\")\npublic List<MemberResponse> findAll(Pageable pagealbe) {\n    // 생략\n}\n\n// Service\nSlice<Member> slices = memberRepository.findSliceBy(pageable); // PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있다.\n```\n\n이렇게 사용하면 PageReqeust를 생성할 필요 없이 바로 객체를 넘겨줄 수 있어 훨씬 편하다. 하지만 문제가 한 가지 발생한다. **프론트에서는 page index를 1부터 계산하는 것과 달리 Pageable은 page index를 0부터 계산한다.** 따라서 page의 인덱스를 1부터 시작하도록 설정할 필요가 있다.\n\n## page 인덱스를 1부터 시작하도록 설정하기\n\n### 방법 1. PageableHandlerMethodArgumentResolverCustomizer 커스터마이징 하기\n\n```java\n@Configuration\npublic class CustomPageableConfiguration {\n    @Bean\n    public PageableHandlerMethodArgumentResolverCustomizer customize() {\n        return p -> p.setOneIndexedParameters(true);\n    }\n}\n```\n\n따로 `CustomPageableConfiguration` 클래스를 만들어 `PageableHandlerMethodArgumentResolverCustomizer`를 커스터마이징 해주는 방법이 있다. 여기서 `setOneIndexedParameters` 메서드를 통해 인덱스를 1부터 시작하게 설정할 수 있다.\n\n### 방법 2. application-properties에 설정 추가하기\n\n```java\nspring.data.web.pageable.one-indexed-parameters=true\n```\n\n### 유의점 1\n\n이렇게 인덱스를 1부터 시작하도록 설정하면 **단순히 Controller에서 받는 Pageable의 page 값이 -1 되어 저장될 뿐이다.** Pageable 객체나 이후에 반환받은 Slice, Page 객체에서는 **page 인덱스가 여전히 0부터 시작**한다. \n\n따라서 getNumber 등의 메서드를 통해 page 번호를 받으면 -1씩 차이가 난다는 사실을 잊어선 안 된다. **현재 페이지 index를 반환하고 싶다면 반드시 -1**을 해줘야 한다. 유의해서 사용하자.\n\n### 유의점 2\n\n**무한 스크롤을 Slice나 Page로 구현하면 데이터가 중복으로 나타날 수 있다.** \n\n예를 들어 생각해보자. 30개의 데이터를 불러왔고 10개의 데이터를 추가로 요청하려 하는데, 그 사이에 10개의 데이터가 앞에 추가되었다. 이 경우 데이터가 10개씩 밀려서 21~30번째 데이터가 중복으로 나타나게 된다.\n달록에서는 아직 이 경우까지 고려해서 페이징을 구현하지 못했다. 좀 더 고민해보아야 할 부분인 것 같다.\n\n### References\n\n[https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/](https://tecoble.techcourse.co.kr/post/2021-08-15-pageable/)\n\n[https://treasurebear.tistory.com/59](https://treasurebear.tistory.com/59)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Slice.html)\n\n[https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html](https://docs.spring.io/spring-data/commons/docs/current/api/org/springframework/data/domain/Page.html)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. 달록에 적절한 패키지 구조 고민하기 우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 , 로 나눌 수 있다. 계층별 패키지 구조 계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로…","fields":{"slug":"/package-structure/"},"frontmatter":{"date":"July 18, 2022","title":"달록에 적절한 패키지 구조 고민하기","tags":["매트","BE","패키지 구조"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## 달록에 적절한 패키지 구조 고민하기\n\n우리는 프로젝트를 진행하며 어떠한 패키지 구조를 구성할지 고민하게 된다. 보통 패키지 구조를 나누는 대표적인 방법으로 `계층별`, `기능별`로 나눌 수 있다.\n\n### 계층별 패키지 구조\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── application\n    │   │               ├── config\n    │   │               ├── domain\n    │   │               ├── dto\n    │   │               ├── exception\n    │   │               ├── presentation\n    │   │               └── DallogApplication.java\n    │   └── resources\n    │       └── application.yml\n```\n\n계층형 구조는 각 계층을 대표하는 패키지를 기준으로 코드를 구성한다. 계층형 구조의 가장 큰 장점은 해당 프로젝트에 대한 이해도가 낮아도 각 계층에 대한 역할을 충분히 숙지하고 있다면 전체적인 구조를 빠르게 파악할 수 있다.\n\n하지만 단점도 존재한다. 하나의 패키지에 모든 클래스들이 모이게 되기 때문에 규모가 커지면 클래스의 개수가 많아져 구분이 어려워진다. 아래는 이전에 계층형 구조를 기반으로 작성한 프로젝트이다.\n\n```json\n└── presentation\n    ├── ChampionController.java\n    ├── CommentController.java\n    ├── PlaylistController.java\n    ├── RankingController.java\n    ├── SearchController.java\n    ├── UserController.java\n    ├── WardController.java\n    └── ...\n```\n\n비교적 적은 코드의 양이지만 규모가 커질수록 애플리케이션에서 presentation 계층에 해당하는 모든 객체가 해당 패키지에 모이게 될 것이다.\n\n### 기능별 패키지 구조\n\n기능별로 패키지를 나눠 구성한다. 기능별 패키지 구조의 장점은 해당 도메인에 관련된 코드들이 응집되어 있다는 점이다. 덕분에 `지역성의 원칙`를 잘 지킬 수 있다고 한다.\n\n> 컴퓨터 과학에서, 참조의 지역성, 또는 지역성의 원칙이란 프로세서가 짧은 시간 동안 동일한 메모리 공간에 반복적으로 접근하는 경향을 의미한다. 참조 지역성엔 두 가지 종류가 있다. 바로 시간적 지역성과 공간적 지역성이다. 시간적 지역성이란 특정 데이터 또는 리소스가 짧은 시간 내에 반복적으로 사용되는 것을 가리킨다. 공간적 지역성이란 상대적으로 가까운 저장 공간에 있는 데이터 요소들이 사용되는 것을 가리킨다.\n\n개발자 역시 복잡하고 거대한 프로젝트의 전체 구조를 모두 인지하는 것은 힘든일이다. 우선 특정 지역의 흐름을 파악할 수 있다면 해당 패키지에 대해서는 마치 캐시에 적재된 데이터에 접근 하듯 빠르게 인지가 가능하다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── auth\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── category\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── schedule\n    │   │               │   ├── application\n    |   |               |   ├── domain\n    │   │               │   ├── dto\n    │   │               │   ├── exception\n    │   │               │   └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.yml\n```\n\n하지만 각 계층이 기능별로 모여 있기 때문에 프로젝트에 대한 이해도가 낮으면 전체 구조를 파악하는데 오랜 시간이 걸린다.\n\n### 더 나아가기\n\n현재 구조에서는 도메인에 해당하는 다양한 기능들이 패키지 전반적으로 퍼져 있다. 각각의 도메인 기능이 밀집되어 있지 않은 구조를 가지고 있다.\n\n```json\n└── src\n    ├── main\n    │   ├── java\n    │   │   └── com\n    │   │       └── allog\n    │   │           └── dallog\n    │   │               ├── domain\n    │   │               │   ├── auth\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   ├── category\n    │   │               │   │   ├── application\n    |   |               │   |   ├── domain\n    │   │               │   │   ├── dto\n    │   │               │   │   ├── exception\n    │   │               │   │   └── presentation\n    │   │               │   └── schedule\n    │   │               │       ├── application\n    |   |               │       ├── domain\n    │   │               │       ├── dto\n    │   │               │       ├── exception\n    │   │               │       └── presentation\n    │   │               ├── global\n    │   │               │   ├── config\n    │   │               │   ├── dto\n    │   │               │   ├── error\n    │   │               │   └── exception\n    │   │               ├── infrastructure\n    │   │               │   └── oauth\n    │   │               └── AllogDallogApplication.java\n    |   |\n    │   └── resources\n    │       └── application.properties\n```\n\n### domain\n\n실제 애플리케이션의 핵심이 되는 도메인 로직이 모여 있다. 애플리케이션의 주요 비즈니스 로직이 모여 있기 때문에 `외부와의 의존성을 최소화`해야 한다. 즉 `외부의 변경에 의해 도메인 내부가 변경되는 것을 막아야 한다는 것`을 인지해야 한다.\n\n### global\n\n`global`은 프로젝트 전반에서 사용하는 객체로 구성한다. 공통적으로 사용하는 dto나 error, config에 대한 것들이 모여 있다.\n\n### infrastructure\n\n`infrasturcture`는 외부와의 통신을 담당하는 로직들이 담겨 있다. 이번 프로젝트에서는 OAuth를 활용한 회원 관리를 진행하기 때문에 google의 인증 서버와 통신이 필요해진다. 이 패키지는 우리의 의지와 다르게 외부의 변화에 따라 변경될 여지를 가지고 있다. 즉 변화에 매우 취약한 구조이며 외부 서버에 의존적 이기 때문에 항시 변화에 대응할 수 있도록 대비해야 한다. 이것이 의미하는 바는 결국 `도메인 관련 패키지에서 infrastructure를 직접적으로 의존하는 것`은 도메인 로직을 안전하게 지킬 수 없다는 의미를 내포한다.\n\n## 정리\n\n각각의 방법은 서로 다른 장단점을 가지고 있기 때문에 정답은 없다고 생각한다. 현재 프로젝트의 규모와 요구사항을 고려하여 선택해야 한다. 다만 선택한 패키지 구조에 충분한 근거를 가져야하고, 객체와 패키지 사이의 의존성에 대해 충분히 고민해야 한다.\n\n달록은 현재 기능별 패키지 구조로 진행되고 있다. 모든 팀원들이 처음 부터 기획과 설계에 대한 고민을 진행했기 때문에 프로젝트의 구조에 대해 분석 하는 시간이 불필요했기 때문이다. 하지만 각각의 기능들이 난잡하게 퍼져 있기 때문에 [더 나아가기](#더-나아가기)에서 언급한 것 처럼 좀 더 밀접한 기능들을 모아둘 필요가 있다고 판단한다. 이것은 추후 팀원들과 충분한 논의를 통해 개선해갈 예정이다.\n\n## References.\n\n[Spring Guide - Directory](https://cheese10yun.github.io/spring-guide-directory)<br>\n[지역성의 원칙을 고려한 패키지 구조: 기능별로 나누기](https://ahnheejong.name/articles/package-structure-with-the-principal-of-locality-in-mind)\n"},{"excerpt":"이 글은 우테코 달록팀 크루 나인이 작성했습니다. 🎯 \"무한 스크롤을 구현해보세요!\" 어떻게 구현하실 건가요? 무한 스크롤을 처음 마주했을때 🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요. scroll event 사용하기 우테코 레벨1 유튜브 미션 처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다. 바로 스크롤 이벤트와 of…","fields":{"slug":"/infinite-scroll/"},"frontmatter":{"date":"July 18, 2022","title":"React에서 무한 스크롤 구현하기","tags":["react"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [나인](https://github.com/jhy979)이 작성했습니다.\n\n🎯 \"무한 스크롤을 구현해보세요!\"\n\n어떻게 구현하실 건가요?\n\n## 무한 스크롤을 처음 마주했을때\n\n🤔 저는 처음 무한 스크롤을 구현할 때 다음과 같은 방식을 사용했어요.\n\n```\n1. scroll 이벤트를 감지한다.\n\n2. 현재 스크롤 영역의 `위치를 계산`한다.\n\n3. 영역 계산을 통해 페이지 아래에 위치하면 API 요청을 진행한다.\n\n4. 받아온 데이터를 추가하여 다시 렌더링한다.\n\n5. 무한 반복\n```\n\n---\n\n## scroll event 사용하기\n\n[우테코 레벨1 유튜브 미션](https://github.dev/jhy979/javascript-youtube-classroom/tree/jhy979-step2)\n\n처음 제가 무한 스크롤을 구현했던 방법은 다음과 같습니다.\n\n바로 스크롤 이벤트와 offset을 이용한 방식이죠!\n\n```js\n  scrollToBottom(callback) {\n    const isScrollBottom =\n      this.$videoList.scrollHeight - this.$videoList.scrollTop <=\n      this.$videoList.offsetHeight + EVENT.SCROLL.OFFSET;\n\n    if (isScrollBottom) {\n      callback(this.$searchInput.value);\n    }\n  }\n```\n\n메서드 네이밍을 통해서도 알 수 있듯이, 화면 하단까지 내려갔을 경우 (offset 정도를 감안하여) 인자로 받은 함수를 실행시켜주었습니다.\n\n아 물론, 스크롤 이벤트는 워낙 많이 발생하기 때문에 throttle을 걸어주었습니다. (이건 필수죠)\n\n😢 하지만, `documentElement.scrollTop`, `documentElement.scrollHeight`, `documentElement.offsetHeight`는 리플로우(Reflow)가 발생합니다.\n\n확실히 비효율적이겠죠!\n\n---\n\n## IntersectionObserver 사용하기\n\n달록에서는 무한 스크롤을 구현할 때 [Intersection Observer](https://developer.mozilla.org/ko/docs/Web/API/Intersection_Observer_API)를 사용했습니다.\n\n> Intersection Observer는 쉽게 말해 지정한 대상이 화면에 보이는지 감시하고 판단하는 도구입니다.\n\n브라우저 Viewport와 Target으로 설정한 요소의 교차점을 관찰하여 그 Target이 Viewport에 포함되는지 구별하는 기능을 제공합니다.\n\n<img src=\"https://velog.velcdn.com/images/jhy979/post/19500233-65fc-4ba9-b421-81516700c00b/image.png\" />\n\n### useIntersect 커스텀훅\n\n> 가장 먼저 useIntersect 라는 커스텀훅을 제작했습니다.\n\n이 커스텀훅은 `인자로 intersect시 실행할 함수`를 받고 `ref를 제공`하여 관찰할 대상을 지정할 수 있습니다.\n\n```ts\ntype IntersectHandler = (\n  entry: IntersectionObserverEntry,\n  observer: IntersectionObserver\n) => void;\n\n// 인자로 onIntersect와 options를 받습니다.\n// onIntersect는 intersect 발생 시 실행하고 싶은 함수입니다.\nfunction useIntersect(\n  onIntersect: IntersectHandler,\n  options?: IntersectionObserverInit\n) {\n  // 관찰하고 싶은 친구를 잡기 위해 ref를 만들어주세요.\n  const ref = useRef<HTMLDivElement>(null);\n\n  // intersect 시 실행할 함수를 만들어줍시다.\n  const callback = useCallback(\n    (entries: IntersectionObserverEntry[], observer: IntersectionObserver) => {\n      entries.forEach((entry) => {\n        if (entry.isIntersecting) {\n          onIntersect(entry, observer);\n        }\n      });\n    },\n    [onIntersect]\n  );\n\n  // 🔨 옵저버에게 일을 시켜봅시다.\n  useEffect(() => {\n    // 우리가 관찰하고 싶은 친구가 없으면 그냥 return 해버려요.\n    if (!ref.current) {\n      return;\n    }\n\n    // 관찰할 대상이 있으면 옵저버 데꼬 와야죠!\n    const observer = new IntersectionObserver(callback, options);\n\n    // 이 옵저버한테 감시를 시킵시다.\n    observer.observe(ref.current);\n\n    // 할 일 끝나면 고생한 옵저버도 쉬게 해줍시다!\n    return () => {\n      observer.disconnect();\n    };\n  }, [ref, options, callback]);\n\n  return ref;\n}\n\nexport default useIntersect;\n```\n\n### 실제 사용\n\nuseIntersect 커스텀훅을 잘 만들었으니 이제 이 커스텀훅을 무한 스크롤에 사용해 봅시다.\n\n![](https://velog.velcdn.com/images/jhy979/post/4643727c-852d-4f23-ab4f-44ce79e2e3b2/image.gif)\n\n다음은 카테고리 목록을 계속 불러와 리스트를 보여주는 컴포넌트입니다.\n\n```tsx\nfunction CategoryList({\n  categoryList,\n  getMoreCategories,\n  hasNextPage,\n}: CategoryListProps) {\n  // useIntersect 커스텀훅의 인자로 (교차 시) 실행할 함수를 넣어줍시다.\n  const ref = useIntersect(() => {\n    hasNextPage && getMoreCategories();\n  });\n\n  return (\n    <div css={categoryTable}>\n      <div css={categoryTableHeader}>\n        <span> 생성 날짜 </span>\n        <span> 카테고리 이름 </span>\n      </div>\n      {categoryList.map((category) => (\n        <CategoryItem key={category.id} category={category} />\n      ))}\n      // 페이지 하단까지 내리면 이 친구가 등장하여 옵저버에게 감지될 거예요.\n      <div ref={ref} css={intersectTarget}></div>\n    </div>\n  );\n}\n```\n\n💪 무한 스크롤함에 따라 props로 받아오는 categoryList가 길어지게 될텐데요, 다행히 React에서는 key값으로 변경 여부를 확인하기 때문에 새롭게 추가된 리스트들만 리렌더링해주었습니다.\n"},{"excerpt":"이 글은 우테코 달록팀 크루 매트가 작성했습니다. Git-flow git 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다. 이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답…","fields":{"slug":"/git-branch-strategy/"},"frontmatter":{"date":"July 12, 2022","title":"달록팀의 git 브랜치 전략을 소개합니다.","tags":["매트","BE","git","git-flow"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [매트](https://github.com/hyeonic)가 작성했습니다.\n\n## Git-flow\n\ngit 브랜치 전략 중 하나이다, 이것은 어떠한 기능을 나타내는 것이 아니라 방법론이다. 각각의 프로젝트와 개발 환경에 따라 알맞게 수정하여 사용해야 한다.\n\n이 게시글은 git을 알고 사용해 본 경험이 있다는 것을 전제로 작성하였다. 또한 직접 프로젝트에 적용하고 연습하고 있기 때문에 정답이 될 수 없고, 지속적으로 개선할 예정이다.\n\n## Git Repository\n\n프로젝트에 적용하기 앞서 어떠한 형태로 Git Repository가 구성되는지 살펴보았다.\n\n![](./git-repository.png)\n\n### Upstream Remote Repository\n\n개발자가 공유하는 저장소로 최신 소스코드가 저장되어 있는 원격 저장소이다.\n\n#### 적용하기\n\n이러한 Remote Repository 생성을 위하여 github에 New organization을 사용히였다.\n\n![](./new-organization.png)\n\n다양한 기능을 제공하는 Team과 Enterprice는 월마다 일정 금액을 사용해야 한다. 하지만 간단한 프로젝트 진행을 위해 생성하였기 때문에 Free만 사용하여도 충분한 실습과 프로젝트를 진행할 수 있다.\n\n![](./fare.png)\n\norganization을 생성하게 되면 소속된 repository를 생성할 수 있다. 이것을 `Upstream Remote Repository`로 적용한다.\n\n### A's, B's, C's Origin Remote Repository\n\n`Upstream Repository`를 Fork한 원격 개인 저장소이다. Upstream Repository를 직접 clone하여 작업하는 것이 아니라 각각의 팀원들이 `Fork`를 하여 원격 저장소를 생성하고 그것을 clone하여 `Local Repository`를 생성하여 작업한다.\n\n이렇게 두 개의 remote repository로 나눈 이유는 Upstream repository의 경우 `팀원이 공유`하고 있는 Repository이기 때문에 다양한 시도를 하기에 큰 위험 부담을 가지고 있다. 각자의 개인 repository에서 `작업을 시도`한 후 적절한 기능 merge 하기 위해 `Pull Request`를 요청한다.\n\n### 운영 방식\n\ngit-flow는 기본적으로 5가지의 브랜치를 사용하여 운영한다.\n\n- `main`: 제품으로 출시될 수 있는 브랜치\n- `develop`: 다음 출시 버전을 개발하는 브랜치\n- `feature`: 기능을 개발하는 브랜치\n- `release`: 이번 출시 버전을 준비하는 브랜치\n- `hotfix`: 출시 버전에서 발생한 버그를 수정하는 브랜치\n\n![](./git-flow-dev.png)\n\n`main`과 `develop` 브랜치이다. 두 브랜치는 항시 운영되어야 하는 브랜치이다. `develop`는 개발을 위한 브랜치이고, `main`은 제품으로 출시될 수 있는 브랜치 이기 때문에 `항시 배포 가능한 상태`이어야 한다.\n\n`main`과 `develop`은 `Upstream remote repository`에서 운영한다.\n\n![](./git-flow-feature.png)\n\n`feature` 브랜치는 단위 기능을 개발하는 브랜치이다. 기능 개발이 완료되면 `develop` 브랜치와 `merge` 된다.\n\n`develop`은 모든 팀원이 `공유`하는 브랜치이다. feature는 각자 맡아 작성한 코드들이 들어 있는 브랜치이다. merge 작업 전에 팀원들 간의 `지속적인 코드 리뷰`가 필요하다.\n\n그렇기 때문에 `Pull Request`를 사용하여 `merge` 작업 전 리뷰어들에게 코드 리뷰를 받고 반영 사항을 수정하여 commit 후 merge 한다. 이 과정은 `협업에서 가장 중요한 부분`이라고 생각된다.\n\n![](./git-flow-release.png)\n\n`release` 브랜치는 배포를 하기 전에 충분한 검증을 위해 생성하는 브랜치이다. 배포 가능한 상태가 되면 `main` 브랜치로 `merge` 작업을 거친다. 또한 `develop`에도 반영사항을 모두 `merge` 시켜야 한다.\n\n![](./git-flow-hotfix.png)\n\n`hotifx` 브랜치는 배포 중 버그가 생겨 긴급하게 수정해야 하는 브랜치이다. 배포 이후에 이루어지는 브랜치이고, 반영 사항을 `main`과 `develop`에 모두 적용 시켜야 한다.\n\n앞서 말했듯이 `main`과 `develop`는 항시 운영되는 브랜치이다. 이 둘을 제외한 나머지 브랜치 들은 제 역할이 마무리 되어 `merge` 작업이 완료되면 브랜치를 삭제하여 정리한다.\n\n### 간단히 적용해보기\n\n`Upstream Remote Repository`를 기반으로 원격 개인 저장소에 `Fork` 해야 한다.\n\n![](./fork.png)\n\nOrganization에 생성한 repository에 Fork를 누르면 손쉽게 할 수 있다. Fork로 생성된 repository를 기반으로 `Local Repository`를 생성해야 한다.\n\n```bash\ngit clone https://github.com/{개인 github 이름}/{repository 이름}.git\n```\n\ngit clone을 사용하여 원격 저장소에 있는 repository를 손쉽게 clone할 수 있다.\n\n```bash\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\n```\n\nclone 받은 local repository를 git remote -v로 확인해보면 원격 저장소가 등록되어 있는 것을 확인 할 수 있다. 매번 최신 코드를 `fetch` 및 `rebase` 하기 위해서는 `Upstream`을 등록해야 한다.\n\n```bash\n$ git remote add upstream https://github.com/{organization 이름}/{repository 이름}.git\n\n$ git remote -v\n\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (fetch)\norigin  https://github.com/{github 사용자 이름}/{repository 이름}.git (push)\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\nupstream        https://github.com/{organization 이름}/{repository 이름}.git\n```\n\n`git remote add upstream`을 통하여 upstream을 등록한다. 정상적으로 등록 된 것을 확인할 수 있다. 이제 작업할 때 마다 브랜치를 생성하고 최신 코드를 pull 받아야 한다.\n\n우리 팀원은 각각 개발해야 하는 기능을 github issue에 등록한 후 등록 번호를 기반으로 브랜치를 생성하기로 하였다.\n\n우선 간단한 예시를 위하여 이슈를 등록한다.\n\n![](./issue.png)\n\n3번 번호가 부여된 이슈라고 가정한다. 해당 번호를 기반으로 `local repository`에서 feature 브랜치를 생성한다.\n\n```bash\n$ git branch feature/3-init-setting\n$ git checkout feature/3-init-setting\n```\n\n이제 Upstream에 있는 remote repository에서 최신 소스코드를 받아 와야 한다.\n\n```bash\n$ git fetch upstream\n$ git rebase upstream/develop\n```\n\ngit pull을 사용하여 등록한 upstream develop에서 commit 기록을 병합한다. 이제 신나게 작업을 진행하고 자신의 원격 저장소인 `Origin remote repository`에 push한다.\n\n```bash\n$ git push origin feature/3-init-setting\n```\n\n그렇게 github repository를 살펴보면 `변경을 감지`하고 pull request를 생성할 것인지에 대한 탭을 확인할 수 있다.\n\n![](./upstream-repository.png)\n\n이제 fork한 개인 원격 저장소를 살펴보면 새롭게 작성한 브랜치를 감지하고 pull request 작성을 위한 버튼이 생성된다.\n\n![](./pull-request.png)\n\n`feature/3-init-setting` 브랜치를 develop에 merge하기 위한 pull request를 진행하는 예시이다. 작성한 코드를 리뷰해줄 팀원들을 선택하고, commit한 코드의 내용을 간단히 요약하여 작성한다. 이제 생성한 PR을 기반으로 `코드리뷰`를 진행한다. 변경 사항이 적용되면 develop에 반영하기 위해 merge한다.\n\n### 달록에 맞게 수정하기\n\ngit-flow는 빠르게 급변하는 웹 서비스에는 맞지 않은 git 브랜치 전략이다. 관리해야 할 브랜치가 늘어나기 때문에 개발자들이 신경써야 할 포인트가 늘어난다.\n\n빈번한 배포로 인해 급작스러운 이슈가 발생할 수 있다. 즉 예상치 못한 롤백이 자주일어날 수 있다. 또한 웹 서비스의 특성상 다양한 릴리즈 버전을 유지할 필요가 없다. 이러한 특성들로 인해 웹 서비스에는 다소 보수적인 git-flow 전략은 맞지 않을 수 있다.\n\n그럼에도 우리 달록팀은 git-flow를 선택했다. 우리는 실제 운영할 수 있는 서비스를 개발하며 다양한 경험을 습득해야 한다. 또한 대부분의 팀원들이 git에 익숙하지 않았으며 다양한 시도를 통해 빠르게 학습해야 한다.\n\n대신 git-flow를 정석적으로 사용하지 않고 필요한 부분을 수정하여 반영하려 한다. 현재 수준에서 `develop`에서 대부분의 빌드를 진행하기 때문에 `release` 브랜치의 필요성이 다소 옅어졌다. 결국 `release`를 제외한 `main`, `develop`, `feature`, `hotfix`만 사용하기로 결정하였다.\n\n### 달록이 집중한 것\n\n달록의 팀 각 구성원들은 맡은 이슈를 기반으로 브랜치를 생성한 뒤 작업을 진행할 것이다. 결국 다수의 브랜치가 아래와 같이 병렬적으로 커밋이 쌓이게 된다.\n\n![](./force-push.png)\n\n만약 팀원 중 한명이 작업을 끝내서 PR이 merge된 상황이라고 가정하자. develop 브랜치의 커밋 베이스는 변경됬으며 이전에 작업을 진행하던 브랜치들은 시작점이 뒤로 밀려나게 된다.\n\n여러 사람이 하나의 저장소를 기반으로 작업을 진행하기 때문에 함께 사용하는 공간의 코드들은 충돌을 야기할 가능성이 크다. 즉 지속적인 `fetch` + `rebase`를 통해 사전에 충돌에 대비하며 항상 `develop` 브랜치와 커밋 싱크를 맞춘다.\n\n정리하면 위 그림과 같이 항시 develop 브랜치의 끝 단에서 시작해야 한다. 이러한 방식은 코드의 충돌을 최소화할 수 있으며 순차적인 git 커밋 목록을 기반으로 쉽게 기능이 추가된 것을 확인할 수 있다.\n\n## 정리\n\n지금까지 간단히 `git-flow의 흐름`과 이것을 기반으로 `달록에 적용한 과정`들을 알아보았다. git-flow는 언급한 것 처럼 부가적인 브랜치로 인해 `관리에 대한 부담감`을 느낄 수 있다. 하지만 `upstream`과 `origin`을 분리한 환경은 좀 더 도전적인 과제들을 적용하기에 매우 좋은 환경을 구성해준다. 또한 `pull request`를 통한 코드 리뷰를 통해 보다 더 양질의 애플리케이션 개발에 힘쓸 수 있다.\n\n## References.\n\n[git flow; 환상과 현실 그 사이에 서비스](https://vallista.kr/git-flow;-%ED%99%98%EC%83%81%EA%B3%BC-%ED%98%84%EC%8B%A4-%EA%B7%B8-%EC%82%AC%EC%9D%B4%EC%97%90-%EC%84%9C%EB%B9%84%EC%8A%A4/)<br>\n[우린 Git-flow를 사용하고 있어요](https://woowabros.github.io/experience/2017/10/30/baemin-mobile-git-branch-strategy.html)<br>\n"},{"excerpt":"이 글은 우테코 달록팀 크루 후디가 작성했습니다. 배경 우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시  키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해  키워드를 사용합니다. 물론…","fields":{"slug":"/intellij-final-keyword/"},"frontmatter":{"date":"July 12, 2022","title":"IntelliJ에서 메소드 추출한 메소드의 파라미터에 final 키워드 자동 추가하기","tags":["intellij"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [후디](https://github.com/devHudi)가 작성했습니다.\n\n## 배경\n\n우테코 레벨3 달록 팀에서 메소드의 파라미터에는 반드시 `final` 키워드를 붙이도록 컨벤션을 정했습니다. 이유는 무엇일까요? 일반적으로 가변적인 변수는 프로그램의 흐름을 예측하기 어렵게 만듭니다. 따라서 변수를 가변적으로 만드는 것이 중요한데, 자바에서는 변수의 재할당을 막기 위해 `final` 키워드를 사용합니다. 물론 `final` 키워드 하나만으로 완전한 불변을 보장하도록 만들수는 없지만, 어느정도 예측 가능한 코드를 만드는데에는 도움이 됩니다.\n\n이는 메소드의 파라미터에도 적용됩니다. 아래의 코드는 `Memo` 객체를 생성하기 위한 생성자입니다. `value` 라는 String 값을 전달받아 객체 필드에 할당합니다.\n\n```java\npublic Memo(String value) {\n    validateLength(value);\n\n    value = \"hello\"; // 예상치 못한 코드\n\n    this.value = value;\n}\n```\n\n하지만 위처럼 예상치 못한 코드가 추가되면 어떻게 될까요? `value` 필드에는 개발자가 의도하지 못한 값이 할당될 것 입니다.\n\n```java\npublic Memo(final String value) {\n    validateLength(value);\n\n    value = 1; // error: final parameter value may not be assigned\n\n    this.value = value;\n}\n```\n\n이를 보완하기 위해서 위처럼 메소드 파라미터에 `final` 키워드를 붙이면, 재할당 시 컴파일 에러가 발생하여 예상치 못한 동작을 사전에 방지할 수 있을 것 입니다.\n\n## IntelliJ 설정하기\n\n하지만, 저희는 아직 메소드 파라미터에 `final` 키워드를 붙이는 습관이 들어있지 않았습니다. 따라서 IDE의 도움이 필요한데요, 다행히도 IntelliJ에서 메소드 추출 리팩토링을 할 때 생성되는 메소드 파라미터에 자동으로 `final` 키워드를 붙여주는 옵션을 발견하였습니다.\n\n![IntelliJ 설정 화면](./intellij.png)\n\n맥 기준으로 Preferences → Editor → Code Style → Java 페이지에서 Code Generation 탭을 클릭합니다. 해당 탭의 하단에 ‘Final Modifier’ 에서 ‘Make generated parameters final’ 을 체크해줍니다. 위와 같이 옵션을 변경하면 메소드 추출 시 파라미터에 자동으로 `final` 키워드가 생성되는 모습을 확인할 수 있습니다 😊\n"},{"excerpt":"이 글은 우테코 달록팀 크루 리버가 작성했습니다. JPA 등장배경 1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이…","fields":{"slug":"/appearance-background-of-jpa/"},"frontmatter":{"date":"July 07, 2022","title":"JPA 등장배경","tags":["JPA"]},"rawMarkdownBody":"\n> 이 글은 우테코 달록팀 크루 [리버](https://github.com/gudonghee2000)가 작성했습니다.\n\n## JPA 등장배경\n\n1990년대 인터넷이 보급되면서 온라인 비지니스가 활성화 되었다.\n자연스럽게, 온라인 비지니스에서 DB에 데이터를 저장하고 가져올때 사용할 Connection Connector에 대한 니즈가 높아졌다.\n그래서 각 언어들에서 DB Connection을 지원하는 API 기술들이 등장하였다. 이후에 Spring에서는 DB Connection을 좀 더 쉽게 관리하는 Spring JDBC API를 만들고 지원하였다. (이외에도 Query문을 XML파일을 통해 관리하게끔 도와주는 Mybatis도 등장하였음)\n하지만, 여전히 쿼리문을 개발자가 직접 작성해야하는 등 다양한 문제를 가지고 있었다.\n그래서 JAVA 진영에서는 개발자가 쿼리문을 직접 작성하지 않아도 프레임워크 내부에서 지원해주는 ORM(Object Relational Model)기술인 JPA가 등장하였다.\n그렇다면, JPA 이전에 개발자들이 직접 쿼리문을 작성하던 SQL 중심적인 개발의 단점은 무엇이 있을까?\n아래에서 살펴보자.\n\n## SQL 중심적인 개발의 단점\n\n#### 1. 쿼리문 무한 반복과 지루한 코딩\n\nJDBC API는 쿼리문을 개발자들이 직접 작성 해야한다.\n그래서 개발자들은 쿼리문을 작성하는 지루한 작업을 개발 과정에서 무한반복해야한다.\n\n#### 2. 객체의 필드가 추가되면 모든 쿼리문을 수정해야한다.\n\n![](https://velog.velcdn.com/images/gudonghee2000/post/488a7899-d55b-4447-aafc-bada8d840192/image.jpg)\n위 그림과 같이 SQL 중심적인 개발에서는 객체의 필드가 변경되면 해당하는 모든 쿼리문을 찾아 개발자가 수정해야한다.\n\n#### 3. 객체와 관계형 DB의 패러다임의 불일치\n\n객체라고 하면 떠오르는 키워드는 무엇이 있을까?\n캡슐화, 협력, 의존, 상속, 참조 등의 기술이 있다. 그런데, DB에서는 이러한 기술들이 없다.\n적용되는 기술들의 패러다임 불일치로 인해 개발자들은 SQL 지향적인 개발을 할 수 밖에 없다.\n아래에서 자세히 살펴보자.\n\n## 객체와 관계형 DB의 패러다임 차이\n\n객체와 관계형 DB는 연관관계를 통해 작업을 수행한다는 공통점을 가진다.\n하지만 연관관계를 맺는 패러다임의 차이를 가진다.\n\n객체는 상속, 참조를 통해 연관관계를 맺는다.\n반면 관계형 DB는 PK, FK를 통해 연관관계를 맺는다.\n이때, 연관관계를 맺는 방식의 차이로 발생하는 문제점을 코드와 함께 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 `Crew` 객체가 `Team` 객체를 필드로 가지고 참조한다고 하자.\n객체지향적인 관점에서, `Crew`와 `Team`의 관계를 위와 같이 표현하는것은 자연스럽다.\n\n하지만, DB에서는 `Crew`가 `Team`을 참조한다는 개념이 없다.\n그래서 위 객체들을 가지고 DB의 `Crew`테이블과 `Team`테이블의 관계를 맺을때, 아래와 같이 `PK` 값인 id를 `FK`로 가지도록 구현 하여야한다.\n![ERD](./erd.png)\n\n연관관계에 대해서 객체의 구조와 DB의 구조가 달라진다는 것이다.\n\n그렇다면 객체지향적인 연관관계를 가진 객체들을 DB에 저장 할 때,\nDB의 연관관계로 변경하는 것이 왜 문제가 될까?\n\n## 객체와 RDB의 연관관계 차이가 가져오는 문제\n\n위에서 봤던 `Crew`와 `Team`의 객체 모델링을 다시한번 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Team team;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 모델링된 `Crew`와 `Team`을 DB에 저장한다고 한다면 다음의 과정이 필요하다.\nDB에 접근하고자 하는 Dao 객체는 `Crew`객체를 분해하고 각자 `Crew` 테이블과 `Team` 테이블에 대한 쿼리를 작성해야한다. 단순히 `Crew`의 객체정보를 저장하는데 3가지 과정을 거쳐야한다.\n\n이러한 복잡한 과정을 피하는 방법은 없을까?\n아래 코드를 살펴보자.\n\n```java\npublic class Crew {\n   private Long id;\n    private String name;\n    private String nickName;\n    private Long team_id;\n}\n\npublic class Team {\n   private Long id;\n    private String team_name;\n}\n```\n\n위와 같이 DB 테이블 구조에 맞추어 `Crew`와 `Team` 객체를 설계하는 방법이있다.\n이러한 객체 모델링은 Dao 객체를 통해 데이터를 DB에 저장 할 때, `Crew` 객체를 분해하는 과정을 삭제 할 수 있다.\n객체가 DB 구조에 맞추어 설계되어 있기 때문이다.\n\n**하지만, 객체 모델링을 할 때 객체가 서로 참조하는 객체지향적인 개발이 아닌\nDB 테이블구조에 맞추어 개발하는 SQL 중심적인 개발을 하게 된다는 문제를 가진다.**\n\n객체와 관계형 DB의 연관관계의 패러다임 차이는 객체를 객체답지 못하게 만든다는 것이다.\n그렇다면, 패러다임의 차이를 해결하는 방법은 없을까?\n\n## JPA\n\n객체와 관계형 DB의 패러다임의 차이로 인해 우리는 객체지향적인 프로그래밍을 하지못하고 DB에 종속적인 개발을 하게된다.\n이러한 문제를 해결하기 위해 JAVA진영에서는 JPA를 제공한다.\n\nJPA를 통해 개발자는 더이상 쿼리문을 반복적으로 작성하거나 유지보수하는 것을 신경쓰지 않아도 된다.\n왜냐하면 JPA가 쿼리문을 작성해주기 때문이다.\n그리고 SQL 중심적인 개발에서 벗어나 객체지향적인 개발을 할 수 있게 된다.\n왜냐하면 패러다임의 불일치를 JPA가 내부적으로 맵핑해주기 때문이다.\n\n다음 포스팅에서는 JPA의 작동 메커니즘을 자세히 살펴보자.\n"}]}},"pageContext":{}},"staticQueryHashes":[]}